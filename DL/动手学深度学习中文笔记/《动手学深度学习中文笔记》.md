## 01-课程安排

### 本节目录

- [1. 课程目标](#1-课程目标)
- [2. 内容](#2-内容)
- [3. 学到什么](#3-学到什么)
- [4. 基本要求](#4-基本要求)
- [5. 课程资源](#5-课程资源)
- [6. 总结](#6-总结)

### 1. 课程目标

- 介绍深度学习经典和最新模型
  - LeNet, ResNet, LSTM, BERT, ...
- 机器学习基础
  - 损失函数、目标函数、过拟合、优化
- 实践
  - 使用Pytorch实现介绍的知识点
  - 在真实数据上体验算法效果

### 2. 内容

> 深度学习基础：线性神经网络，多层感知机
> 
> 卷积神经网络：LeNet, AlexNet, VGG, Inception, ResNet
> 
> 循环神经网络：RNN，GRU，LSTM，seq2seq
> 
> 注意力机制：Attention， Transformer
> 
> 优化算法：SGD，Momentum，Adam
> 
> 高性能计算：并行，多GPU，分布式
> 
> 计算机视觉：目标检测，语义分割
> 
> 自然语言处理：词嵌入，BERT

### 3. 学到什么

- What：深度学习有哪些技术，以及哪些技术可以帮你解决问题
- How：如何实现（产品 or paper）和调参（精度or速度）
- Why：背后的原因（直觉、数学）

### 4. 基本要求

- **AI相关从业人员**（产品经理等）：掌握What，知道名词，能干什么
- **数据科学家、工程师**：掌握What、How，手要快，能出活
- **研究员、学生**：掌握What、How、Why，除了知道有什么和怎么做，还要知道为什么，思考背后的原因，做出新的突破

### 5. 课程资源

- 课程主页：[https://courses.d2l.ai/zh-v2/](https://courses.d2l.ai/zh-v2/)
- 教材：[https://zh-v2.d2l.ai/](https://zh-v2.d2l.ai/)
- 课程论坛讨论：[https://discuss.d2l.ai/c/chinese-version/16](https://discuss.d2l.ai/c/chinese-version/16)
- Pytorch论坛：[https://discuss.pytorch.org/](https://discuss.pytorch.org/)
- b站视频合集：[https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497]

### 6. 总结

- 课程目标、内容和要求
- 相关课程资源链接

**<mark>整合版来自→公众号：坚持打代码</mark>**

## 02-深度学习介绍

### 本节目录

- [1. 概述](#1-%E6%A6%82%E8%BF%B0)
  - [1.1 AI地图](#11-ai%E5%9C%B0%E5%9B%BE)
  - [1.2 AI地图解读](#12-ai%E5%9C%B0%E5%9B%BE%E8%A7%A3%E8%AF%BB)
- [2. 深度学习的应用](#2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8)
  - [2.1 图片分类](#21-%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB)
  - [2.2 物体检测和分割](#22-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%92%8C%E5%88%86%E5%89%B2)
  - [2.3 样式迁移](#23-%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB)
  - [2.4 人脸合成](#24-%E4%BA%BA%E8%84%B8%E5%90%88%E6%88%90)
  - [2.5 文字生成图片](#25-%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87)
  - [2.6 文字生成](#26-%E6%96%87%E5%AD%97%E7%94%9F%E6%88%90)
  - [2.7 无人驾驶](#27-%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6)
  - [2.8 案例研究——广告点击](#28-%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%E5%B9%BF%E5%91%8A%E7%82%B9%E5%87%BB)
    - [2.8.1 步骤](#281-%E6%AD%A5%E9%AA%A4)
    - [2.8.2 模型的预测与训练](#282-%E6%A8%A1%E5%9E%8B%E7%9A%84mark%E9%A2%84%E6%B5%8B%E4%B8%8E%E8%AE%AD%E7%BB%83mark)
    - [2.8.3 完整的故事](#283-%E5%AE%8C%E6%95%B4%E7%9A%84%E6%95%85%E4%BA%8B)
- [3. 总结](#3-%E6%80%BB%E7%BB%93)
- [4. 深度学习介绍 Q&A](#4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D-qa)

### 1. 概述

#### 1.1 AI地图

首先画一个简单的人工智能地图：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-01.png)

- x轴表示不同的模式or方法：最早的是符号学，接下来是概率模型，之后是机器学习

- y轴表示可以达到的层次：由底部向上依次是
  
  > 感知：了解是什么，比如能够可以看到物体，如面前的一块屏幕
  > 
  > 推理：基于感知到的现象，想象或推测未来会发生什么
  > 
  > 知识：根据看到的数据或者现象，形成自己的知识
  > 
  > 规划：根据学习到的知识，做出长远的规划

#### 1.2 AI地图解读

- 问题领域的一个简单分类
  
  - **自然语言处理**：
    
    - 停留在比较简单的**感知**层面，比如自然语言处理用的比较多的机器翻译，给一句中文翻译成英文，很多时候是人的潜意识里面大脑感知的一个问题。一般来说，人可以几秒钟内反应过来的东西，属于感知范围。
    - 自然语言处理最早使用的方法是**符号学**，由于语言具有符号性；之后一段时间比较流行的有**概率模型**，以及现在也用的比较多的**机器学习**。
  
  - **计算机视觉**：
    
    - 在简单的感知层次之上，可以对图片做一些**推理**。
    - 图片里都是一些像素，很难用符号学解释，所以一般采用**概率模型**和**机器学习**。
  
  - **深度学习**
    
    - 机器学习的一种，更深层的神经网络。
    - 可以做计算机视觉，自然语言处理，强化学习等。

- 过去八年最热的方向，也是本课程关心的重点：
  
  - 深度学习+计算机视觉 / 自然语言处理

本节课只关注AI中的一小部分领域，即深度学习背景下的视觉和自然语言处理相关的基础应用。

---

### 2. 深度学习的应用

#### 2.1 图片分类

深度学习最早是在图片分类上有比较大的突破，ImageNet是一个比较大的图片分类数据集，

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-02.png)

`x轴`：年份 `y轴`：错误率 `圆点`：表示某年份某研究工作/paper的错误率 [IMAGENET](https://image-net.org/) [数据来源](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/)

> 在2010年时，错误率比较高，最好的工作错误率也在26%、%27左右；
> 
> 在2012年，有团队首次使用深度学习将错误率降到25%以下；
> 
> 在接下来几年中，使用深度学习可以将误差降到很低。
> 
> 2017年基本所有的团队可以将错误率降到5%以下，基本可以达到人类识别图片的精度。

#### 2.2 物体检测和分割

当你不仅仅想知道图片里有什么内容，还想知道物体是什么，在什么位置，这就是**物体检测**。**物体分割**是指每一个像素属于什么，属于飞机还是属于人(如下图)，这是图像领域更深层次的一个应用。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-03.png)

#### 2.3 样式迁移

原图片+想要迁移的风格=风格迁移后的图片，加了一个可以根据输入改变图片风格的滤镜。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-04.png)

#### 2.4 人脸合成

下图中所有的人脸都是假的，由机器合成的图片：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-05.png)

#### 2.5 文字生成图片

1. 描述：一个胡萝卜宝宝遛狗的图片。

2. 描述：一个牛油果形状的靠背椅。
   
   ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-06.png)

#### 2.6 文字生成

> 示例1：
> 
> 问题输入：如何举行一个有效的董事会议
> 
> 机器输出：生成篇章回答

> 示例2：
> 
> 输入：将Students从School这个table中选出来
> 
> 输出：用于查询的SQL语言

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-07.png)

#### 2.7 无人驾驶

识别车、道路以及各种障碍物等，并规划路线。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-08.png)

#### 2.8 案例研究——广告点击

> 用户输入想要搜索的广告内容，如：baby toy

> 网站呈现最具有效益的广告(用户更可能点击，且给网站带来更高经济效益)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-09.png)

##### 2.8.1 步骤

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-10.png)

1. 触发：用户输入关键词，机器先找到一些相关的广告
2. 点击率预估： 利用机器学习的模型预测用户对广告的点击率
3. 排序：利用点击率x竞价的结果进行排序呈现广告，排名高的在前面呈现

##### 2.8.2 模型的预测与训练

上述步骤的第二步中涉及到模型预测用户的点击率，具体过程如下：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-11.png)

**模型预测**

数据 (待预测广告) → 特征提取 → 模型 → 点击率预测

**模型训练**

训练数据 (过去广告展现和用户点击) → 特征(X)和用户点击(Y) → 喂给模型训练

##### 2.8.3 完整的故事

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/02-12.png)

**领域专家**：对特定的应用有比较深的了解，根据展现情况以及用户点击分析用户的行为，期望模型对应用做一些拟合，符合真实数据和分析情况。

**数据科学家**：利用数据训练模型，训练后模型投入使用，进行预测呈现。

**AI专家**：应用规模扩大，用户数量增多，模型更加复杂，需要进一步提升精度和性能。

### 3. 总结

- 通过AI地图，课程从纵向和横向两个维度解读了深度学习在重要问题领域的概况。
- 介绍了深度学习在CV和NLP方面的一些应用
- 简单分析并研究了深度学习实例——广告点击。

---

### 4. 深度学习介绍 Q&A

◆ **Q1：机器学习的可解释性？机器学习在图片分割为什么有效，目前有没有定论？**

> A1：模型的可解释性不管是深度学习还是机器学习都是非常受人关注的点，因为这像是一个黑盒，训练了一个模型，也不知道为什么work或者为什么不work。对于深度学习来说，模型的**可解释性**是做得不够好的；对于机器学习来说，我们对简单的模型可以理解，但当模型变得很深的时候，几乎只能放弃理解的过程。
> 
> 特别地，**为什么有效**和**可解释性**是两个不同的概念，所有提出的新模型都会解释一下为什么有效，可解释性是说人是不是能理解这个模型，理解模型为什么工作是一个方面，还需要知道它什么时候不工作以及在什么地方会出现偏差。
> 
> 目前来说，一个模型在一个应用/领域上为什么可以工作，会有一些解释，我们会在解释不同的模型的时候给大家进行讲解。

◆ **Q2：领域专家是什么意思？**

> A2：举个例子，比如我要做农业上的物体识别，我种了一棵树，想要看今年的收成怎么样，我有很多很多土地，用人去一个个查看很费力，于是我用一个无人机，将农作物的情况拍下来，假设得到了树的一些图片，而数据科学家不知道农作物什么样的情况是好，什么样是坏，于是**领域专家进行解释**，比如多少叶子算是好，什么样不好。同时**数据科学家**将领域专家的**问题翻译**成机器学习能做的任务。所以可以认为**领域专家**是**提需求**的人**甲方**，而**数据科学家**是**乙方**。

◆ **Q3：MXNet的GPU版本的安装必须要卸载CPU版本的吗？**

> A3：是的，但我们的课程是**基于Pytorch**，Pytorch会不一样一些。

◆ **Q4：深度学习无法用数学规范表述，只能用直觉理解是吗？**

> A4：不一定，深度学习模型可以用**数学形式表示**，接下来也会讲到很多数学的东西，但是说具体用数学解释它**为什么工作**，**为什么不工作**，这个是目前我们做的不好的地方。

◆ **Q5：符号学可以和机器学习融合起来吗？**

> A5：确实是可以的。目前来说，**符号学**在**深度学习**有一些新的进展，以前说符号学就是做一些符号上的推理，目前**深度学习如图神经网络**，可以做一些比较复杂的推理。

◆ **Q6：数据科学家和AI专家的主要区别在哪里？**

> A6：我觉得没有太多区别。数据科学家很多时候关注的是给一个数据，赶紧出一个模型，能工作就好了，**关注**的是如何把领域专家的一个实际的业务问题，变成一个机器学习能做的任务，训练成一个还不错的模型。**AI专家也可能是数据科学家**，也就是说其不仅要训练出来一个模型能用，而且会**关心**如何把模型精度做得很高，或者可以说，**资深数据科学家可以认为是AI专家**。
> 
> 数据科学家可以有**两条路线**，一个是**不断开拓新领域**，比如机器学习在农业上的应用，在医疗上的应用等，这个是**往广的走**，**往深的走**的话可以称为是AI专家，在某一块方面了解得很深。

◆ **Q7：MAC是不是可以支持Pytorch？**

> A7：可以。

◆ **Q8：说自然语言处理仅仅停留在感知层面似乎不太合适？因为语言的理解和产出不仅仅是感知，也涉及到语言知识和世界知识，也涉及到规划，比如机器规划下一步要做什么。**

> A8：确实是这样，语言当然是一个很复杂的过程，我只是想说，自然语言处理我们做得还很一般，虽然能做一些感知以外的东西，但是我感觉是说，**不如**深度学习特别机器学习，在图片上的应用做得好一些。当然AI地图上也只是一个**大致的分类**，大家不用特别纠结。

◆ **Q9：请问老师有考虑后面讲一讲如何寻找自己领域的paper的经验吗？**

> A9：这是一个很好的问题，因为大家如果现在去读paper的话，可能每天都有一百篇paper出来，你怎么样去找到你想要的paper，总不能天天看朋友圈推文，这样只能知道别人读过的paper，不会有自己**独特的见解**，怎么样找到自己感兴趣和有启发性的论文，后面有机会和大家分享一下我的做法。

◆ **Q10：以无人驾驶为例，误判率在不断下降，但误判的影响还是很严重的，有可能从已有的判断case(样例)得到修正，从而完全避免这样的错误吗？**

> A10：**无人驾驶**中，任何一次出现的错误，都可能带来毁灭性的灾难。大家可能看到，特斯拉今天撞了，明天又撞了。所以说，无人驾驶对于**错误率**确实是非常注重的。
> 
> 机器学习在学术界现在有很多关于**uncertainty**或者**robustness**的研究，就是说模型在数据偏移或者极端情况下会不会给出很不好的答案，我们不会特别深入去讲这个事情，但是无人驾驶这一块确实会通过大量的技术，比如说把不同的模型融合在一起，不是仅仅train一个模型，用多个模型来做投票。汽车有很多雷达、摄像头，它会通过不同的传感器来进行模型的融合，从而降低误差。
> 
> 我们这个课程不会特别地讲，因为涉及到评价无人驾驶的特别技术，但在竞赛中我们会给大家看到如何通过**融合多个模型提升精度**的做法。

**<mark>整合版来自→公众号：坚持打代码</mark>**

## 03-安装

### 本节目录

- [1.安装python](#1%E5%AE%89%E8%A3%85python)
- [2.安装Miniconda/Anaconda](#2%E5%AE%89%E8%A3%85minicondaanaconda)
  - [2.1 安装Miniconda](#21-%E5%AE%89%E8%A3%85miniconda)
  - [2.2 Miniconda环境操作](#22-miniconda%E7%8E%AF%E5%A2%83%E6%93%8D%E4%BD%9C)
- [3.安装Pytorch, d2l, jupyter包](#3%E5%AE%89%E8%A3%85pytorch-d2l-jupyter%E5%8C%85)
- [4. 总结](#4-%E6%80%BB%E7%BB%93)

### 1.安装python

首先前提是安装python，这里推荐安装python3.8 输入命令 ***sudo apt install python3.8*** 即可

### 2.安装Miniconda/Anaconda

- 然后第二步，安装 Miniconda（如果已经安装conda或者Miniconda，则可以跳过该步骤)。
  
  #### 2.1 安装Miniconda
  
  - 安装MIniconda的好处是可以创建很多虚拟环境，并且不同环境之间互相不会有依赖关系，对日后的项目有帮助，如果只想在本地安装的话，不装Miniconda只使用pip即可，第二步可以跳过。
  - 如果是Windows系统，输入命令 ***wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Windows-x86_64.exe***
  - 如果是macOS，输入命令 ***wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-MacOSX-x86_64.sh*** 之后要输入命令 ***sh Miniconda3-py38_4.10.3-MacOSX-x86_64.sh -b***
  - 如果是Linux系统，输入命令 ***wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.10.3-Linux-x86_64.sh*** 之后输入命令 ***sh Miniconda3-py38_4.10.3-Linux-x86_64.sh -b***
  - 以上都是基于python3.8版本，对于其他版本，可以访问 ***https://docs.conda.io/en/latest/miniconda.html*** ，下载对应版本即可。
  
  #### 2.2 Miniconda环境操作
  
  - 对于第一次安装Miniconda的，要初始化终端shell，输入命令 ***~/miniconda3/bin/conda init***
  - 这样我们就可以使用 ***conda create --name d2l python=3.8 -y*** 来创建一个名为xxx的环境，这里命名为d2l
  - 打开xxx环境命令: ***conda activate xxx*** ；关闭命令：***conda deactivate xxx***。对于基础conda环境不用添加名

### 3.安装Pytorch, d2l, jupyter包

- 第三步，安装深度学习框架和`d2l`软件包
  
  在安装深度学习框架之前，请先检查你的计算机上是否有可用的GPU（为笔记本电脑上显示器提供输出的GPU不算）。 例如，你可以查看计算机是否装有NVIDIA GPU并已安装[CUDA](https://developer.nvidia.com/cuda-downloads)。 如果你的机器没有任何GPU，没有必要担心，因为你的CPU在前几章完全够用。 但是，如果你想流畅地学习全部章节，请提早获取GPU并且安装深度学习框架的GPU版本。
  
  - 你可以按如下方式安装PyTorch的CPU或GPU版本：
    
    ```
    pip install torch==1.8.1
    pip install torchvision==0.9.1
    ```
  
  - 也可以访问官网 ***https://pytorch.org/get-started/locally/*** 选择适合自己电脑pytorch版本下载
  
  - ![0301](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/02/03-01.png)
  
  - 本课程的jupyter notebook代码详见 ***https://zh-v2.d2l.ai/d2l-zh.zip***
  
  - 下载jupyter notebook ：输入命令 ***pip install jupyter notebook*** （若pip失灵可以尝试pip3），输入密命令 ***jupyter notebook*** 即可打开。

### 4. 总结

- 本节主要介绍**安装Miniconda**、**CPU环境下的Pytorch**和其它课程所需**软件包**(d2l, jupyter)。对于前面几节来说，CPU已经够用了。
  
  - 如果您**已经安装**了Miniconda/Anaconda, Pytorch框架和jupyter记事本, 您只需再安装**d2l包**，就可以跳过本节视频了**开启深度学习之旅**了; 如果希望后续章节在**GPU下跑深度学习**, 可以**新建环境**安装**CUDA版本的Pytorch**。
  
  - 如果需要在Windows下**安装CUDA和Pytorch**(cuda版本)，用**本地GPU跑深度学习**，可以参考李沐老师[Windows下安装CUDA和Pytorch跑深度学习](https://www.zhihu.com/zvideo/1363284223420436480)，如果网慢总失败的同学可以参考[cuda11.0如何安装pytorch？ - Glenn1Q84的回答 - 知乎](https://www.zhihu.com/question/425647129/answer/2278290137)。当然，如果不方便在本地进行配置(如无GPU, GPU显存过低等)，也可以选择[Colab](https://colab.research.google.com/)(需要科学上网)，或其它**云服务器**GPU跑深度学习。
    
    ### 

## 04-数据读取和操作

### 本节目录

- [1. 数据操作](#1-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C)
  - [1.1 基本操作](#11-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C)
  - [1.2 简单运算](#12-%E7%AE%80%E5%8D%95%E8%BF%90%E7%AE%97)
  - [1.3 广播机制](#13-%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6)
  - [1.4 索引和切片](#14-%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87)
  - [1.5 节约内存](#15-%E8%8A%82%E7%BA%A6%E5%86%85%E5%AD%98)
  - [1.6 转换为其他Python对象](#16-%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%85%B6%E4%BB%96python%E5%AF%B9%E8%B1%A1)
- [2. 数据预处理](#2-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86)
  - [2.1 读取数据集](#21-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86)
  - [2.2 处理缺失值](#22-%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC)
  - [2.3 转换为张量格式](#23-%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%BC%A0%E9%87%8F%E6%A0%BC%E5%BC%8F)
- [3. Q&A](#3-qa)

### 1. 数据操作

为了能够完成各种数据操作，我们需要某种方法来存储和操作数据。通常，我们需要做两件重要的事：

1. 获取数据；
2. 将数据读入计算机后对其进行处理。

如果没有某种方法来存储数据，那么获取数据是没有意义的。

首先，我们介绍 n 维数组，也称为**张量**（tensor）。PyTorch的**张量类**与Numpy的`ndarray`类似。但在深度学习框架中应用PyTorch的**张量类**，又比Numpy的`ndarray`多一些重要功能：

1. tensor可以在很好地支持GPU加速计算，而NumPy仅支持CPU计算；
2. tensor支持自动微分。

这些功能使得张量类更适合深度学习。

#### 1.1 基本操作

[**张量表示由一些数值组成的数组，这个数组可能有多个维度**]。具有一个轴的张量对应数学上的**向量**（vector）；具有两个轴的张量对应数学上的**矩阵**（matrix）；具有两个轴以上的张量没有特殊的数学名称。

我们可以使用`arange`创建一个行向量`x`。这个行向量包含从0开始的前12个整数，它们被**默认创建为浮点数**。张量中的每个值都称为张量的**元素**（element）。例如，张量`x`中有12个元素。除非额外指定，新的张量默认将存储在内存中，并采用基于CPU的计算。

```python
>>> x = torch.arange(12)
    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
```

[**可以通过张量的`shape`属性来访问张量（沿每个轴的长度）的*形状***]。

```python
>>> x.shape
    torch.Size([12])
```

如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。

因为这里在处理的是一个向量，所以它的`shape`与它的`size`相同。在处理更高维度的的张量时，可以用这种方法获取张量中元素的个数。

```python
>>> x.numel()
    12
```

[**要想改变一个张量的形状而不改变元素数量和元素值，可以调用`reshape`函数。**]

例如，可以把张量`x`从形状为（12,）的行向量转换为形状为（3,4）的矩阵。这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。值得注意的是，虽然张量的形状发生了改变，但其元素值并没有变。改变张量的形状时，张量中元素的个数不会改变。

```python
>>> X = x.reshape(3, 4)
    tensor([[ 0,  1,  2,  3],
            [ 4,  5,  6,  7],
            [ 8,  9, 10, 11]])
```

我们不需要通过手动指定每个维度来改变形状。如果我们的目标形状是（高度 x 宽度），那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。我们可以通过`-1`来调用此自动计算出维度的功能即可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。

有时，我们希望[**使用全0、全1、其他常量，或者从特定分布中随机采样的数字**]来初始化矩阵。我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0或者1。

```python
>>> torch.zeros((2, 3, 4))
    tensor([[[0., 0., 0., 0.],
              [0., 0., 0., 0.],
              [0., 0., 0., 0.]],

            [[0., 0., 0., 0.],
              [0., 0., 0., 0.],
              [0., 0., 0., 0.]]])

>>> torch.ones((2, 3, 4))
    tensor([[[1., 1., 1., 1.],
              [1., 1., 1., 1.],
              [1., 1., 1., 1.]],

            [[1., 1., 1., 1.],
              [1., 1., 1., 1.],
              [1., 1., 1., 1.]]])
```

有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。以下代码创建一个形状为（3,4）的张量。其中的每个元素都从均值为0、标准差为1的标准正态分布中随机采样。

```python
>>> torch.randn(3, 4)
    tensor([[ 0.1364,  0.3546, -0.9091, -1.8926],
            [ 0.5786, -0.9019, -0.1305, -0.1899],
            [ 0.5696,  1.1626, -0.5987,  0.4085]])
```

我们还可以[**通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值**]。在这里，最外层的列表对应于轴0，内层的列表对应于轴1。

```python
>>> torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
    tensor([[2, 1, 4, 3],
            [1, 2, 3, 4],
            [4, 3, 2, 1]])
```

#### 1.2 简单运算

我们想在这些数据上执行数学运算，其中最简单且最有用的操作是**按元素**（elementwise）运算。它们将标准标量运算符应用于数组的每个元素。对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。我们可以基于任何从标量到标量的函数来创建按元素函数。我们通过将标量函数升级为按元素向量运算来生成向量值F: \mathbb{R}^d, \mathbb{R}^d \rightarrow \mathbb{R}^d。

对于任意具有相同形状的张量，[**常见的标准算术运算符（`+`、`-`、`*`、`/`和`**`）都可以被升级为按元素运算**]。我们可以在同一形状的任意两个张量上调用按元素操作。我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。

```python
>>> x = torch.tensor([1.0, 2, 4, 8])
>>> y = torch.tensor([2, 2, 2, 2])
>>> x + y, x - y, x * y, x / y, x ** y
    (tensor([ 3.,  4.,  6., 10.]),
      tensor([-1.,  0.,  2.,  6.]),
      tensor([ 2.,  4.,  8., 16.]),
      tensor([0.5000, 1.0000, 2.0000, 4.0000]),
      tensor([ 1.,  4., 16., 64.]))
```

(**“按元素”方式可以应用更多的计算**)，包括像求幂这样的一元运算符。

```python
>>> torch.exp(x)
    tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
```

[**我们也可以把多个张量*连结*（concatenate）在一起**]，把它们端对端地叠起来形成一个更大的张量。我们只需要提供张量列表，并给出沿哪个轴连结。下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素）和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。我们可以看到，第一个输出张量的轴-0长度（6）是两个输入张量轴-0长度的总和（3 + 3）；第二个输出张量的轴-1长度（8）是两个输入张量轴-1长度的总和（4 + 4）。

```python
>>> X = torch.arange(12, dtype=torch.float32).reshape((3,4))
>>> Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
>>> torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
    (tensor([[ 0.,  1.,  2.,  3.],
              [ 4.,  5.,  6.,  7.],
              [ 8.,  9., 10., 11.],
              [ 2.,  1.,  4.,  3.],
              [ 1.,  2.,  3.,  4.],
              [ 4.,  3.,  2.,  1.]]),
      tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
              [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
              [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
```

由上述例子可见，当需要按轴-x连结两个张量时，我们就在第x+1层括号内将两张量中的元素相组合。类似地，我们将两个三维张量相连结。

```python
>>> X = torch.arange(12, dtype=torch.float32).reshape((3, 2, 2))
>>> Y = torch.tensor([[[2.0, 1], [4, 3]], [[1, 2], [3, 4]], [[4, 3], [2, 1]]])
>>> torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1), torch.cat((X, Y), dim=2)
    (tensor([[[ 0.,  1.],
                [ 2.,  3.]],

              [[ 4.,  5.],
                [ 6.,  7.]],

              [[ 8.,  9.],
                [10., 11.]],

              [[ 2.,  1.],
                [ 4.,  3.]],

              [[ 1.,  2.],
                [ 3.,  4.]],

              [[ 4.,  3.],
                [ 2.,  1.]]]),
      tensor([[[ 0.,  1.],
                [ 2.,  3.],
                [ 2.,  1.],
                [ 4.,  3.]],

              [[ 4.,  5.],
                [ 6.,  7.],
                [ 1.,  2.],
                [ 3.,  4.]],

              [[ 8.,  9.],
               [10., 11.],
               [ 4.,  3.],
                [ 2.,  1.]]]),
      tensor([[[ 0.,  1.,  2.,  1.],
                [ 2.,  3.,  4.,  3.]],

             [[ 4.,  5.,  1.,  2.],
                [ 6.,  7.,  3.,  4.]],

              [[ 8.,  9.,  4.,  3.],
               [10., 11.,  2.,  1.]]]))
```

有时，我们想[**通过*逻辑运算符*构建二元张量**]。 以`X == Y`为例： 对于每个位置，如果`X`和`Y`在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句`X == Y`在该位置处为真，否则该位置为0。

```python
>>> X == Y
    tensor([[False,  True, False,  True],
            [False, False, False, False],
            [False, False, False, False]])
```

[**对张量中的所有元素进行求和，会产生一个单元素张量。**]

```python
>>> X.sum()
    tensor(66.)
```

#### 1.3 广播机制

在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。在某些情况下，[**即使形状不同，我们仍然可以通过调用*广播机制*（broadcasting mechanism）来执行按元素操作**]。这种机制的工作条件是：两个张量从后开始数，每个维度相等或者其中一个为1。这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：

```python
>>> a = torch.arange(3).reshape((3, 1))
>>> b = torch.arange(2).reshape((1, 2))
>>> a, b, a + b
    (tensor([[0],
              [1],
              [2]]),
      tensor([[0, 1]]))
      tensor([[0, 1],
             [1, 2],
             [2, 3]])
```

由于`a`和`b`分别是3\times1和1\times2矩阵，如果让它们相加，它们的形状不匹配。我们将两个矩阵**广播**为一个更大的3\times2矩阵，矩阵`a`复制列，矩阵`b`复制行，然后再按元素相加。需要注意的是，广播机制只能扩展维度，而不能凭空增加张量的维度，例如在计算沿某个轴的均值时，若张量维度不同，则会报错：

```python
>>> C = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4)
>>> C / C.sum(axis=1)
    RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1
```

此时我们需要将`keepdims`设为True，才能正确利用广播机制扩展`C.sum(axis=1)`的维度：

```python
>>> C / C.sum(axis=1, keepdims=True)
    tensor([[[0.0000, 0.0667, 0.1111, 0.1429],
              [0.3333, 0.3333, 0.3333, 0.3333],
              [0.6667, 0.6000, 0.5556, 0.5238]],

            [[0.2500, 0.2549, 0.2593, 0.2632],
              [0.3333, 0.3333, 0.3333, 0.3333],
              [0.4167, 0.4118, 0.4074, 0.4035]]])
```

#### 1.4 索引和切片

就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1；可以指定范围以包含第一个元素和最后一个之前的元素。如下所示，我们[**可以用`[-1]`选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素**]：

```python
>>> X[-1], X[1:3]
    (tensor([ 8.,  9., 10., 11.]),
      tensor([[ 4.,  5.,  6.,  7.],
              [ 8.,  9., 10., 11.]]))
```

我们[**可以用`[::2]`每间隔一个元素选择一个元素，可以用`[::3]`每间隔两个元素选择一个元素**]：

```python
>>> X[::2, ::3]
    tensor([[ 0.,  3.],
              [ 8., 11.]])
```

[**除读取外，我们还可以通过指定索引来将元素写入矩阵。**]

```python
>>> X[1, 2] = 9
    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  9.,  7.],
                [ 8.,  9., 10., 11.]])
```

如果我们想[**为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。**]例如，`[0:2, :]`访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。

```python
>>> X[0:2, :] = 12
    tensor([[12., 12., 12., 12.],
            [12., 12., 12., 12.],
                [ 8.,  9., 10., 11.]])
```

#### 1.5 节约内存

[**如果在后续计算中没有重复使用`X`，我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。**]

```python
>>> before = id(X)
>>> X += Y
>>> id(X) == before
    True
```

#### 1.6 转换为其他Python对象

将深度学习框架定义的张量[**转换为NumPy张量（`ndarray`）**]很容易，反之也同样容易。torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。

```python
>>> A = X.numpy()
>>> B = torch.tensor(A)
>>> type(A), type(B)
    (numpy.ndarray, torch.Tensor)
```

要(**将大小为1的张量转换为Python标量**)，我们可以调用`item`函数或Python的内置函数。

```python
>>> a = torch.tensor([3.5])
>>> a, a.item(), float(a), int(a)
    (tensor([3.5000]), 3.5, 3.5, 3)
```

### 2. 数据预处理

为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始，而不是从那些准备好的张量格式数据开始。在Python中常用的数据分析工具中，我们通常使用`pandas`软件包。像庞大的Python生态系统中的许多其他扩展包一样，`pandas`可以与张量兼容。本节我们将简要介绍使用`pandas`预处理原始数据，并将原始数据转换为张量格式的步骤。

#### 2.1 读取数据集

举一个例子，我们首先(**创建一个人工数据集，并存储在CSV（逗号分隔值）文件**)`../data/house_tiny.csv`中。以其他格式存储的数据也可以通过类似的方式进行处理。下面我们将数据集按行写入CSV文件中。

```python
>>> import os
>>> os.makedirs(os.path.join('..', 'data'), exist_ok=True)
>>> data_file = os.path.join('..', 'data', 'house_tiny.csv')
>>> with open(data_file, 'w') as f:
>>>     f.write('NumRooms,Alley,Price\n')  # 列名
>>>     f.write('NA,Pave,127500\n')  # 每行表示一个数据样本
>>>     f.write('2,NA,106000\n')
>>>     f.write('4,NA,178100\n')
>>>     f.write('NA,NA,140000\n')
```

要[**从创建的CSV文件中加载原始数据集**]，我们导入`pandas`包并调用`read_csv`函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）。

```python
>>> import pandas as pd
>>> data = pd.read_csv(data_file)
>>> print(data)
```

|     | NumRooms | Alley | Price  |
| --- | -------- | ----- | ------ |
| 0   | NaN      | Pave  | 127500 |
| 1   | 2.0      | NaN   | 106000 |
| 2   | 4.0      | NaN   | 178100 |
| 3   | NaN      | NaN   | 140000 |

#### 2.2 处理缺失值

“NaN”项代表缺失值。[**为了处理缺失的数据，典型的方法包括*插值法*和*删除法*，**]其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`，其中前者为`data`的前两列，而后者为`data`的最后一列。对于`inputs`中缺少的数值，我们用同一列的均值替换“NaN”项。

```python
>>> inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
>>> inputs = inputs.fillna(inputs.mean())
>>> print(inputs)
```

|     | NumRooms | Alley |
| --- | -------- | ----- |
| 0   | 3.0      | Pave  |
| 1   | 2.0      | NaN   |
| 2   | 4.0      | NaN   |
| 3   | 3.0      | NaN   |

利用删除法，我们删除缺失元素最多的一个样本。首先，`data.isnull()`矩阵统计每个元素是否缺失，之后在轴-1的方向上将`data.isnull()`元素求和，得到每个样本缺失元素个数，取得缺失元素个数最大的样本的序号，并将其删除。

```python
>>> nan_numer = data.isnull().sum(axis=1)
>>> nan_max_id = nan_numer.idxmax()
>>> data_delete = data.drop([nan_max_id], axis=0)
```

|     | NumRooms | Alley | Price  |
| --- | -------- | ----- | ------ |
| 0   | NaN      | Pave  | 127500 |
| 1   | 2.0      | NaN   | 106000 |
| 2   | 4.0      | NaN   | 178100 |

一般情况下，利用`dropna`删除数据，其中•Axis哪个维度How如何删除，‘any’表示有nan即删除，‘all’表示全为nan删除，Thresh有多少个nan删除，Subset在哪些列中查找nan，Inplace是否原地修改。

```python
dropna( axis=0, how=‘any’, thresh=None, subset=None, inplace=False)
```

[**对于`inputs`中的类别值或离散值，我们将“NaN”视为一个类别。**]由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，`pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。

```python
>>> inputs = pd.get_dummies(inputs, dummy_na=True)
>>> print(inputs)
```

|     | NumRooms | Alley_Pave | Alley_nan |
| --- | -------- | ---------- | --------- |
| 0   | 3.0      | 1          | 0         |
| 1   | 2.0      | 0          | 1         |
| 2   | 4.0      | 0          | 1         |
| 3   | 3.0      | 0          | 1         |

#### 2.3 转换为张量格式

[**现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式。**]

```python
>>> import torch
>>> X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
    (tensor([[3., 1., 0.],
              [2., 0., 1.],
              [4., 0., 1.],
              [3., 0., 1.]], dtype=torch.float64),
      tensor([127500, 106000, 178100, 140000]))
```

### 3. Q&A

**`Q1：reshape和view的区别？`**

> View为浅拷贝，只能作用于连续型张量；Contiguous函数将张量做深拷贝并转为连续型；Reshape在张量连续时和view相同，不连续时等价于先contiguous再view。

**`Q2：数组计算吃力怎么办？`**

> 学习numpy的知识。

**`Q3：如何快速区分维度？`**

> 利用`a.shape`或`a.dim()`。

**`Q4：Tensor和Array有什么区别？`**

> Tensor是数学上定义的张量，Array是计算机概念数组，但在深度学习中有时将Tensor视为多维数组。

**`Q5：新分配了y的内存，那么之前y对应的内存会自动释放吗？`**

> Python会在不需要时自动释放内存。

****<mark>整合版来自→公众号：坚持打代码 </mark>****

## 05-线性代数

### 本节目录

- [1. 线性代数基础知识](#1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86)
  - [1.1 标量](#11-%E6%A0%87%E9%87%8F)
  - [1.2 向量](#12-%E5%90%91%E9%87%8F)
  - [1.3 矩阵](#13-%E7%9F%A9%E9%98%B5)
    - [1.3.1 矩阵的操作](#131-%E7%9F%A9%E9%98%B5%E7%9A%84%E6%93%8D%E4%BD%9C)
    - [1.3.2 特殊矩阵](#132-%E7%89%B9%E6%AE%8A%E7%9F%A9%E9%98%B5)
    - [1.3.3 特征向量和特征值](#133-%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E5%92%8C%E7%89%B9%E5%BE%81%E5%80%BC)
- [2. 线性代数实现](#2-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AE%9E%E7%8E%B0)
  - [2.1 标量](#21-%E6%A0%87%E9%87%8F)
  - [2.2 向量](#22-%E5%90%91%E9%87%8F)
  - [2.3 矩阵](#23-%E7%9F%A9%E9%98%B5)
    - [2.3.1 创建](#231-%E5%88%9B%E5%BB%BA)
    - [2.3.2 转置](#232-%E8%BD%AC%E7%BD%AE)
    - [2.3.3 reshape](#233-reshape)
    - [2.3.4 clone](#234-clone)
    - [2.3.5 sum](#235-sum)
    - [2.3.6 numel](#236-numel)
    - [2.3.7 mean](#237-mean)
    - [2.3.8 dot](#238-dot)
    - [2.3.9 mm、mv](#239-mmmv)
    - [2.3.10 L1、L2、F范数](#2310--l1l2f%E8%8C%83%E6%95%B0)
    - [2.3.11 运算](#2311--%E8%BF%90%E7%AE%97)
    - [2.3.12 广播](#2312-%E5%B9%BF%E6%92%AD)

### 1. 线性代数基础知识

这部分主要是由标量过渡到向量，再从向量拓展到矩阵操作，重点在于理解矩阵层面上的操作（都是大学线代课的内容，熟悉的可以自动忽略）

#### 1.1 标量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-01.png)

#### 1.2 向量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-02.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-03.png)

#### 1.3 矩阵

##### 1.3.1 矩阵的操作

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-04.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-05.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-06.png)

​ ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-07.png)

(矩阵范数麻烦且不常用，一般用F范数)

##### 1.3.2 特殊矩阵

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-08.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-09.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-10.png)

​ (深度学习里基本不会涉及到正定、置换矩阵，这里明确个概念就行)

##### 1.3.3 特征向量和特征值

- 数学定义：设A是n阶方阵，如果存在常数![img](https://images0.cnblogs.com/blog/650633/201407/141700142243782.png)及非零n向量x，使得![img](https://images0.cnblogs.com/blog/650633/201407/141700145536982.png)，则称![img](https://images0.cnblogs.com/blog/650633/201407/141700149439396.png)是矩阵A的特征值，x是A属于特征值![img](https://images0.cnblogs.com/blog/650633/201407/141700153035353.png)的特征向量

- 直观理解：不被矩阵A改变方向的向量x就是A的一个特征向量
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/05/05-11.png)

- 矩阵不一定有特征向量，但是对称矩阵总是可以找到特征向量

### 2. 线性代数实现

这部分主要是应用pytorch实现基本矩阵操作，同样由标量过渡到向量最后拓展到矩阵

#### 2.1 标量

```python
import torch    # 应用pytorch框架

# 标量由只有一个元素的张量表示
x = torch.tensor([3.0])     # 单独一个数字表示标量也可以
y = torch.tensor([2.0])     # 单独一个数字表示标量也可以
print(x + y)    # tensor([5.])
print(x * y)    # tensor([6.])
print(x / y)    # tensor([1.5000])
print(x ** y)   # tensor([9.]) 指数运算
```

#### 2.2 向量

```python
# 向量可以看作是若干标量值组成的列表
x = torch.arange(4)     # tensor([0, 1, 2, 3])
                        # 生成[0, 4)范围内所有整数构成的张量tensor
print(x[3])             # tensor(3)
                        # 和列表相似，通过张量的索引访问元素
print(len(x))           # 4
                        # 获取张量x的长度
print(x.shape)          # torch.Size([4])
                        # 获取张量形状，这里x是只有一个轴的张量因此形状只有一个元素
```

#### 2.3 矩阵

##### 2.3.1 创建

```python
A = torch.arange(6)     # tensor([0, 1, 2, 3, 4, 5])
B = torch.tensor([[1,2,3],[2,0,4],[3,4,5]])
C = torch.tensor([[[1,2,3],
                   [4,5,6],
                   [7,8,9]],
                  [[0,0,0],
                   [1,1,1],
                   [2,2,2]]])
D = torch.arange(20, dtype=torch.float32)
```

##### 2.3.2 转置

```python
A = torch.arange(6)     # tensor([0, 1, 2, 3, 4, 5])
A = A.reshape(3,2)      # tensor([[0, 1],
                        #         [2, 3],
                        #         [4, 5]])

A = A.T                 # 转置 A.T
                        # tensor([[0, 2, 4],
                        #         [1, 3, 5]])
```

##### 2.3.3 reshape

```python
# 使用reshape方法创建一个形状为3 x 2的矩阵A
A = torch.arange(6)     # tensor([0, 1, 2, 3, 4, 5])
A = A.reshape(3,2)      # tensor([[0, 1],
                        #         [2, 3],
                        #         [4, 5]])
```

*<u>tips（确定矩阵shape)：*</u>

<u><em>由外层到内层依次去中括号，并记下去掉中括号后此时元素的个数，任选其中一个元素重复上述去括号的操作直到该元素中无中括号，记下的数字从左到右依次排序中间用x连接即为矩阵shape</em></u>

##### 2.3.4 clone

```python
A = torch.arange(20, dtype=torch.float32)
A = A.reshape(5,4)
B = A.clone()   # 通过分配新内存，将A的一个副本分给B，该边B并不影响A的值
print(B)        # tensor([[ 0.,  1.,  2.,  3.],
                #         [ 4.,  5.,  6.,  7.],
                #         [ 8.,  9., 10., 11.],
                #         [12., 13., 14., 15.],
                #         [16., 17., 18., 19.]])
```

##### 2.3.5 sum

```python
A = torch.tensor([[[1,2,3],
                   [4,5,6],
                   [7,8,9]],
                  [[0,0,0],
                   [1,1,1],
                   [2,2,2]]])
print(A.shape)
# torch.Size([2, 3, 3])

print(A.sum())
# tensor(54)

print(A.sum(axis=0))
"""
tensor([[ 1,  2,  3],
        [ 5,  6,  7],
        [ 9, 10, 11]])
"""
print(A.sum(axis=0, keepdims=True))
"""
tensor([[[ 1,  2,  3],
         [ 5,  6,  7],
         [ 9, 10, 11]]])
"""

print(A.sum(axis=1))
"""
tensor([[12, 15, 18],
        [ 3,  3,  3]])
"""
print(A.sum(axis=1, keepdims=True))
"""
tensor([[[12, 15, 18]],

        [[ 3,  3,  3]]])
"""

print(A.sum(axis=2))
"""
tensor([[ 6, 15, 24],
        [ 0,  3,  6]])
"""
print(A.sum(axis=2, keepdims=True))
"""
tensor([[[ 6],
         [15],
         [24]],

        [[ 0],
         [ 3],
         [ 6]]])
"""

print(A.sum(axis=[0,1]))
# tensor([15, 18, 21])

print(A.sum(axis=[0,1], keepdims=True))
# tensor([[[15, 18, 21]]])
```

##### 2.3.6 numel

```python
A = torch.tensor([[0.,0.,0.],[1.,1.,1.]])
print(A.numel())    # 6 元素个数
```

##### 2.3.7 mean

```python
A = torch.tensor([[0.,0.,0.],[1.,1.,1.]])
print(A.numel())    # 6 元素个数
print(A.sum())      # tensor(3.)
print(A.mean())     # tensor(0.5000)

# 特定轴
A = torch.tensor([[0.,0.,0.],[1.,1.,1.]])
print(A.shape[0])       # 2
print(A.sum(axis=0))    # tensor([1., 1., 1.])
print(A.mean(axis=0))   # tensor([0.5000, 0.5000, 0.5000])    平均值
```

##### 2.3.8 dot

```python
x = torch.tensor([0.,1.,2.,3.])
y = torch.tensor([1.,1.,1.,1.])
print(torch.dot(x, y))  # tensor(6.)
```

##### 2.3.9 mm、mv

```python
A = torch.tensor([[0,1,2],
                  [3,4,5]])
B = torch.tensor([[2,2],
                  [1,1],
                  [0,0]])
x = torch.tensor([3,3,3])

print(torch.mv(A, x))
"""
向量积
tensor([ 9, 36])
"""

print(torch.mm(A, B))
"""
矩阵积
tensor([[ 1,  1],
        [10, 10]])
"""
```

##### 2.3.10 L1、L2、F范数

```python
x = torch.tensor([3.0, -4.0])
print(torch.abs(x).sum())   # 向量的L1范数: tensor(7.)  x中的每个元素绝对值的和
print(torch.norm(x))        # 向量的L2范数: tensor(5.)  x中的每个元素平方的和开根号

A = torch.ones((4, 9))
print(torch.norm(A))        # 矩阵的F范数:  tensor(6.)  A中的每个元素平方的和开根号
```

##### 2.3.11 运算

```python
A = torch.arange(20, dtype=torch.float32)
A = A.reshape(5,4)
B = A.clone()   

print(B)        # tensor([[ 0.,  1.,  2.,  3.],
                #         [ 4.,  5.,  6.,  7.],
                #         [ 8.,  9., 10., 11.],
                #         [12., 13., 14., 15.],
                #         [16., 17., 18., 19.]])

print(A == B)
"""
tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])
"""

print(A + B)
"""
tensor([[ 0.,  2.,  4.,  6.],
        [ 8., 10., 12., 14.],
        [16., 18., 20., 22.],
        [24., 26., 28., 30.],
        [32., 34., 36., 38.]])
"""


print(A * B)
"""
tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
"""
```

##### 2.3.12 广播

```python
A = torch.tensor([[1.,2.,3.],
                  [4.,5.,6.]])
B = A.sum(axis=1, keepdims=True)

print(B)
"""
tensor([[ 6.],
        [15.]])
"""

print(A / B)
"""
tensor([[0.1667, 0.3333, 0.5000],
        [0.2667, 0.3333, 0.4000]])
"""

print(A + B)
"""
tensor([[ 7.,  8.,  9.],
        [19., 20., 21.]])
"""

print(A * B)
"""
tensor([[ 6., 12., 18.],
        [60., 75., 90.]])
"""
```

**<mark>整合版来自→公众号：坚持打代码</mark>**

## 06-矩阵计算

### 本节目录

- [1. 导数的概念及几何意义](#1-%E5%AF%BC%E6%95%B0%E7%9A%84%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%87%A0%E4%BD%95%E6%84%8F%E4%B9%89)
  - [1.1 标量导数](#11-%E6%A0%87%E9%87%8F%E5%AF%BC%E6%95%B0)
  - [1.2 亚导数](#12-%E4%BA%9A%E5%AF%BC%E6%95%B0)
- [2. 函数与标量，向量，矩阵](#2-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A0%87%E9%87%8F%E5%90%91%E9%87%8F%E7%9F%A9%E9%98%B5)
  - [2.1 f 为是一个标量](#21-f-%E4%B8%BA%E6%98%AF%E4%B8%80%E4%B8%AA%E6%A0%87%E9%87%8F)
    - [2.1.1 input是一个标量](#211-input%E6%98%AF%E4%B8%80%E4%B8%AA%E6%A0%87%E9%87%8F)
    - [2.1.2 input是一个向量](#212-input%E6%98%AF%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F)
    - [2.1.3 input是一个矩阵](#213-input%E6%98%AF%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5)
  - [2.2 f是一个向量](#22-f%E6%98%AF%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F)
    - [2.2.1 input是一个标量](#221-input%E6%98%AF%E4%B8%80%E4%B8%AA%E6%A0%87%E9%87%8F)
    - [2.2.2 input是一个向量](#222-input%E6%98%AF%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F)
    - [2.2.3 input是一个矩阵](#223-input%E6%98%AF%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5)
  - [2.3 F是一个矩阵](#23-f%E6%98%AF%E4%B8%80%E4%B8%AA%E7%9F%A9%E9%98%B5)
    - [2.3.1 input是一个标量](#231-input%E6%98%AF%E4%B8%80%E4%B8%AA%E6%A0%87%E9%87%8F)
    - [2.3.2 input是一个向量](#232-input%E6%98%AF%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F)
    - [2.3.3 input是一个向量](#233-input%E6%98%AF%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F)
- [3. 求导的本质](#3-%E6%B1%82%E5%AF%BC%E7%9A%84%E6%9C%AC%E8%B4%A8)
- [4. 矩阵求导的布局](#4-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E7%9A%84%E5%B8%83%E5%B1%80)
  - [4.1 分子布局](#41-%E5%88%86%E5%AD%90%E5%B8%83%E5%B1%80)
  - [4.2 分母布局](#42-%E5%88%86%E6%AF%8D%E5%B8%83%E5%B1%80)

### 1. 导数的概念及几何意义

#### 1.1 标量导数

- 导数是切线的斜率

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-01.png)

- 指向值变化最大的方向

![image7](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-02.png)

#### 1.2 亚导数

- 将导数拓展到不可微的函数，在不可导的点的导数可以用一个范围内的数表示

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-03.png)

### 2. 函数与标量，向量，矩阵

该部分结合课程视频和参考文章进行总结（参考了知乎文章：[矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/263777564)）

- 当f，input为不同形式时，f(input)结果的表达形式

#### 2.1 f 为是一个标量

##### 2.1.1 input是一个标量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-04.png)

##### 2.1.2 input是一个向量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-05.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-06.png)

##### 2.1.3 input是一个矩阵

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-07.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-08.png)

#### 2.2 f是一个向量

- **f**是由若干个f(标量)组成的向量

##### 2.2.1 input是一个标量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-09.png)

##### 2.2.2 input是一个向量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-10.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-11.png)

##### 2.2.3 input是一个矩阵

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-12.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-13.png)

#### 2.3 F是一个矩阵

- **F**是一个由若干**f**组成的一个矩阵

##### 2.3.1 input是一个标量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-14.png)

##### 2.3.2 input是一个向量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-15.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-16.png)

##### 2.3.3 input是一个向量

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-17.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-18.png)

### 3. 求导的本质

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-19.png)

**可以将f对x1，x2，x3的偏导分别求出来，即**

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-20.png)

- 矩阵求导也是一样的，**本质就是** ![公式](https://www.zhihu.com/equation?tex=%5Ctext%7Bfunction%7D) 中的**每个** ![公式](https://www.zhihu.com/equation?tex=f) **分别对变元中的每个元素逐个求偏导，只不过写成了向量、矩阵形式而已。**

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-21.png)

（课上是按行向量展开的）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-22.png)

**X为矩阵时**，先把矩阵变元 ![公式](https://www.zhihu.com/equation?tex=%5Cpmb%7BX%7D) 进行**转置**，再对**转置后**的**每个位置**的元素逐个求偏导，结果布局和**转置布局一样**。（课上讲的是这种展开方式）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-23.png)

- 所以，如果 ![公式](https://www.zhihu.com/equation?tex=%5Ctext%7Bfunction%7D) 中有 ![公式](https://www.zhihu.com/equation?tex=m) 个 ![公式](https://www.zhihu.com/equation?tex=f) (标量)，变元中有 ![公式](https://www.zhihu.com/equation?tex=n) 个元素，那么，每个 ![公式](https://www.zhihu.com/equation?tex=f) 对变元中的每个元素逐个求偏导后，我们就会产生 ![公式](https://www.zhihu.com/equation?tex=m+%5Ctimes+n) 个结果。

### 4. 矩阵求导的布局

- 经过上述对求导本质的推导，关于矩阵求导的问题，实质上就是对求导结果的进一步排布问题 **对于2.2（f为向量，input也为向量）中的情况，其求导结果有两种排布方式，一种是`分子布局`，一种是`分母布局`**

##### 4.1 分子布局

**分子布局**，就是分子是**列向量**形式，分母是**行向量**形式 （课上讲的）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-24.png)

##### 4.2 分母布局

2.**分母布局**，就是分母是**列向量**形式，分子是**行向量**形式

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/06/06-25.png)

**将求导推广到矩阵，由于矩阵可以看作由多个向量所组成，因此对矩阵的求导可以看作先对每个向量进行求导，然后再增加一个维度存放求导结果。**

- 例如当F为矩阵，input为矩阵时，F中的每个元素f(标量）求导后均为一个矩阵（按照课上的展开方式），因此每个**f**（包含多个f（标量））求导后为存放多个矩阵的三维形状，再由于矩阵F由多个**f**组成，因此F求导后为存放多个**f**求导结果的四维形状。 **对于不同f和input求导后的维度情况总结如下图所示（课程中的截图）**

**<mark>整合版来自→公众号：坚持打代码</mark>** 

## 07-链式法则与自动求导

### 本节目录

- [1. 向量链式法则](#1-%E5%90%91%E9%87%8F%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99)
  - [例1（标量对向量求导）](#%E4%BE%8B1%E6%A0%87%E9%87%8F%E5%AF%B9%E5%90%91%E9%87%8F%E6%B1%82%E5%AF%BC)
  - [例2（涉及到矩阵的情况）](#%E4%BE%8B2%E6%B6%89%E5%8F%8A%E5%88%B0%E7%9F%A9%E9%98%B5%E7%9A%84%E6%83%85%E5%86%B5)
- [2. 自动求导](#2-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC)
  - [2.1 计算图](#21-%E8%AE%A1%E7%AE%97%E5%9B%BE)
  - [2.2 自动求导的两种模式](#22-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F)
  - [2.3 复杂度比较](#23-%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%AF%94%E8%BE%83)
- [3. 代码部分](#3-%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86)
- [4. 自动求导 Q&A](#4-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC-qa)
- [5. 练习](#5-%E7%BB%83%E4%B9%A0)

### 1. 向量链式法则

- #### 1.1 标量链式法则

- #### 1.2 拓展到向量
  
  > 需要注意维数的变化
  > 
  > 下图三种情况分别对应：
  > 
  > 1. y为标量，x为向量
  > 2. y为标量，x为矩阵
  > 3. y、x为矩阵

---

##### 例1（标量对向量求导）

> 这里应该是用分子布局，所以是X转置

​ 

##### 例2（涉及到矩阵的情况）

> X是mxn的矩阵,w为n维向量，y为m维向量；
> z对Xw-y做L2 norm,为标量；
> 过程与例一大体一致；

​ 

---

> 由于在神经网络动辄几百层，手动进行链式求导是很困难的，因此我们需要借助自动求导

---

### 2. 自动求导

- 含义：计算一个函数在指定值上的导数

- 自动求导有别于
  
  - 符号求导
  
  - 数值求导

为了更好地理解自动求导，下面引入计算图的概念

#### 2.1 计算图

- 将代码分解成操作子

- 将计算表示成一个**无环图**
  
  > 下图自底向上其实就类似于链式求导过程

- 计算图有两种构造方式
  
  - 显示构造
    
    > 可以理解为先定义公式再代值
    > 
    > Tensorflow/Theano/MXNet
  
  - 隐式构造
    
    > 系统将所有的计算记录下来
    > 
    > Pytorch/MXNet

#### 2.2 自动求导的两种模式

- 正向累积

- 反向累积（反向传递back propagation）

​ **反向累积计算过程**

> 反向累积的正向过程：自底向上，需要存储中间结果
> 
> 反向累积的反向过程：自顶向下，可以去除不需要的枝（图中的x应为w）
> 
> ​

#### 2.3 复杂度比较

- 反向累积
  
  - 时间复杂度：O(n),n是操作子数
    - 通常正向和反向的代价类似
  - 空间复杂度：O(n)
    - 存储正向过程所有的中间结果

- 正向累积
  
  > 每次计算一个变量的梯度时都需要将所有节点扫一遍
  
  - 时间复杂度：O(n)
  - 空间复杂度：O(1)

---

### 3. 代码部分

```python
#对y = x.Tx关于列向量x求导
import torch

x = torch.arange(4.0)
x
```

```
tensor([0., 1., 2., 3.])
```

```python
#存储梯度
x.requires_grad_(True)#等价于x = torch.arange(4.0,requires_grad=True)
x.grad#默认值是None
```

```python
y = torch.dot(x,x)
y
#PyTorch隐式地构造计算图，grad_fn用于记录梯度计算
```

```
tensor(14., grad_fn=<DotBackward0>)
```

**通过调用反向传播函数来自动计算y关于x每个分量的梯度**

```python
y.backward()
x.grad
```

```
tensor([0., 2., 4., 6.])
```

```python
x.grad==2*x#验证
```

```
tensor([True, True, True, True])
```

```python
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()#如果没有这一步结果就会加累上之前的梯度值，变为[1,5,9,13]
y = x.sum()
y.backward()
x.grad
```

```
tensor([1., 1., 1., 1.])
```

```python
x.grad.zero_()
y=x*x#哈达玛积，对应元素相乘

#在深度学习中我们一般不计算微分矩阵
#而是计算批量中每个样本单独计算的偏导数之和

y.sum().backward()#等价于y.backword(torch.ones(len(x)))
x.grad
```

```
tensor([0., 2., 4., 6.])
```

**将某些计算移动到记录的计算图之外**

```python
# 后可用于用于将神经网络的一些参数固定住
x.grad.zero_()
y = x*x
u = y.detach()#把y当作常数
z = u*x

z.sum().backward()
x.grad == u
```

```
tensor([True, True, True, True])
```

```python
x.grad.zero_()
y.sum().backward()
x.grad == 2*x
```

```
tensor([True, True, True, True])
```

**即使构建函数的计算图需要用过Python控制流，仍然可以计算得到的变量的梯度**

**这也是隐式构造的优势，因为它会存储梯度计算的计算图，再次计算时执行反向过程就可以**

```python
def f(a):
    b = a * 2
    while b.norm()<1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c

a = torch.randn(size=(),requires_grad=True)
d = f(a)
d.backward()

a.grad == d / a
```

---

### 4. 自动求导 Q&A

**`Q1：ppt上隐式构造和显式构造看起来为啥差不多？`**

> 显式和隐式的差别其实就是数学上求梯度和python求梯度计算上的差别，不用深究
> 
> 显式构造就是我们数学上正常求导数的求法，先把所有求导的表达式选出来再代值

**`Q2:需要正向和反向都算一遍吗？`**

> 需要正向先算一遍，自动求导时只进行反向就可以，因为正向的结果已经存储

**`Q3:为什么PyTorch会默认累积梯度`**

> 便于计算大批量；方便进一步设计

**`Q4:为什么深度学习中一般对标量求导而不是对矩阵或向量求导`**

> loss一般都是标量

**`Q5:为什么获取.grad前需要backward`**

> 相当于告诉程序需要计算梯度，因为计算梯度的代价很大，默认不计算

**`Q6:pytorch或mxnet框架设计上可以实现矢量的求导吗`**

> 可以

### 5. 练习

**1.为什么计算二阶导数比一阶导数的开销要更大？**

二阶导数是在一阶导数的基础上进行的，开销自然更大

**2.在运行反向传播函数之后，立即再次运行它，看看会发生什么。**

"RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."

说明不能连续两次运行,pytorch使用的是动态计算图,反向传播函数运行一次后计算图就被释放了

**只需要在函数接口将参数retain_graph设为True即可**

In [51]:

```
def f(a):
    b = a * 2
    while b.norm()<1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
a.grad.zero_()
a = torch.randn(size=(),requires_grad=True)#size=0表示a是标量
d = f(a)
#d.backward(retain_graph=True)
#a.grad
d.backward()
a.grad
```

Out[51]:

```
tensor(4096.)
```

**3.在控制流的例子中，我们计算d关于a的导数，如果我们将变量a更改为随机向量或矩阵，会发生什么？此时，计算结果f(a)不再是标量。结果会发生什么？我们如何分析这个结果？**

backward函数的机制本身不允许张量对张量求导，如果输入是向量或矩阵，需要将其在各个分量上求和，变为标量；所以还需要传入一个与输入同型的张量

In [53]:

```
def f(a):
    b = a * 2
    while b.norm()<1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
a.grad.zero_()
a = torch.randn(10,requires_grad=True)
d = f(a)
#d.backward(retain_graph=True)
#a.grad
#d.backward()#RuntimeError: grad can be implicitly created only for scalar outputs
d.sum().backward()#需要加上.sum()否则会报错 
a.grad
```

Out[53]:

```
tensor([51200., 51200., 51200., 51200., 51200., 51200., 51200., 51200., 51200.,
        51200.])
```

**4.重新设计一个求控制流梯度的例子。运行并分析结果。**

In [56]:

```
def h(x):
    y = x * x
    while y.norm() < 2500:
        y = y * 2
    if y.sum() < 0:
        c = 100*y
    else:
        c = y
    return c
x.grad.zero_()
x = torch.randn(size=(),requires_grad=True)
y = h(x)
y.backward()
x.grad
```

Out[56]:

```
tensor(-3311.5398)
```

**5.使f(x)=sin(x)，绘制f(x)和df(x)/dx的图像，其中后者不使用f'(x)=\cos(x)。**

In [66]:

```
import matplotlib.pyplot as plt
x = torch.arange(-20,20,0.1,requires_grad=True,dtype=torch.float32)
y = torch.sin(x)
y.sum().backward()
plt.plot(x.detach(),y.detach(),label='y=sinx')
plt.plot(x.detach(),x.grad,label='dy/dx')
plt.legend(loc='lower right')
```

Out[66]:

```
<matplotlib.legend.Legend at 0x1d627d00280>
```

**<mark>整合版来自→公众号：坚持打代码</mark>**

## 08-线性回归+基础优化算法

- [线性回归+基础优化算法](#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)
  - [1.线性回归](#1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92)
  - [2.基础优化算法](#2%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)
  - [3.线性回归的从零开始实现](#3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0)
  - [4.新型回归的简洁实现](#4%E6%96%B0%E5%9E%8B%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0)
  - [5.- 线性回归+基础优化算法](#5--%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)

### 1.线性回归

- 房价预测例子

- **线性模型**
  
  - 输入：x=[x_1,x_2,...,x_n]^T
  
  - 线性模型需要确定一个n维权重和一个标量偏差\omega=[\omega_1,\omega_2,...,\omega_n]^T,b
  
  - 输出 ：y=\omega_1x_1+\omega_2x_2+...+\omega_nx_n+b，
    
    向量版本的是 y=<\omega,x>+b
  
  - 线性模型可以看作是单层神经网络（图片）
    
    > - 神经网络源于神经科学（图片）
    >   - 最早的神经网络是源自神经科学的，但是时至今日，很多神经网络已经远远高于神经科学，可解释性也不是很强，不必纠结

- 衡量估计质量
  
  - 我们需要估计模型的预估值和真实值之间的差距，例如房屋售价和股价
  
  - 假设y是真实值，\tilde{y}是估计值，我们可以比较
    
    l(y,\tilde{y})=\frac{1}{2}(y-\tilde{y})^2，这个叫做**平方损失**

- **训练数据**
  
  - 收集一些数据点来决定参数值（权重\omega和偏差b），例如6个月内被卖掉的房子。
  
  - 这被称之为训练数据
  
  - 通常越多越好。需要注意的是，现实世界的数据都是有限的，但是为了训练出精确的参数往往需要训练数据越多越好，当训练数据不足的时候，我们还需要进行额外处理。
  
  - 假设我们有n个样本，记为
    
    X=[x_1,x_2,...,x_n]^T,y=[y_1,y_2,...y_n]^T
    
    X的每一行是一个样本，y的每一行是一个输出的实数值。

- **参数学习**
  
  - **训练损失**。但我们训练参数的时候，需要定义一个损失函数来衡量参数的好坏，应用前文提过的平方损失有公式：
    
    ​ l(X,x,\omega,b)=\frac{1}{2n}\sum_{i=1}^n(y_i--b)^2=\frac{1}{2n}||y-X\omega-b||^2
  
  - **最小化损失来学习参数**。训练参数的目的就是使损失函数的值尽可能小（这意味着预估值和真实值更接近）。最后求得的参数值可表示为：
    
    \omega^*,b^*=argmin_{\omega,b}l(X,x,\omega,b)

- **显示解**
  
  - 线性回归有显示解，即可以直接矩阵数学运算，得到参数w和b的最优解，而不是用梯度下降，牛顿法等参数优化方式一点点逼近最优解。
  
  - **推导过程**：
    
    - 为了方便矩阵表示和计算，将偏差加入权重，X\gets[X,1],\omega\gets[\omega,b]
    
    - 损失函数是凸函数，最优解满足导数为0，可解出显示解
      
      令\frac{\partial}{\partial\omega} l(X,y,\omega)=0
      
      有\frac{1}{n}(y-X\omega)^TX=0
      
      解得\omega^*=(X^TX)^{-1}X^Ty

- 总结
  
  - 线性回归是对n维输入的加权，外加偏差
  - 使用**平方损失**来衡量预测值和真实值之间的误差
  - **线性回归有显示解**
  - 线性回归可以看作单层神经网络

### 2.基础优化算法

- **梯度下降**
  
  - 当模型没有显示解的时候，应用梯度下降法逼近最优解。
  - 梯度下降法的具体步骤：
    - 挑选一个初始值\omega_0
    - 重复迭代参数，迭代公式为：\omega_t=\omega_{t-1}-\lambda\frac{\partial l}{\partial\omega_{t-1} }
      - **-\frac{\partial l}{\partial\omega_{t-1}}为函数值下降最快的方向，学习率\lambda为学习步长。**
  - 选择学习率
    - 学习率\lambda为学习步长，代表了沿负梯度方向走了多远，这是超参数（人为指定的的值，不是训练得到的）
    - 学习率不能太大，也不能太小，需要选取适当。

- **小批量随机梯度下降**
  
  - 在整个训练集上算梯度太贵了
    
    - 在实际应用中，很少直接应用梯度下降法，这是因为每次更新都需要计算训练集上所有的样本，耗费时间太长。一个深度神经网络模型，迭代一次可能需要数分钟甚至数小时。
  
  - 为了减少运算代价，我们可以==随机采样==b个样本i_1,i_2,...,i_b来近似损失，损失函数为：
    
    ​ \frac{1}{b}\sum_{i\in I_b}l(x_i,y_i,\omega) ,
    
    其中**b是批量大小(batch size)，也是超参数**
  
  - **选择批量大小**
    
    - b也不能太大：内存消耗增加；浪费计算资源，一个极端的情况是可能会重复选取很多差不多的样本，浪费计算资源
    - b也不能太小：每次计算量太小，很难以并行，不能最大限度利用GPU资源

- **总结**
  
  - 梯度下降通过不断**沿着负梯度方向**更新参数求解
  - 小批量随机梯度下降是深度学习默认的求解算法（简单，稳定）
  - **两个重要的超参数：批量大小（batch size），学习率（lr）**

### 3.线性回归的从零开始实现

- 代码

### 4.新型回归的简洁实现

- 代码

### 5.- [线性回归+基础优化算法](#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)

- [1.线性回归](#1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92)

- [2.基础优化算法](#2%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)

- [3.线性回归的从零开始实现](#3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0)

- [4.新型回归的简洁实现](#4%E6%96%B0%E5%9E%8B%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0)

- [5.QA](#5qa)QA

- **1.为什么使用平方损失而不是绝对差值？**
  
  - 其实差别不大，最开始使用平方损失是因为它可导，现在其实都可以使用。

- **2.损失为什么要求平均？**
  
  - 本质上没有关系，但是如果不求平均，梯度的数值会比较大，这时需要学习率除以n。如果不除以n，可能会随着样本数量的增大而让梯度变得很大。

- **3.不管是梯度下降还是随机梯度下降，怎么找到合适的学习率？**
  
  - 选择对学习率不敏感的优化方法，比如Adam
  - 合理参数初始化

- **4.训练过程中，过拟合和欠拟合情况下，学习率和batch_size应该如何调整？**
  
  - 理论上学习率和batch_size对最后的拟合结果不会有影响

- **5.深度学习上，设置损失函数的时候，需要考虑正则吗？**
  
  - 会考虑，但是和损失函数是分开的，深度学习中正则没有太大的用处，有很多其他的技术可以有正则的效果。

- **6.如果样本大小不是批量数的整数倍，需要随机剔除多余的样本吗？**
  
  - 就取多余的样本作为一个批次
  - 直接丢弃
  - 从下一个epoch里面补少的样本

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 09-softmax回归

### 本节目录：

- [09-softmax回归](#09-softmax%E5%9B%9E%E5%BD%92)
  - [本节目录：](#%E6%9C%AC%E8%8A%82%E7%9B%AE%E5%BD%95)
  - [1.回归VS分类：](#1%E5%9B%9E%E5%BD%92vs%E5%88%86%E7%B1%BB)
    - [1.1 从回归到多类分类：](#11-%E4%BB%8E%E5%9B%9E%E5%BD%92%E5%88%B0%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB)
      - [回归：](#%E5%9B%9E%E5%BD%92)
      - [分类：](#%E5%88%86%E7%B1%BB)
      - [均方损失：](#%E5%9D%87%E6%96%B9%E6%8D%9F%E5%A4%B1)
      - [无校验比例](#%E6%97%A0%E6%A0%A1%E9%AA%8C%E6%AF%94%E4%BE%8B)
      - [校验比例](#%E6%A0%A1%E9%AA%8C%E6%AF%94%E4%BE%8B)
    - [1.2 Softmax和交叉熵损失](#12-softmax%E5%92%8C%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1)
  - [2.损失函数](#2%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)
    - [2.1 L2 Loss](#21-l2-loss)
    - [2.2 L1 Loss](#22-l1-loss)
    - [2.3Huber's Robust Loss](#23hubers-robust-loss)
  - [3.图片分类数据集](#3%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86)
    - [3.1 Fashion-MNIST数据集：](#31-fashion-mnist%E6%95%B0%E6%8D%AE%E9%9B%86)
  - [4.从零实现softmax回归](#4%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0softmax%E5%9B%9E%E5%BD%92)
    - [softmax:](#softmax)
  - [5.softmax的简洁实现](#5softmax%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0)
  - [6.softmax回归Q&A](#6softmax%E5%9B%9E%E5%BD%92qa)

### 1.回归VS分类：

- 回归估计一个连续值

- 分类预测一个离散类别
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-01.png)

#### 1.1 从回归到多类分类：

##### 回归：

- 单连续数值输出

- 自然区间R

- 跟真实值的区别作为损失
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-02.png)

##### 分类：

- 通常多个输出

- 输出i是预测为第i类的置信度
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-03.png)

##### 均方损失：

- 对类别进行一位有效编码
  
   y=[y_{1},y_{2},...,y_{n}]^{T}
  )
   y_{i}=\begin{cases}
  1&i=y\
  2&otherwise
  \end{cases}
  )

- 使用均方损失训练

- 最大值为预测
   \hat{y}=\underset {i}{argmax}\quad o^{i}
  )
  
  ##### 无校验比例

- 对类别进行一位有效编码

- 最大值为预测
   \hat{y}=\underset {i}{argmax}\quad o^{i}
  )

- 需要更置信的识别正确类（大余量）
  
  )

##### 校验比例

- 输出匹配概率（非负，和为1）
  

- 概率y和\hat{y}的区别作为损失

#### 1.2 Softmax和交叉熵损失

- 交叉熵用来衡量两个概率的区别H(p,q)=\sum_{i} -p_{i}log(q_i)

- 将它作为损失
  =-\sum_{i}y_{i}log\hat{y_{i}}=-log\hat{y_y}
  )

- 其梯度是真实概率和预测概率的区别
  *{i}-y*{i}
  )

> Softmax回归是一个多类分类模型
> 
> 使用Softmax操作子得到每个类的预测置信度
> 
> 使用交叉熵来衡量和预测标号的区别

### 2.损失函数

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-04.png)

#### 2.1 L2 Loss



![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-05.png)

> 梯度会随着结果逼近而下降

#### 2.2 L1 Loss



![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-06.png)

> 梯度保持不变，但在0处梯度随机

#### 2.3Huber's Robust Loss

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-07.png)

> 结合L1 Loss和L2 Loss的优点

### 3.图片分类数据集

#### 3.1 Fashion-MNIST数据集：

- 读取数据集
  
  ```python
  trans=transforms.ToTensor()
  mnist_train=torchvision.datasets.FashionMNIST(root="../data",train=True,                                              transform=trans,download=True)
  mnist_test=torchvision.datasets.FashionMNIST(root="../data",train=False,                                             transform=trans,download=True)
  ```

- 数据集内图片大小
  
  ```python
  mnist_train[0][0].shape
  torch.Size([1, 28, 28])
  ```
  
  表示图片为单通道（黑白）的28X28的图片

- 显示数据集图像
  
  ```
  X,y = next(iter(data.DataLoader(mnist_train,batch_size=18)))
  show_images(X.reshape(18,28,28),2,9,titles=get_fashion_mnist_labels(y))
  ```
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-08.png)

### 4.从零实现softmax回归

#### softmax:

$$
softmax(X)_{ij}=\frac{exp(X_{ij})}{\sum_{k} exp(X_{ik})}
$$

```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition
```

1. 将图像展平，每个图像看做长度为784的向量，因为数据集有十个类别，所以网络输出维度为10。以此设定参数大小并初始化：
   
   ```python
   num_inputs = 784
   num_outputs = 10
   
   W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
   b = torch.zeros(num_outputs, requires_grad=True)
   ```

2. 实现softmax回归模型：
   
   ```python
   def net(X):
     return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
   ```

3. 实现交叉熵损失函数：
   
   ```python
   def cross_entropy(y_hat, y):
     return - torch.log(y_hat[range(len(y_hat)), y])
   ```

4. 计算正确率：
   
   ```python
   def accuracy(y_hat, y):  
     """计算预测正确的数量"""
     if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
         y_hat = y_hat.argmax(axis=1)
     cmp = y_hat.type(y.dtype) == y
     return float(cmp.type(y.dtype).sum())
   ```

5. 评估net精度
   
   ```python
   def evaluate_accuracy(net, data_iter):  
     """计算在指定数据集上模型的精度"""
     if isinstance(net, torch.nn.Module):
         net.eval()
     metric = Accumulator(2)
     with torch.no_grad():
         for X, y in data_iter:
             metric.add(accuracy(net(X), y), y.numel())
     return metric[0] / metric[1]
   ```
   
   ```python
   class Accumulator:  
     """在n个变量上累加"""
     def __init__(self, n):
         self.data = [0.0] * n
   
     def add(self, *args):
         self.data = [a + float(b) for a, b in zip(self.data, args)]
   
     def reset(self):
         self.data = [0.0] * len(self.data)
   
     def __getitem__(self, idx):
         return self.data[idx]
   ```

6. 定义训练模型：
   
   ```python
   def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  
     """训练模型（定义见第3章）"""
     animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                         legend=['train loss', 'train acc', 'test acc'])
     for epoch in range(num_epochs):
         train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
         test_acc = evaluate_accuracy(net, test_iter)
         animator.add(epoch + 1, train_metrics + (test_acc,))
     train_loss, train_acc = train_metrics
     assert train_loss < 0.5, train_loss
     assert train_acc <= 1 and train_acc > 0.7, train_acc
     assert test_acc <= 1 and test_acc > 0.7, test_acc
   ```

7. 预测：
   
   ```python
   def predict_ch3(net, test_iter, n=6):  
     """预测标签（定义见第3章）"""
     for X, y in test_iter:
         break
     trues = d2l.get_fashion_mnist_labels(y)
     preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
     titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
     d2l.show_images(
         X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])
   
   predict_ch3(net, test_iter)
   ```
   
   ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/09/09-09.png)

### 5.softmax的简洁实现

> 调用torch内的网络层

```python
import torch
from torch import nn
from d2l import torch as d2l
batch_size=256
train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)
net=nn.Sequential(nn.Flatten(),nn.Linear(784,10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight,std=0.01)

net.apply(init_weights)
loss=nn.CrossEntropyLoss()
trainer=torch.optim.SGD(net.parameters(),lr=0.1)
num_epochs=10
d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)
```

### 6.softmax回归Q&A

**Q1:softlabel训练策略以及为什么有效？**

> softmax用指数很难逼近1，softlabel将正例和负例分别标记为0.9和0.1使结果逼近变得可能，这是一个常用的小技巧。

##### Q2:softmax回归和logistic回归？

> logistic回归为二分类问题，是softmax回归的特例

##### Q3:为什么使用交叉熵，而不用相对熵，互信息熵等其他基于信息量的度量？

> 实际上使用哪一种熵的效果区别不大，所以哪种简单就用哪种

##### Q4: 为什么我们只关心正确类，而不关心不正确的类呢？

> 并不是不关心，而是不正确的的类标号为零，所以算式中不体现，如果使用softlabel策略，就会体现出不正确的类。

##### Q5:似然函数曲线是怎么得出来的？有什么参考意义？

> 最小化损失函数也意味着最大化似然函数，似然函数表示统计概率和模型的拟合程度。

##### Q6:在多次迭代之后欧如果测试精度出现上升后再下降是过拟合了吗？可以提前终止吗？

> 很有可能是过拟合，可以继续训练来观察是否持续下降

##### Q7:cnn网络主要学习到的是纹理还是轮廓还是所有内容的综合？

> 目前认为主要学习到的是纹理信息

##### Q8:softmax可解释吗？

> 单纯softmax是可解释的，可以在统计书籍中找到相关的解释。

**<mark>整合版来自→公众号：坚持打代码</mark>**

# 10-多层感知机

- [多层感知机](#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA)
  - [感知机](#%E6%84%9F%E7%9F%A5%E6%9C%BA)
    - [定义](#%E5%AE%9A%E4%B9%89)
    - [训练](#%E8%AE%AD%E7%BB%83)
    - [收敛定理](#%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86)
  - [线性模型的缺陷](#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BC%BA%E9%99%B7)
  - [多层感知机](#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-1)
    - [XOR问题的多层次解决](#xor%E9%97%AE%E9%A2%98%E7%9A%84%E5%A4%9A%E5%B1%82%E6%AC%A1%E8%A7%A3%E5%86%B3)
    - [多层感知机](#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-2)
    - [激活函数](#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)
    - [补充](#%E8%A1%A5%E5%85%85)
  - [练习](#%E7%BB%83%E4%B9%A0)
  - [总结](#%E6%80%BB%E7%BB%93)

## 多层感知机

### 感知机

#### 定义

从现在的观点来看，感知机实际上就是神经网络中的一个神经单元

![感知机](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E6%84%9F%E7%9F%A5%E6%9C%BA.png)

感知机能解决二分类问题，但与线性回归和softmax回归有所区别：线性回归与softmax回归的输出均为实数，softmax回归的输出同时还满足概率公理。

#### 训练

训练感知机的伪代码如下：

```python
initialize w = 0 and b = 0
repeat
    #此处表达式小于0代表预测结果错误
    if y_i[<w,x_i>+b] <= 0 then
        w=w + yixi
        b=b + yi
    end if
until all classified correctly
```

可以看出这等价于使用如下损失函数的随机梯度下降（batch_size=1）: 
$$
\ell(y,\bold x,\bold w)=max(0,-y<\bold w,\bold x>)\
=max(0,-y\bold w^T\bold x)
$$
当预测错误时，偏导数为 
$$
\frac{\partial \ell}{\partial \bold w}=-y\cdot \bold x 
$$

注：此处为了方便计算，将偏置项b归入w中的最后一维，并在特征x中相应的最后一维加入常数1

#### 收敛定理

设数据在特征空间能被半径为r的圆（球）覆盖，并且分类时有余量（即\sigma函数的输入不会取使输出模棱两可的值）y(\bold x^T\bold w)\geq \rho，若初始参数满足\|\bold w\|^2+b^2 \leq 1，则感知机保证在\frac{r^2+1}{\rho ^2}步内收敛

[收敛性的证明](https://zhuanlan.zhihu.com/p/46762820)

### 线性模型的缺陷

在前面的课程中我们学习了softmax回归，线性回归，他们有将输入向量与一个权重向量做内积再与一个偏置相加得到一个值的过程：
$$
O =W^TX+b
$$
这个过程被称为仿射变换，它是一个带有偏置项的线性变换，它最终产生的模型被称为线性模型，线性模型的特点是只能以线性的方式对特征空间进行划分：

![线性化分](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E7%BA%BF%E6%80%A7%E5%88%92%E5%88%86.png)

然而，这种线性划分依赖于线性假设，是非常不可靠的

- 线性假设意味着单调假设，这是不可靠的：
  - 对于人体的体温与健康情况的建模，人体在37℃时最为健康，过小过大均有风险，然而这不是单调的
- 线性假设意味着特征与预测存在线性相关性，这也是不可靠的：
  - 如果预测一个人偿还债务的可能性，那这个人的资产从0万元增至5万元和从100万元增至105万元对应的偿还债务的可能性的增幅肯定是不相等的，也就是不线性相关的
- 线性模型的评估标准是有位置依赖性的，这是不可靠的：
  - 如果需要判断图片中的动物是猫还是狗，对于图片中一个像素的权重的改变永远是不可靠的，因为如果将图片翻转，它的类别不会改变，但是线性模型不具备这种性质，像素的权重将会失效

课程中所提到的例子是XOR问题，即希望模型能预测出XOR分类（分割图片中的一三象限与二四象限）：

![XOR问题](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/XOR%E9%97%AE%E9%A2%98.png)

### 多层感知机

#### XOR问题的多层次解决

仍以XOR问题为例，XOR问题的一个解决思路是分类两次，先按x轴分类为+和-，再按y轴分类为+和-，最后将两个分类结果相乘，+即为一三象限，-即为二四象限：

![多层分类XOR1](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR1.png)

![多层分类XOR2](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%A4%9A%E5%B1%82%E5%88%86%E7%B1%BBXOR2.png)

这实际上将信息进行了多层次的传递：

![XOR信息多层次传递](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/XOR%E4%BF%A1%E6%81%AF%E5%A4%9A%E5%B1%82%E6%AC%A1%E4%BC%A0%E9%80%92.png)

其中蓝色为按X坐标的正负进行的分类，橙色为按Y坐标的正负进行的分类，灰色为将二者信息的综合，这就实现了用多层次的线性模型对非线性进行预测

#### 多层感知机

有了XOR问题的解决经验，可以想到如果将多个感知机堆叠起来，形成具有多个层次的结构，如图：

![单隐藏层](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/10/%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82.png)

这里的模型称为多层感知机，第一层圆圈x_1,x_2,x_3,x_4称为输入（实际上他并非感知机），之后的一层称为隐藏层，由5个感知机构成，他们均以前一层的信息作为输入，最后是输出层，以前一层隐藏层的结果作为输入。除了输入的信息和最后一层的感知机以外，其余的层均称为隐藏层，隐藏层的设置为模型一个重要的超参数，这里的模型有一个隐藏层。

#### 激活函数

但是仅仅有线性变换是不够的，如果我们简单的将多个线性变换按层次叠加，由于线性变换的结果仍为线性变换，所以最终的结果等价于线性变换，与单个感知机并无区别，反而加大了模型，浪费了资源，为了防止这个问题，需要对每个单元（感知机）的输出通过激活函数进行处理再交由下一层的感知机进行运算，这些激活函数就是解决非线性问题的关键。

*激活函数*（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。大多数激活函数都是非线性的。

主要的激活函数有：

##### ReLU函数

最受欢迎的激活函数是*修正线性单元*（Rectified linear
unit，*ReLU*），因为它实现简单，同时在各种预测任务中表现良好。**ReLU提供了一种非常简单的非线性变换**。给定元素x，ReLU函数被定义为该元素与0的最大值：
$$
\operatorname{ReLU}(x) = \max(x, 0)
$$
ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。为了直观感受一下，我们可以画出函数的曲线图。正如从图中所看到，激活函数是分段线性的。使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现的更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题

##### sigmoid函数

**对于一个定义域在\mathbb{R}中的输入，*sigmoid函数*将输入变换为区间(0,1)上的输出**。 因此，sigmoid通常称为*挤压函数*（squashing function）：它将范围（-\infty, \infty）中的任意输入压缩到区间（0,1）中的某个值：
$$
\operatorname{sigmoid}(x) = \frac{1}{1 + e^{-x}}.
$$
在基于梯度的学习中，sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。当我们想要将输出视作二元分类问题的概率时，sigmoid仍然被广泛用作输出单元上的激活函数（你可以将sigmoid视为softmax的特例）。然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的ReLU所取代。

##### tanh函数

与sigmoid函数类似，**tanh(双曲正切)函数也能将其输入压缩转换到区间(-1,1)上**。tanh函数的公式如下： 
$$
\operatorname{tanh}(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}
$$

#### 补充

还可以使用更多隐藏层的感知机和softmax函数解决分类问题

### 练习

1. 证明一个仅使用ReLU（或pReLU）的多层感知机构造了一个连续的分段线性函数。

> 绘制出RELU的图像后，我们可以发现，输出值在经过下一层隐藏层的计算后，如果结果小于等于0，则这个数据被舍弃，结果大于0则被保留，类似一个筛选的过程。相当于上一层的输出经过线性变换后在下一层被筛选，线性变换和上述筛选的过程都是连续的，因此就会产生连续而且分段的结果。

2. 构建多个超参数的搜索方法。

> 有四种主要的策略可用于搜索最佳配置。
> 
> - 试错
> - 网格搜索
> - 随机搜索
> - 贝叶斯优化

详见[超参数搜索不够高效？这几大策略了解一下](https://www.jiqizhixin.com/articles/101401)

3. 权重初始化方法

> 1. 全零初始化：在神经网络中，把w初始化为0是不可以的。这是因为如果把w初始化0，那么每一层的神经元学到的东西都是一样的（输出是一样的），而且在BP的时候，每一层内的神经元也是相同的，因为他们的gradient相同，weight
>    update也相同。
> 2. 随机初始化
> 3. Xavier初始化：保持输入和输出的方差一致（服从相同的分布），这样就避免了所有输出值都趋向于0。
> 4. He
>    initialization：在ReLU网络中，假定每一层有一半的神经元被激活，另一半为0（x负半轴中是不激活的），所以要保持variance不变，只需要在Xavier的基础上再除以2。
> 5. pre-training

详见[权重/参数初始化](https://zhuanlan.zhihu.com/p/72374385)

4. 超参数的调节

> 1. 在mlp中，第一个隐藏的的单元数可能大于输入的个数，每个隐藏层中的单元数由前至后递减，逐渐接近输出的个数。
> 2. 多数情况下，将mlp的深度设置得较深，而每层的单元数相对较少，这样易于训练，不易过拟合，也利于逐步学习样本特征。
> 3. 激活函数种类的选择对训练的影响小于其余的因素。

### 总结

- 多层感知机使用隐藏层和激活函数来得到非线性模型

- 常用激活函数：Sigmoid，Tanh，ReLU

- 使用softmax进行多分类

- 隐藏层数、大小为超参数

**<mark>整合版来自→公众号：坚持打代码</mark>**

# 11-模型选择+过拟合和欠拟合

### 本节目录

- [1. 模型选择](#1-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9)
  - [1.1 实例分析：预测谁会偿还贷款](#11-%E5%AE%9E%E4%BE%8B%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E8%B0%81%E4%BC%9A%E5%81%BF%E8%BF%98%E8%B4%B7%E6%AC%BE)
  - [1.2 训练误差和泛化误差](#12-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE)
  - [1.3 验证数据集和测试数据集](#13-%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86)
  - [1.4 K-则交叉验证](#14-k-%E5%88%99%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81)
  - [1.5 总结](#15-%E6%80%BB%E7%BB%93)
- [2. 过拟合和欠拟合](#2-%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88)
  - [2.1 什么是过拟合和欠拟合？](#21-%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88)
  - [2.2 模型容量](#22-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F)
  - [2.3 模型容量的影响](#23-%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F%E7%9A%84%E5%BD%B1%E5%93%8D)
  - [2.4 估计模型容量](#24-%E4%BC%B0%E8%AE%A1%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F)
  - [2.5 VC维](#25-vc%E7%BB%B4)
  - [2.6 VC维的用处](#26-vc%E7%BB%B4%E7%9A%84%E7%94%A8%E5%A4%84)
  - [2.7 数据复杂度](#27-%E6%95%B0%E6%8D%AE%E5%A4%8D%E6%9D%82%E5%BA%A6)
  - [2.8 总结](#28-%E6%80%BB%E7%BB%93)
- [3. 多项式回归](#3-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92)
  - [3.1 导入库](#31-%E5%AF%BC%E5%85%A5%E5%BA%93)
  - [3.2 生成数据集](#32-%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86)
  - [3.3 NumPyndarray转换为tensor](#33-numpyndarray%E8%BD%AC%E6%8D%A2%E4%B8%BAtensor)
  - [3.4 对模型进行训练和测试](#34-%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95)
  - [3.5 [**三阶多项式函数拟合(正态)**]](#35-%E4%B8%89%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E6%AD%A3%E6%80%81)
  - [3.6 [**线性函数拟合(欠拟合)**]](#36-%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88)
  - [3.7 [**高阶多项式函数拟合(过拟合)**]](#37-%E9%AB%98%E9%98%B6%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88%E8%BF%87%E6%8B%9F%E5%90%88)

### 1. 模型选择

本小节主要介绍了评估模型的一些指标和方法

#### 1.1 实例分析：预测谁会偿还贷款

- 银行雇你来调查谁会偿还贷款，你得到了100个申请人的信息，其中五个人在3年内违约了。然后你惊讶的发现，**所有的五个人在面试时都穿了蓝色衬衫**。显然，你的模型也发现了这个强信号，这会有什么问题？

**答案是，你的模型很有可能会认为所有来面试的人都会穿蓝色衬衫，而这当然是不对的。**

#### 1.2 训练误差和泛化误差

- 训练误差：模型在训练数据上的误差

- 泛化误差：模型在新数据上的误差

- 例子：根据模考成绩来预测未来考试分数
  
  - 在过去的考试中表现很好（**训练误差**）不代表未来会好（**泛化误差**）
  - 学生A通过背书在模考中拿到很好成绩
  - 学生B知道答案后面的原因

- **其中，泛化误差是我们所最关心的**

#### 1.3 验证数据集和测试数据集

- 验证数据集：一个用来评估模型好坏的数据集
  
  - 例如拿出50%的训练数据
  - 不要跟训练数据混在一起（常犯错误）

- 测试数据集：只用一次的数据集。例如：
  
  - 未来的考试
  - 我出价的房子的实际成交价
  - 用在kaggle私有排行榜中的数据集

- **二者最大的区别就是，验证数据集可以那来用很多次，相当于平时的模拟考，而测试数据集则只能用一次来评估模型的性能，相当于最终的考试。**

#### 1.4 K-则交叉验证

- 在没有足够多数据时使用（这是常态）

- 算法：
  
  - 将训练数据分割k块
  - For i = 1，……，k
    - 使用第i块作为验证数据集，其余的作为训练数据集
  - 报告k个验证集误差的平均

- 常用：k = 5或10

- K-则交叉验证的目的是在没有足够多数据使用时评估模型和超参数的性能，也就是说，**K次训练和验证使用的是相同的超参数和模型**

#### 1.5 总结

- 训练数据集：训练模型参数
- 验证数据集：选择模型超参数
- 非大数据集上通常使用k-则交叉验证

### 2. 过拟合和欠拟合

#### 2.1 什么是过拟合和欠拟合？

| 模型容量\数据 | 简单  | 复杂  |
| ------- | --- | --- |
| 低       | 正常  | 欠拟合 |
| 高       | 过拟合 | 正常  |

- tips：模型容量即模型的复杂度，也代表了模型拟合各种函数的能力

#### 2.2 模型容量

- 拟合各种函数的能力
- 低容量的模型难以拟合训练数据
- 高容量的模型可以记住所有的训练数据

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/11/11-01.png)

- 显然，模型容量太低或太高都不好。太低（对应第一种）过于简单，模型分类效果差，太高（对应第二种）则过于复杂，把噪声全部都拟合住了，这是我们所不希望的。

#### 2.3 模型容量的影响

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/11/11-02.png)

- 我们的核心任务就是把泛化误差往下降

#### 2.4 估计模型容量

- 难以在不同的种类算法之间比较
  - 例如树模型和神经网络
- 给定一个模型种类，将有两个主要因素
  - 参数的个数
  - 参数的选择范围

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/11/11-03.png)

#### 2.5 VC维

VC维是统计学习理论的一个核心思想，这里大致了解就行，因为很难计算之后学习的模型（如CNN,RNN)的VC维，故并不经常用

- 定义：对于一个分类模型，VC维等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型对它进行完美分类。即存在H个样本，模型能把H个样本的2^H种标号方式打散的H的最大值。
- 例子：线性分类器的VC维
  - 2维输入的感知机，VC维=3（对于三个点的任意标号都能分类，而任意四个点的样本都存在不能被打散的标号形式个，如之前讲过的XOR）

3个点：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/11/11-04.png)

4个点：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/11/11-05.png)

- 支持N维输入的感知机的VC维是N+1
- 一些多层感知机的VC维是:

#### 2.6 VC维的用处

- 提供为什么一个模型好的理论依据
  - 它可以衡量训练误差和泛化误差之间的间隔
- 但深度学习中很少使用
  - 衡量不是很准确
  - 计算深度学习模型的VC维很困难

#### 2.7 数据复杂度

- 多个重要因素
  - 样本的元素个数
  - 每个样本的元素个数
  - 时间、空间结构
  - 多样性

#### 2.8 总结

- 模型容量需要匹配数据复杂度，否则可能导致欠拟合和过拟合
- 统计机器学习提供数学工具来衡量模型复杂度
- 实际中一般考观察训练误差和验证误差

### 3. 多项式回归

- 本小节使用多项式回归为例子，在pytorch上展示过拟合和欠拟合的实际表现

#### 3.1 导入库

```python
import math
import numpy as np
import torch
from torch import nn
from d2l import torch as d2l
```

#### 3.2 生成数据集

```python
max_degree = 20  # 多项式的最大阶数
n_train, n_test = 100, 100  # 训练和测试数据集大小
true_w = np.zeros(max_degree)  # 分配大量的空间
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])#前五个参数是有用的已知的参数，其他都是0，是不希望被学习的参数

features = np.random.normal(size=(n_train + n_test, 1))#创建特征值
np.random.shuffle(features)#打乱顺序
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))#通过广播机制得到每个特征值的所有多项式值
for i in range(max_degree):
    poly_features[:, i] /= math.gamma(i + 1)  # gamma(n)=(n-1)!，除以gamma防止梯度过大
# labels的维度:(n_train+n_test,)
labels = np.dot(poly_features, true_w)#将对应多项式值与其系数相乘
labels += np.random.normal(scale=0.1, size=labels.shape)#加上噪声项
```

#### 3.3 NumPyndarray转换为tensor

```python
true_w, features, poly_features, labels = [torch.tensor(x, dtype=
    torch.float32) for x in [true_w, features, poly_features, labels]]
```

#### 3.4 对模型进行训练和测试

首先让我们[实现一个函数来评估模型在给定数据集上的损失]。

```python
def evaluate_loss(net, data_iter, loss):  #@save
    """评估给定数据集上模型的损失"""
    metric = d2l.Accumulator(2)  # 损失的总和,样本数量
    for X, y in data_iter:
        out = net(X)#预测值
        y = y.reshape(out.shape)#将y维度变为与out一样
        l = loss(out, y)#计算损失
        metric.add(l.sum(), l.numel())#加入到迭代器中，进入下一个batch
    return metric[0] / metric[1]#返回平均损失
```

现在[定义训练函数]

```python
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = nn.MSELoss()#定义损失
    input_shape = train_features.shape[-1]
    # 不设置偏置，因为我们已经在多项式特征中实现了它（即x^0）
    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))#创建模型
    batch_size = min(10, train_labels.shape[0])
    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),
                                batch_size)#训练集
    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),
                               batch_size, is_train=False)#测试集
    trainer = torch.optim.SGD(net.parameters(), lr=0.001)#设置优化器，这里使用SGD
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])#动画
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)#训练
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))#将当前的训练集和测试集的损失存入animator中，用于绘图
    print('weight:', net[0].weight.data.numpy())#打印训练后的参数
```

#### 3.5 [**三阶多项式函数拟合(正态)**]

我们将首先使用三阶多项式函数，它与数据生成函数的阶数相同。 结果表明，该模型能有效降低训练损失和测试损失。 学习到的模型参数也接近真实值𝑤=[5,1.2,−3.4,5.6]。

```python
# 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!
train(poly_features[:n_train, :4], poly_features[n_train:, :4],
      labels[:n_train], labels[n_train:])
```

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/11/11-06.png)

#### 3.6 [**线性函数拟合(欠拟合)**]

让我们再看看线性函数拟合，减少该模型的训练损失相对困难。 在最后一个迭代周期完成后，训练损失仍然很高。 当用来拟合非线性模式（如这里的三阶多项式函数）时，线性模型容易欠拟合。

```python
# 从多项式特征中选择前2个维度，即1和x
train(poly_features[:n_train, :2], poly_features[n_train:, :2],
      labels[:n_train], labels[n_train:])
```

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/11/11-06.png)

#### 3.7 [**高阶多项式函数拟合(过拟合)**]

现在，让我们尝试使用一个阶数过高的多项式来训练模型。 在这种情况下，没有足够的数据用于学到高阶系数应该具有接近于零的值。 因此，这个过于复杂的模型会轻易受到训练数据中噪声的影响。 虽然训练损失可以有效地降低，但测试损失仍然很高。 结果表明，复杂模型对数据造成了过拟合。

```python
# 从多项式特征中选取所有维度
train(poly_features[:n_train, :], poly_features[n_train:, :],
      labels[:n_train], labels[n_train:], num_epochs=1500)
```

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 12- 权重衰退 Weight Decay

### 目录

- [1. 硬性限制/直观理解](#1-%E7%A1%AC%E6%80%A7%E9%99%90%E5%88%B6%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3)
- [2. 柔性限制/实际应用](#2-%E6%9F%94%E6%80%A7%E9%99%90%E5%88%B6%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8)
- [3.参数更新](#3-%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0)
  - [3.1 计算梯度](#31-%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6)
  - [3.2 更新参数](#32-%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0)
- [4.总结](#4-%E6%80%BB%E7%BB%93)
- [5.Q&A](#5-qa)

权重衰退是最常见的一种处理过拟合的方法，是最广泛使用的正则化技术之一。

> #### 复习：控制模型容量
> 
> 1. 使用更少参数
> 2. 控制每个参数（取值/可选择的值）范围较小

其中权重衰退属于第二种方法。

### 1. 硬性限制/直观理解

我们的优化目标仍然是![](http://latex.codecogs.com/svg.latex?min%5Cspace%5Cell(%5Cbold%7Bw%7D,b))，只是额外对![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D)添加一个限制条件![](http://latex.codecogs.com/svg.latex?%7C%7C%5Cbold%7Bw%7D%7C%7C%5E2%5Cleqslant%5Ctheta)，即权重的各项平方和小于一个特定的常数![](http://latex.codecogs.com/svg.latex?%5Ctheta)。那么设定一个较小的![](http://latex.codecogs.com/svg.latex?%5Ctheta)就会使得![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D)中每个元素的值都不会太大。

通常不会限制偏移b，理论上讲b表示整个数据在零点上的偏移，因此是不应该限制的，但实践中限制与否对结果都没什么影响。

**吴恩达课程中对这一现象的解释是w是高维向量，已经包含了绝大多数参数足以表达高方差问题，b作为单个数字对结果的影响就会很小.**

小的![](http://latex.codecogs.com/svg.latex?%5Ctheta)意味着更强的正则项，对于相同的![](http://latex.codecogs.com/svg.latex?%5Ctheta)，![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D)中元素越多则单个元素的值会越小。

### 2. 柔性限制/实际应用

上文说的硬性限制在实际使用时比较麻烦，实际上常用的函数是

![](http://latex.codecogs.com/svg.latex?min%5Cspace%5Cell(%5Cbold%7Bw%7D,b)+%5Cfrac%7B%5Clambda%7D%7B2%7D%7C%7C%5Cbold%7Bw%7D%7C%7C%5E2)

可以通过拉格朗日乘子证明对于每个![](http://latex.codecogs.com/svg.latex?%5Ctheta)都可以找到![](http://latex.codecogs.com/svg.latex?%5Clambda)使得硬性限制的目标函数等价于上式。

其中![](http://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Clambda%7D%7B2%7D%7C%7C%5Cbold%7Bw%7D%7C%7C%5E2)这一项被称为罚(penalty)，![](http://latex.codecogs.com/svg.latex?%5Clambda)是超参数，控制了正则项的重要程度。

当![](http://latex.codecogs.com/svg.latex?%5Clambda=0)时无作用，![](http://latex.codecogs.com/svg.latex?%5Clambda%5Crightarrow%5Cinfty)时最优解![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D%5E*%5Crightarrow0)，也就是说![](http://latex.codecogs.com/svg.latex?%5Clambda)越大模型复杂度就被控制的越低。

下面是老师给出的演示图

![1201](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/12/12-01.JPG)

以![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D)中只有两个参数为例，其中绿色的部分是原本损失函数函数值的“等高线”，黄色部分可以看作是正则项对应函数值的“等高线” ，使用权重衰减后需要优化的损失函数相当于图中两组等高线叠加。原本最优解位于绿色中心，现在这一位置在对于正则项有很高的损失，而正则项最小值位于原点，因此现在的最终优化解会更靠近原点，而当所有参数都更靠近原点时模型的规模也就更小。

### 3. 参数更新

#### 3.1 计算梯度

![](http://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%7B%7D%7D%7B%5Cpartial%7B%5Cbold%7Bw%7D%7D%7D(%5Cell(%5Cbold%7Bw%7D,b)+%5Cfrac%7B%5Clambda%7D%7B2%7D%7C%7C%5Cbold%7Bw%7D%7C%7C%5E2)=%5Cfrac%7B%5Cpartial%7B%5Cell(%5Cbold%7Bw%7D,b)%7D%7D%7B%7B%5Cpartial%7B%5Cbold%7Bw%7D%7D%7D%7D+%5Clambda%5Cbold%7Bw%7D)

#### 3.2 更新参数

将上式结果带入更新参数公式整理可得

![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D_%7Bt+1%7D=(1-%5Ceta%5Clambda)%5Cbold%7Bw%7D_%7Bt%7D-%5Ceta%5Cfrac%7B%5Cpartial%7B%5Cell(%5Cbold%7Bw%7D_t,b_t)%7D%7D%7B%7B%5Cpartial%7B%5Cbold%7Bw%7D_%7Bt%7D%7D%7D%7D)

注意到这个公式中后一项与原来更新参数的公式没有区别，仅仅是在前一项![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D_%7Bt%7D) 上加了一个系数![](http://latex.codecogs.com/svg.latex?(1-%5Ceta%5Clambda))。通常![](http://latex.codecogs.com/svg.latex?%5Ceta%5Clambda%3C1) ，也就是说由于引入了![](http://latex.codecogs.com/svg.latex?%5Clambda)，每次更新参数前先给待更新参数乘上一个小于1的权重再更新，权重衰退由此得名。

### 4. 总结

- 权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度
- 正则项权重（![](http://latex.codecogs.com/svg.latex?%5Clambda)）是控制模型复杂度的超参数

### 5. Q&A

- Q：Pytorch是否支持复数神经网络？

- A：应该不支持，但复数可以看作是二维的数，可以尝试将对应结构变成二维来实现需要的效果。

- Q：为什么参数不过大复杂度就低呢？

- A：确切的说是限制模型优化时只能在很小范围内取参数会使模型复杂度降低，见下图
  
  ![1202](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/12/12-02.JPG)

参数选择范围大时可拟合出很复杂的曲线，限制后只能学到更平滑的曲线/选择更简单的模型，那么模型复杂度就变低了。

- Q：如果使用L1范数如何更新权重？

- A：编写代码时只需把罚项改成如

```python
def l1_penalty(w):
  return torch.sum(torch.abs(w))
```

老师解答就到这里，但实操不应该只改罚项函数，还需重新定义带正则项的损失函数并求导化简。

![](http://latex.codecogs.com/svg.latex?%5Cfrac%7B%5Cpartial%7B%7D%7D%7B%5Cpartial%7B%5Cmathbf%7Bw%7D%7D%7D(%5Cell(%5Cmathbf%7Bw%7D,b)+%5Clambda%7C%7C%5Cmathbf%7Bw%7D%7C%7C_1)=%5Cfrac%7B%5Cpartial%7B%5Cell(%5Cmathbf%7Bw%7D,b)%7D%7D%7B%7B%5Cpartial%7B%5Cmathbf%7Bw%7D%7D%7D%7D+I'%5Clambda)

其中![](http://latex.codecogs.com/svg.latex?I'=(a_1,...,a_n)),当![](http://latex.codecogs.com/svg.latex?%5Cmathbf%7Bw%7D)中第i个元素为正时![](http://latex.codecogs.com/svg.latex?a_i=1)，反之![](http://latex.codecogs.com/svg.latex?a_i=-1).（=0时随意）

代入公式化简得

![](http://latex.codecogs.com/svg.latex?%5Cmathbf%7Bw%7D_%7Bt+1%7D=%5Cmathbf%7Bw%7D_%7Bt%7D-%5Ceta%5Cfrac%7B%5Cpartial%7B%5Cell(%5Cmathbf%7Bw%7D_t,b_t)%7D%7D%7B%7B%5Cpartial%7B%5Cmathbf%7Bw%7D_%7Bt%7D%7D%7D%7D-I'%5Ceta%5Clambda)

从这个式子可以看出使用L1正则化时只能对所有同号的参数施加一个相同大小的正则项（增减一个定值），而反观L2正则化对参数的影响是与参数本身的值有关的（乘上一个系数）似乎是更好的选择。不过L1正则化在特征提取上会有用处。

- Q：实践中权重衰减的值设置为多少好？跑代码时感觉效果不明显。

- A：一般取1e-2,1e-3,1e-4，权重衰退的效果确实有限，之后还会讲解更多方法。如果模型真的很复杂那么权重衰退一般不会带来特别好的效果。

- Q：关于L2范数的记法

- A：完整的写法是![](http://latex.codecogs.com/svg.latex?%7C%7C%5Cbold%7Bw%7D%7C%7C%5E2_2)，上标的2表示平方，下标的2表示是L2范数，下标有时省略。

- Q：为什么要把![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D)往小拉？如果最优解的![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D)本来就较大权重衰减是否会起反作用？/正则项使得![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D)变得更平均没有突出的值为什么可以拟合的更好呢？

- A：实际训练的数据都是有噪音的，而这些噪音可能会被拟合进去使得我们实际求解时得不到数学上的最优解，正则化起到将结果拉向最优解的作用。当然如果![](http://latex.codecogs.com/svg.latex?%5Clambda)选取过大可能会拉小的过多，如果没有过拟合那权重衰减就不起作用。
  
  **笔者注：这部分老师花了较长时间解释，建议大家自己去看视频。我的个人理解是重点不在于w大小/是否平均，而是由于数据有噪声，而噪声引起过拟合使得求出的w比数学上的最优解更大/更不平均，这时就需要正则化起到一个将结果拉向更小/平均/接近最优解的作用。**

- Q：噪音大会使得![](http://latex.codecogs.com/svg.latex?%5Cbold%7Bw%7D)较大是经验所得还是可以证明？

- A：可以证明，但本课程中不讲，可以自己尝试。

- Q：怎样调整![](http://latex.codecogs.com/svg.latex?%5Clambda)？

- A：不能确定什么时候是最优，但可以用前面讲的验证集/k折交叉验证，先取![](http://latex.codecogs.com/svg.latex?%5Clambda=0)看训练结果，再改变![](http://latex.codecogs.com/svg.latex?%5Clambda)看是否有改善。

**代码和部分课后题参考答案见本讲的ipynb文件。

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 13-丢弃法

## 本节目录

- [1.丢弃法动机、实现及原则](#1%E4%B8%A2%E5%BC%83%E6%B3%95%E5%8A%A8%E6%9C%BA%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%8E%9F%E5%88%99)
  - [1.1动机](#11%E5%8A%A8%E6%9C%BA)
  - [1.2如何实现模型的这一能力](#12%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%99%E4%B8%80%E8%83%BD%E5%8A%9B)
  - [1.3加入噪音的原则](#13%E5%8A%A0%E5%85%A5%E5%99%AA%E9%9F%B3%E7%9A%84%E5%8E%9F%E5%88%99)
- [2.丢弃法内容](#2%E4%B8%A2%E5%BC%83%E6%B3%95%E5%86%85%E5%AE%B9)
- [3.丢弃法使用](#3%E4%B8%A2%E5%BC%83%E6%B3%95%E4%BD%BF%E7%94%A8)
  - [3.1丢弃法的使用位置](#31%E4%B8%A2%E5%BC%83%E6%B3%95%E7%9A%84%E4%BD%BF%E7%94%A8%E4%BD%8D%E7%BD%AE)
  - [3.2训练中的丢弃法](#32%E8%AE%AD%E7%BB%83%E4%B8%AD%E7%9A%84%E4%B8%A2%E5%BC%83%E6%B3%95)
- [4.总结](#4%E6%80%BB%E7%BB%93)
- [5.代码部分](#5%E4%BB%A3%E7%A0%81%E9%83%A8%E5%88%86)
  - [5.1Dropout部分](#51dropout%E9%83%A8%E5%88%86)
  - [5.2在神经网络中使用丢弃法](#52%E5%9C%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E4%BD%BF%E7%94%A8%E4%B8%A2%E5%BC%83%E6%B3%95)

### 1.丢弃法动机、实现及原则

#### 1.1动机

- 一个好的模型需要对输入数据的扰动鲁棒（健壮性）

#### 1.2如何实现模型的这一能力

- 使用有噪音的数据。
- 丢弃法：在层之间加入噪音。

#### 1.3加入噪音的原则

![1301](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-01.png)

- 例如模型的功能是识别猫猫，加入噪音可以是输入模糊的猫猫图片，但尽量不要是狗狗的图片。

### 2.丢弃法内容

- 丢弃法对每个元素作如下扰动

![1304](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-04.png)

- 能够满足加入噪音的期望相同原则

![1305](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-05.png)

### 3.丢弃法使用

#### 3.1丢弃法的使用位置

- 通常将丢弃法作用在隐藏全连接层的输出上

![1306](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-06.png)

- 随机选中某些神经元将其输出置位0，因此模型不会过分依赖某些神经元

![1307](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/13/13-07.png)

#### 3.2训练中的丢弃法

- 正则项（丢弃法）仅在训练中使用：影响模型参数的更新，预测的时候便不再使用

### 4.总结

- 丢弃法将一些输出项随机置0来控制模型复杂度
- 常作用在多层感知机的隐藏层输出上
- 丢弃概率是控制模型复杂度的超参数（常取0.9，0.5，0.1）

### 5.代码部分

#### 5.1Dropout部分

```python
import torch
from torch import nn
from d2l import torch as d2l

def dropout_layer (X,dropout)：   #X为dropout层的输入，dropout为设置的丢弃概率
    assert 0<=dropout<=1        #丢弃概率介于0，1之间
    if dropout == 1:
       return torch.zeros_like(x) #若丢弃概率为1，则X的全部项均被置0
    if dropout == 0:
       return X                   #若丢弃概率为0，不对X作丢弃操作，直接返回X
    mask=(torch.Tensor(X.shape).uniform_(0,1)>dropout).float() #用uniform函数生成0-1间的随机实数，利用”>"，将大于dropout的记为1，小于dropout的记为0，实现丢弃操作
    return mask*X/(1-dropout) #将mask与X相乘实现丢弃操作，并除以(1-dropout)，这里不使用选中X中元素置0的原因是相乘操作相比选中操作更快
```

#### 5.2在神经网络中使用丢弃法

```python
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
dropout1, dropout2 = 0.2, 0.5
class Net(nn.Module)
    def _init_(self,num_inputs,num_outputs,num_outputs,num_hiddens1,num_hiddens2,is_training=True):
       super(Net,self)._init_()
       self.num_inputs=num_inputs
       self.training=is_training
       self.lin1=nn.Linear(num_inputs,num_hiddens1)
       self.lin2=nn.Linear(num_hiddens1,num_hiddens2)
       self.lin2=nn.Linear(num_hiddens2,num_outputs)
       self.relu=nn.ReLU()
    def forward(self,X):
       H1=self.relu(self.lin1(X.reshape((-1,self.num_inputs))))
       if self.training == True:  #丢弃法仅在训练中使用
           H1=dropout_layer(H1,dropout1)
       H2=self.relu(self.lin2(H1))
       if self.training == True: #丢弃法仅在训练中使用
           H2=dropout_layer(H2,dropout2)
       out=self.lin3(H2)  #output层不再使用丢弃法
       return out
```

<mark>****整合版来自→公众号：坚持打代码 ****</mark>

# 14-数值稳定性+模型初始化和激活函数

### 本节目录

- [1. 数值稳定性](#1-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7)
  - [1.1 神经网络的梯度](#11-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6)
  - [1.2 数值稳定性的常见两个问题](#12-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E7%9A%84%E5%B8%B8%E8%A7%81%E4%B8%A4%E4%B8%AA%E9%97%AE%E9%A2%98)
  - [1.3 例子：MLP](#13-%E4%BE%8B%E5%AD%90mlp)
  - [1.3 梯度爆炸](#13-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8)
    - [1.3.1 使用ReLU作为激活函数](#131-%E4%BD%BF%E7%94%A8relu%E4%BD%9C%E4%B8%BA%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)
    - [1.3.2 梯度爆炸问题](#132-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98)
  - [1.4 梯度消失](#14-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1)
    - [1.4.1 使用Sigmoid作为激活函数](#141-%E4%BD%BF%E7%94%A8sigmoid%E4%BD%9C%E4%B8%BA%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)
    - [1.4.2 梯度消失的问题](#142-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E9%97%AE%E9%A2%98)
- [2. 模型初始化和激活函数](#2-%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)
  - [2.1 让训练更加稳定](#21-%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E5%8A%A0%E7%A8%B3%E5%AE%9A)
  - [2.2 基本假设：让每层的均值/方差是一个常数](#22-%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE%E8%AE%A9%E6%AF%8F%E5%B1%82%E7%9A%84%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E6%98%AF%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0)
  - [2.3 权重初始化](#23-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96)
  - [2.4 例子：MLP](#24-%E4%BE%8B%E5%AD%90mlp)
    - [2.4.1 模型假设](#241-%E6%A8%A1%E5%9E%8B%E5%81%87%E8%AE%BE)
    - [2.4.2 正向方差](#242-%E6%AD%A3%E5%90%91%E6%96%B9%E5%B7%AE)
    - [2.4.3 反向均值和方差](#243-%E5%8F%8D%E5%90%91%E5%9D%87%E5%80%BC%E5%92%8C%E6%96%B9%E5%B7%AE)
    - [2.4.4 Xavier初始](#244-xavier%E5%88%9D%E5%A7%8B)
    - [2.4.5 假设线性的激活函数](#245-%E5%81%87%E8%AE%BE%E7%BA%BF%E6%80%A7%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)
    - [2.4.6 检查常用激活函数](#246-%E6%A3%80%E6%9F%A5%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)
- [3. 总结](#3-%E6%80%BB%E7%BB%93)
- [4.Q&A](#4qa)

### 1. 数值稳定性

数值稳定性是深度学习中比较重要的点，特别是当神经网络变得很深的时候，数值通常很容易变得不稳定。

#### 1.1 神经网络的梯度

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-01.png)

**考虑d层神经网络**

- t表示层数，![](https://latex.codecogs.com/svg.image?h%5E%7Bt-1%7D "h^{t-1}")表示第*t-1*层的输出，经过一个![](https://latex.codecogs.com/svg.image?f_%7Bt%7D "f_{t}")函数后，得到第*t*层的输出。

- 最终输出y的表示：输入x经过若干层(*d*层)的函数作用，最后被损失函数作用得到输出y。

**计算损失函数*L*关于第*t*层参数的梯度**

- 由链导法则得到上图中乘积公式

- 需要进行d-t次**矩阵乘法**（为什么是矩阵乘法？答：由于所有的*h*都是一些**向量**，导数中分子分母均为向量，所以求导得到的是矩阵，维数为[分子维度]x[分母维度]，可以参考第6节[视频](https://www.bilibili.com/video/BV1eZ4y1w7PY)和[笔记](./06-%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97.md)）。这也是导致数值稳定性问题的**主要因素**，由于做了太多次的矩阵乘法。

#### 1.2 数值稳定性的常见两个问题

**梯度爆炸**

假设梯度都是一些比1大的数比如1.5，做100次乘积之后得到![](https://latex.codecogs.com/svg.image?4%5Ctimes&space;10%5E%7B17%7D "4\times 10^{17}")，这个数字很容易带来一些浮点数上限的问题（需了解更多请参考计算机系统-计算机中浮点数的存储方式）。

**梯度消失**

假设梯度都是一些比1小的数比如0.8，做100次乘积之后得到![](https://latex.codecogs.com/svg.image?2%5Ctimes10%5E%7B-10%7D "2\times10^{-10}")，也可能会带来浮点数下溢的问题。

#### 1.3 例子：MLP

此处我们着重探讨[1.1节](#11-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6)中所述的求梯度时所做的d-t次矩阵乘法，并以一个实例MLP来探讨其结果的具体形式。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-02.png)

- 第一行公式，定义![](https://latex.codecogs.com/svg.image?h%5E%7Bt%7D "h^{t}")和![](https://latex.codecogs.com/svg.image?h%5E%7Bt-1%7D "h^{t-1}")(均为向量)的函数关系![](https://latex.codecogs.com/svg.image?f_%7Bt%7D "f_{t}")，第t层的权重矩阵作用于t-1层的输出![](https://latex.codecogs.com/svg.image?h%5E%7Bt-1%7D "h^{t-1}")后经过激活函数![](https://latex.codecogs.com/svg.image?%5Csigma&space; "\sigma")得到![](https://latex.codecogs.com/svg.image?h%5E%7Bt%7D "h^{t}")，注意激活函数![](https://latex.codecogs.com/svg.image?%5Csigma&space; "\sigma")逐元素计算。

- 第二行公式：这里用到链导法则，激活函数![](https://latex.codecogs.com/svg.image?%5Csigma&space; "\sigma")先对内部向量逐元素求导，然后把求导后这个向量变成对角矩阵（可以理解为链导法则中内部向量![](https://latex.codecogs.com/svg.image?W_%7Bt%7Dh_%7Bt-1%7D "W_{t}h_{t-1}")对自身进行求导，变成一个nxn的对角矩阵，更多请参考[邱锡鹏 《神经网络与深度学习》](https://nndl.github.io/nndl-book.pdf)[^ 图片1]）
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-03.png)

[^图片1]: 引自[邱锡鹏 《神经网络与深度学习》](https://nndl.github.io/nndl-book.pdf)附录：数学基础

- 视频中**勘误说明**：链导法则中 ![](https://latex.codecogs.com/svg.image?%5Cfrac%7B%5Cpartial&space;W%5E%7Bt%7Dh%5E%7Bt-1%7D%7D%7B%5Cpartial&space;h%5E%7Bt-1%7D%7D=&space;W%5E%7Bt%7D "\frac{\partial W^{t}h^{t-1}}{\partial h^{t-1}}= W^{t}") 而不是![](https://latex.codecogs.com/svg.image?%5Cleft&space;(W%5E%7Bt%7D&space;&space;%5Cright&space;)%5E%7BT%7D "\left (W^{t} \right )^{T}")（这点由分子分母维度也容易推出），故最终求导结果包含![](https://latex.codecogs.com/svg.image?W%5E%7Bt%7D "W^{t}")，而不是其转置。

#### 1.3 梯度爆炸

##### 1.3.1 使用ReLU作为激活函数

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-04.png)

由于激活函数Relu求导后或者是1或者是0，变为对角矩阵的斜对角线元素后，与![](https://latex.codecogs.com/svg.image?W%5E%7Bi%7D "W^{i}")做乘积，斜对角线为1的部分会使得W中元素保留，最终该连乘式中有一些元素来自![](https://latex.codecogs.com/svg.image?%5Cprod%5Cleft&space;(&space;W%5E%7Bi%7D&space;%5Cright&space;)&space; "\prod\left ( W^{i} \right )")，如果大部分![](https://latex.codecogs.com/svg.image?W%5E%7Bi%7D "W^{i}")中 值都大于1，且层数比较大，那么连乘之后可能导致梯度爆炸的问题。

##### 1.3.2 梯度爆炸问题

- 值超出值域（infinity）
  
  - 对于16位浮点数尤为严重（数值区间 [6e-5 , 6e4]），GPU用16位浮点数更快

- 对学习率敏感
  
  - 如果学习率太大→大参数值→更大的梯度，如此循环几次，容易导致梯度爆炸
  
  - 如果学习率太小→训练无进展
  
  - 我们可能需要在训练过程中不断调整学习率

#### 1.4 梯度消失

##### 1.4.1 使用Sigmoid作为激活函数

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-05.png)

- 蓝色曲线为函数值

- 黄色曲线为梯度，注意到当输入x值取±6时，此时梯度已经变得很小，由图也可以看出，当输入值稍大或稍小都很容易引起小梯度。
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-06.png)

所以最终连乘式中![](https://latex.codecogs.com/svg.image?%5Cprod&space;diag%5Cleft&space;(&space;%5Csigma&space;%5E%7B'%7D%5Cleft&space;(&space;W%5E%7Bi%7Dh%5E%7Bi-1%7D&space;%5Cright&space;)&space;%5Cright&space;)&space; "\prod diag\left ( \sigma ^{'}\left ( W^{i}h^{i-1} \right ) \right )")项乘出来会很小，导致整个梯度很小，产生梯度消失问题。

##### 1.4.2 梯度消失的问题

- 梯度值变为0
  
  - 对16位浮点数尤为严重

- 训练没有进展
  
  - 不管如何选择学习率，由于梯度已经为0了，学习率x梯度=0

- 对于底部层尤为严重
  
  - 仅仅顶部层训练得较好。第*t*层导数包含d-t个矩阵乘积，越往底层走，t越小，乘得越多，梯度消失越严重，所以底部层效果更差。
  - 无法让神经网络更深。只能把顶部层训练得比较好，底部层跑不动，这和给一个浅的神经网络没有什么区别。

### 2. 模型初始化和激活函数

#### 2.1 让训练更加稳定

我们的一个核心目标是如何让训练更稳定，梯度值不要太大也不要太小

- 目标：让梯度值在合理的范围内
  
  - 例如 [1e-6, 1e3]

- 常用方法：
  
  - 将乘法变加法：
    
    - ResNet（跳跃连接，如果很多层，加入加法进去）
    - LSTM（引入记忆细胞，更新门，遗忘门，通过门权重求和，控制下一步是否更新）
  
  - 归一化：
    
    - 梯度归一化（归一化均值，方差）
    
    - 梯度裁剪(clipping)：比如大于/小于一个固定的阈值，就让梯度等于这个阈值，将梯度限制在一个范围中。（可以缓解梯度爆炸）
  
  - 合理的权重初始和激活函数：本节课讲述重点

**下面我们重点探讨最后一种方法：合理的权重初始和激活函数**

#### 2.2 基本假设：让每层的均值/方差是一个常数

- **将每层的输出和梯度都看做随机变量**
  
  比如第i层有100维，就将输出和梯度分别看成100个随机变量

- **让它们的均值和方差都保持一致**
  
  我们的目标，这样不管神经网络多深，最后一层总与第一层差不多，从而不会梯度爆炸和消失

根据我们的假设，可以列出如下方程式：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-07.png)

#### 2.3 权重初始化

- 在合理值区间里随机初始参数
- 训练**开始**的时候更容易有数值不稳定
  - 远离最优解的地方损失函数表面可能很复杂
  - 最优解附近表面会比较平
- 使用N(0, 0.01)分布来初始可能对小网络没问题，但不能保证深度神经网络

#### 2.4 例子：MLP

下面我们以MLP为例，考虑需要什么条件，才能满足[2.2节](#22-%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE%EF%BC%9A%E8%AE%A9%E6%AF%8F%E5%B1%82%E7%9A%84%E5%9D%87%E5%80%BC/%E6%96%B9%E5%B7%AE%E6%98%AF%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0)的假设。

##### 2.4.1 模型假设

- 每一层**权重**中的变量均为**独立同分布**，并设出均值、方差。

- 每一层**输入**的变量**独立于**该层**权重**变量。同时**输入变量**之间**独立同分布**。

- 假设没有激活函数(先简化分析，之后会考虑有激活函数的情况)，可以求得该层输出的期望为0。
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-08.png)

此处用到了一个重要性质：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-09.png)

更多均值、方差运算可以参考[期望、方差、协方差及相关系数的基本运算](https://blog.csdn.net/MissXy_/article/details/80705828)

##### 2.4.2 正向方差

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-10.png)

- 第二行的计算中仍然用到了[2.4.1节](241%E6%A8%A1%E5%9E%8B%E5%81%87%E8%AE%BE)的期望的重要性质：如果两个变量独立，它们乘积的均值=均值的乘积，再结合w的期望为0(注意w和h独立，w之间独立同分布)，即有第二行末项期望为0。

- 最后一行由于wi,j独立同分布，方差相同，加上做了hj独立同分布的假设，所以可以写成 **[t-1层输出维度] x [t层权重方差] x [t-1层输出方差]** 的形式

- 此时，我们回过头来看我们的终极目标[2.2节](#22-%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE%EF%BC%9A%E8%AE%A9%E6%AF%8F%E5%B1%82%E7%9A%84%E5%9D%87%E5%80%BC/%E6%96%B9%E5%B7%AE%E6%98%AF%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0)的假设，每层输出期望为0我们已经可以满足(2.4.1节已经推导出)，而方差相同这一目标，通过上图的推导，我们发现需要![](https://latex.codecogs.com/svg.image?&space;n_%7Bt-1%7D%5Cgamma&space;_%7Bt%7D=1&space; "n_{t-1}\gamma _{t}=1")。

##### 2.4.3 反向均值和方差

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-11.png)

反向的情况和正向的类似，不过此时我们需要满足的式子变为![](https://latex.codecogs.com/svg.image?&space;n_%7Bt%7D%5Cgamma&space;_%7Bt%7D=1&space; "n_{t}\gamma _{t}=1")。

##### 2.4.4 Xavier初始

- 上述推导带来的问题：难以同时满足![](https://latex.codecogs.com/svg.image?&space;n_%7Bt-1%7D%5Cgamma&space;_%7Bt%7D=1&space; "n_{t-1}\gamma _{t}=1")和![](https://latex.codecogs.com/svg.image?&space;n_%7Bt%7D%5Cgamma&space;_%7Bt%7D=1&space; "n_{t}\gamma _{t}=1")。（需要每层输出的维度都相同）

- 采用Xavier折中解决，不能同时满足上面两式，转而满足 [**上面两式做加法后除以2**] 得到的式子，用两种分布进行初始化（每层方差、均值满足推导式）。
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-12.png)

- 如果能确定每层输入、输出维度大小，则能确定该层权重的方差大小。

- 权重初始化方式：正态分布、均匀分布，均值/方差满足Xavier的假设。

##### 2.4.5 假设线性的激活函数

真实情况下，我们并不会用线性的激活函数（这样相当于没有进行激活），这里为了简化问题，假设激活函数是线性的。

- **正向**
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-13.png)

上述推导表明，为了使得前向传播的均值为0，方差固定的话，激活函数必须f(x)=x，这种恒等映射。

- **反向**
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-14.png)

PPT上的推导似乎有点问题（上图中第二行方程），笔者重新进行了下述推导，读者也可自行推导验证：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-15.png)

**通过正向和反向的推导，我们可以得出的【结论】是：当激活函数为f(x)=x，这种恒等映射更有利于维持神经网络的稳定性。**

##### 2.4.6 检查常用激活函数

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/14/14-16.png)

对于常用激活函数：tanh，relu满足在零点附近有f(x)=x，而sigmoid函数在零点附近不满足要求，可以对sigmoid函数进行调整（根据Taylor展开式，调整其过原点）

### 3. 总结

- 当数值过大或者过小时，会导致数值问题。

- 常发生在深度模型中，因为其会对n个数累乘。

- 合理的权重初始值(如Xavier)和激活函数的选取(如relu, tanh, 调整后的sigmoid)可以提升数值稳定性。

### 4.Q&A

**问题：nan, inf是怎么产生的以及怎么解决的？**

> NaN和Inf怎么产生的：参考[出现nan、inf原因](https://blog.csdn.net/qq_16334327/article/details/86526854)

> 如何解决：参考[深度学习中nan和inf的解决](https://blog.csdn.net/u011119817/article/details/103908065)以及[训练网络loss出现Nan解决办法 ]([https://zhuanlan.zhihu.com/p/89588946#:~:text=一般来说，出现NaN有以下几种情况：](https://zhuanlan.zhihu.com/p/89588946#:~:text=%E4%B8%80%E8%88%AC%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%87%BA%E7%8E%B0NaN%E6%9C%89%E4%BB%A5%E4%B8%8B%E5%87%A0%E7%A7%8D%E6%83%85%E5%86%B5%EF%BC%9A) 1.,如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。 可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。)

**问题：训练过程中，如果网络层的输出的中间层特征元素的值突然变成nan了，是发生梯度爆炸了吗？**

> 参考[训练网络loss出现Nan解决办法 ]([https://zhuanlan.zhihu.com/p/89588946#:~:text=一般来说，出现NaN有以下几种情况：](https://zhuanlan.zhihu.com/p/89588946#:~:text=%E4%B8%80%E8%88%AC%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%87%BA%E7%8E%B0NaN%E6%9C%89%E4%BB%A5%E4%B8%8B%E5%87%A0%E7%A7%8D%E6%83%85%E5%86%B5%EF%BC%9A) 1.,如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。 可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。)

**问题：老师，让每层方差是一个常数的方法，您指的是batch normalization吗？想问一下bn层为什么要有伽马和贝塔？去掉可以吗**

> 让每层方差是一个常数，和batch norm没有太多关系，(本节课介绍的方法是合理地初始化权重和设置激活函数)。batch norm可以让你的输出变成一个均值为0，方差差不多是一个固定值的东西，但它不一定能保证你的梯度。

(此处节选几个重要的Q&A，建议观看完整Q&A，获得更深的理解)

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 15-实战Kaggle比赛：预测房价

- 实际上这个时间竞赛已经结束了。

- 这节目的是提供大家一个实际操作的机会，使用的大多是前面学到的知识。课程提供了代码样本，酒体内容详见含注释的代码中。

- 简单介绍一下数据集的内容，数据集是加州2020年几乎全部的房子交易记录，这里选择前半年的数据作为训练集，后半年作为验证集，包括许多的信息，如：ID，洗手间个数，卧室个数，政府预测价格等等共41个特征，连结如下：[California House Prices | Kaggle](https://www.kaggle.com/c/california-house-prices/data?select=train.csv) 需要大家自行下载数据集（没有kaglle账户的要注册才可以下载）

- 在真实数据集上，已经提供的程序得到的效果会很差，需要我们使用各种方法来优化，例如dropout，weught decay等，期待大家取得好成绩！

# 16- Pytorch神经网络基础

### 层和块

在之前的内容中，我们认识了一些神经网络，比如：线性回归，Softmax回归，多层感知机；他们有的是整个模型，有的是一层神经网络，有的甚至只是一个单元，他们的功能以及复杂程度也各不相同，但他们都有着如下三个特征：

- 接受一些输入
- 产生对应的输出
- 由一组可调整参数描述

对于一些复杂的网络，研究讨论比层大但比整个模型小的部分很有意义，因为复杂的网络中经常有重复出现的部分，每个部分也常常有自己的功能。考虑到上面的三个特征，这就使得我们思考是否可以对这些部分进行一个抽象，这就得到了块的概念：块指单个层，多个层组成的部分，或者整个模型本身。使用块对整个模型进行描述就简便许多，这一过程是递归的，块的内部还可以划分为多个块，直至满足需要为止。

PyTorch帮我们实现了块的大部分所需功能，包括自动求导，我们只需从nn.Module继承并改写其中的一部分就能得到我们需要的块以及模型，具体做法和细节见代码中的注释

### 参数管理

在选择了架构并设置了超参数后，我们就进入了训练阶段。此时，我们的目标是找到使损失函数最小化的模型参数值。经过训练后，我们将需要使用这些参数来做出未来的预测。此外，有时我们希望提取参数，以便在其他环境中复用它们，将模型保存下来，以便它可以在其他软件中执行，或者为了获得科学的理解而进行检查。

此部分主要为代码实现，笔记见代码中的注释

### 延后初始化

有时在建立网络时，我们不会指定网络的输入输出维度，也就不能确定网络的参数形状，深度学习框架支持延后初始化，即当第一次将数据传入模型时自动的得到所有的维度，然后初始化所有的参数。

PyTorch也支持这一点，比如nn.LazyLinear，但本门课程中并未介绍。

### 自定义层

深度学习成功背后的一个因素是神经网络的灵活性：我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。同样的，对于层而言，深度学习框架并不能满足我们所有的需求，然而，层本身也具有极大的灵活性，我们可以自定义想要的层。

此部分主要为代码实现，笔记见代码中的注释

### 读写文件

到目前为止，我们讨论了如何处理数据，以及如何构建、训练和测试深度学习模型。然而，有时我们希望保存训练的模型，以备将来在各种环境中使用（比如在部署中进行预测）。此外，当运行一个耗时较长的训练过程时，最佳的做法是定期保存中间结果，以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 17-使用和购买GPU

目录：

- [使用和购买GPU](#%E4%BD%BF%E7%94%A8%E5%92%8C%E8%B4%AD%E4%B9%B0gpu)

- [使用GPU](#%E4%BD%BF%E7%94%A8gpu)

- [购买GPU](#%E8%B4%AD%E4%B9%B0gpu)

## 使用和购买GPU

### 使用GPU

（简而言之，自2000年以来，GPU性能每10年增长1000倍，本节主要介绍如何利用这种计算性能进行研究，首先是使用单个GPU，然后是如何使用多个GPU和多个服务器）

- 准备：
  
  - （首先确保至少安装了一个NVDIA GPU，然后下载[NVIDIA驱动和CUDA](https://developer.nvidia.com/cuda-downloads)并按照提示设置适当的路径）
  
  - 查看显卡信息：

```python
!nvidia-smi
```

```python
Fri Jan 14 03:23:18 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |
| N/A   43C    P0    74W / 300W |   1608MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |
| N/A   42C    P0    62W / 300W |   1706MiB / 16130MiB |      9%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |
| N/A   64C    P0    68W / 300W |     11MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   57C    P0    45W / 300W |     11MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      5034      C   ...conda3/envs/d2l-en-release-0/bin/python  1597MiB |
|    1      5034      C   ...conda3/envs/d2l-en-release-0/bin/python  1695MiB |
```

（可以看到这里显示有4块Tesla V100的GPU，Memory-Usage显示的是“当前使用空间 / 总空间”，GPU-Util显示的是模型训练时GPU的使用率，如果为50%以下说明模型可能不太好）

- 准备（续）：
  
  - 在PyTorch中，每个数组都有一个设备（device）， 我们通常将其称为上下文（context）。 默认情况下，所有变量和相关的计算都分配给CPU。 有时上下文可能是GPU。 当我们跨多个服务器部署作业时，事情会变得更加棘手。 通过智能地将数组分配给上下文， 我们可以最大限度地减少在设备之间传输数据的时间。 例如，当在带有GPU的服务器上训练神经网络时， 我们通常希望模型的参数在GPU上。
  - 要运行此部分中的程序，至少需要两个GPU。 注意，对于大多数桌面计算机来说，这可能是奢侈的，但在云中很容易获得。 例如，你可以使用AWS EC2的多GPU实例。 本书的其他章节大都不需要多个GPU， 而本节只是为了展示数据如何在不同的设备之间传递。

- 计算设备：
  
  - 我们可以指定用于存储和计算的设备，如CPU和GPU。 默认情况下，张量是在内存中创建的，然后使用CPU计算它。所有的深度学习框架都是默认在CPU上做运算，如果要使用GPU则需要指定计算机更换运算位置 。
  
  - 在PyTorch中，CPU和GPU可以用`torch.device('cpu')` 和`torch.device('cuda')`表示。
  
  - `cpu`设备意味着所有物理CPU和内存， 这意味着PyTorch的计算将尝试使用所有CPU核心。 然而，`gpu`设备只代表一个卡和相应的显存。
  
  - 如果有多个GPU，我们使用`torch.device(f'cuda:{i}')` 来表示第*i*块GPU（*i*从0开始）。 另外，`cuda:0`和`cuda`是等价的。
  
  - ```python
    import torch
    from torch import nn
    ```
  
  - ```python
    """指定cpu, gpu设备"""
    torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')
    # cpu, gpu0, gpu1
    
    """
    输出:
    (device(type='cpu'), device(type='cuda'), device(type='cuda', index=1))
    """
    ```
  
  - ```python
    """查询可用gpu数量"""
    torch.cuda.device_count()
    
    """
    输出:
    2
    """
    ```
  
  - ```python
    """定义了两个方便的函数， 这两个函数允许我们在不存在所需所有GPU的情况下运行代码"""
    ```
    
    """如果存在，则返回gpu(i)，否则返回cpu()"""
    def try_gpu(i=0):
    
    # 不输入参数则默认i = 0
    
    ```
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
     # 如果当前可用gpu的总数大于等于i+1，则返回第i个gpu（从0计数）        
    return torch.device('cpu')
    # 否则证明当前没有更多可用gpu，则返回cpu
    ```
    
    """返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""
    def try_all_gpus():
    
    ```
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    # 所有可用gpu设备序号组成的列表devices
    
    return devices if devices else [torch.device('cpu')]
    # 如果列表devices不为空则证明此时有可用的gpu，则返回可用gpu序号列表；否则证明没有可用gpu，则返回cpu
    ```
    
    try_gpu(), try_gpu(10), try_all_gpus()
    
    # 测试函数功能
    
    # try_gpu():检测是否有第i=0号gpu
    
    # try_gpu(10):检测是否有第i=10号gpu
    
    # try_all_gpus():返回所有可用gpu序号列表，如果没有gpu则返回cpu
    
    """
    输出：
    (device(type='cuda', index=0),
     device(type='cpu'),
     [device(type='cuda', index=0), device(type='cuda', index=1)])
    """
    
    # device(type='cuda', index=0): 有第0号gpu
    
    # device(type='cpu'): 没有第10号gpu
    
    # [device(type='cuda', index=0), device(type='cuda', index=1)]: 共有序号为0、1的两个gpu

- 张量与GPU
  
  - ```python
    """我们可以查询张量所在的设备。 默认情况下，张量是在CPU上创建的。"""
    x = torch.tensor([1, 2, 3])
    x.device
    
    """
    device(type='cpu')
    """
    # 默认情况下，张量是在CPU上创建的
    ```
  
  - 注意：无论何时我们要对多个项进行操作， 它们都必须在同一个设备上。 例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上， 否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。
  
  - 存储在GPU上：
    
    ```python
    """我们在第一个gpu上创建张量变量X""" 
    X = torch.ones(2, 3, device=try_gpu())
    """
    tensor([[1., 1., 1.],
            [1., 1., 1.]], device='cuda:0')
    """
    
    """假设你至少有两个GPU，下面的代码将在第二个GPU上创建一个随机张量"""
    Y = torch.rand(2, 3, device=try_gpu(1))
    """
    tensor([[0.3432, 0.4088, 0.7725],
            [0.0571, 0.3341, 0.2544]], device='cuda:1')
    """
    ```
  
  - 复制：如果我们要计算`X + Y`，我们需要决定在哪里执行这个操作。 例如，如下图所示， 我们可以将`X`传输到第二个GPU并在那里执行操作。 *不要*简单地`X`加上`Y`，因为这会导致异常， 运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。 由于`Y`位于第二个GPU上，所以我们需要将`X`移到那里， 然后才能执行相加运算。
    
    ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/17/17-01.png)

- ```python
  """将gpu(0)中的X复制到gpu(1)中的Z"""
  Z = X.cuda(1)
  print(X)
  print(Z)
  
  """
  tensor([[1., 1., 1.],
          [1., 1., 1.]], device='cuda:0')
  tensor([[1., 1., 1.],
          [1., 1., 1.]], device='cuda:1')
  """
  ```
  
  """现在数据在同一个GPU上（Z和Y都在），我们可以将它们相加。"""
   Y + Z
  
  """
   tensor([[1.3432, 1.4088, 1.7725],
  
  ```
        [1.0571, 1.3341, 1.2544]], device='cuda:1')
  ```
  
  """
  
  """如果变量Z已经存在于第i个GPU上，再调用Z.cuda(i)只会返回Z并不会复制并分配新内存"""
   Z.cuda(1) is Z
  
  """
   True
   """

- 旁注：人们使用GPU来进行机器学习，因为单个GPU相对运行速度快。 但是在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。 这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收）， 然后才能继续进行更多的操作。 这就是为什么拷贝操作要格外小心。根据经验，多个小操作比一个大操作糟糕得多。 此外，一次执行几个操作比代码中散布的许多单个操作要好得多（除非你确信自己在做什么）。 如果一个设备必须等待另一个设备才能执行其他操作， 那么这样的操作可能会阻塞。 这有点像排队订购咖啡，而不像通过电话预先订购： 当你到店的时候，咖啡已经准备好了。当我们打印张量或将张量转换为NumPy格式时， 如果数据不在内存中，框架会首先将其复制到内存中， 这会导致额外的传输开销。 更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。

- 神经网络与GPU
  
  - 类似地，神经网络模型可以指定设备。 下面的代码将模型参数放在GPU上。
    
    ```python
    net = nn.Sequential(nn.Linear(3, 1))
    net = net.to(device=try_gpu())
    ```
  
  - 当输入为GPU上的张量时，模型将在同一GPU上计算结果。总之，只要所有的数据和参数都在同一个设备上， 我们就可以有效地学习模型。
    
    ```python
    net(X)
    """
    tensor([[0.5037],
            [0.5037]], device='cuda:0', grad_fn=<AddmmBackward>)
    """
    
    net[0].weight.data.device
    """
    device(type='cuda', index=0)
    """
    ```

### 购买GPU

目前，AMD和NVIDIA是专用GPU的两大主要制造商。NVIDIA是第一个进入深度学习领域的公司，通过CUDA为深度学习框架提供更好的支持。因此，大多数买家选择NVIDIA GPU。

NVIDIA提供两种类型的GPU，针对个人用户（例如，通过GTX和RTX系列）和企业用户（通过其Tesla系列）。这两种类型的GPU提供了相当的计算能力。但是，企业用户GPU通常使用强制（被动）冷却、更多内存和ECC（纠错）内存。这些GPU更适用于数据中心，通常成本是消费者GPU的十倍。

如果你是一个拥有100个服务器的大公司，你应该考虑英伟达Tesla系列，或者在云中使用GPU服务器。对于实验室或10+服务器的中小型公司，英伟达RTX系列可能是最具成本效益的。你可以购买超微或华硕机箱的预配置服务器，这些服务器可以有效地容纳4-8个GPU。

GPU供应商通常每一到两年发布一代，例如2017年发布的GTX 1000（Pascal）系列和2019年发布的RTX 2000（Turing）系列。每个系列都提供几种不同的型号，提供不同的性能级别。GPU性能主要是以下三个参数的组合：

1. **计算能力**。通常我们追求32位浮点计算能力。16位浮点训练（FP16）也进入主流。如果你只对预测感兴趣，还可以使用8位整数。最新一代图灵GPU提供4-bit加速。不幸的是，目前训练低精度网络的算法还没有普及。
2. **内存大小**。随着你的模型变大或训练期间使用的批量变大，你将需要更多的GPU内存。检查HBM2（高带宽内存）与GDDR6（图形DDR）内存。HBM2速度更快，但成本更高。
3. **内存带宽**。只有当你有足够的内存带宽时，你才能最大限度地利用你的计算能力。如果使用GDDR6，请追求宽内存总线。

对于大多数用户来说，只需看看计算能力就足够了。请注意，许多GPU提供不同类型的加速。例如，NVIDIA的Tensor Cores将操作符子集的速度提高了5×

。确保你的库支持这一点。GPU内存应不小于4GB（8GB更好）。尽量避免将GPU也用于显示GUI（改用内置显卡）。如果无法避免，请添加额外的2GB RAM以确保安全。

下图比较了各种GTX 900、GTX 1000和RTX 2000系列的（GFlops）和价格（Price）。价格是维基百科上的建议价格。![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/17/17-02.png)

我们可以看到很多事情：

1. 在每个系列中，价格和性能大致成比例。Titan因拥有大GPU内存而有相当的溢价。然而，通过比较980 Ti和1080 Ti可以看出，较新型号具有更好的成本效益。RTX 2000系列的价格似乎没有多大提高。然而，它们提供了更优秀的低精度性能（FP16、INT8和INT4）。
2. GTX 1000系列的性价比大约是900系列的两倍。
3. 对于RTX 2000系列，浮点计算能力是价格的“仿射”函数。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/17/17-03.png)

上图显示了能耗与计算量基本成线性关系。其次，后一代更有效率。这似乎与对应于RTX 2000系列的图表相矛盾。然而，这是TensorCore不成比例的大能耗的结果。

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 18-预测房价竞赛总结

### 本节目录

- [1.方法总结](#1%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93)
- [2.分析](#2%E5%88%86%E6%9E%90)
- [3.关于automl](#3%E5%85%B3%E4%BA%8Eautoml)
  - [3.1 课程内容](#31-%E8%AF%BE%E7%A8%8B%E5%86%85%E5%AE%B9)
  - [3.2 补充内容](#32-%E8%A1%A5%E5%85%85%E5%86%85%E5%AE%B9)
    - [AutoGluon](#autogluon)
- [4.总结](#4%E6%80%BB%E7%BB%93)
- [5.预测房价竞赛总结 Q&A](#5%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7%E7%AB%9E%E8%B5%9B%E6%80%BB%E7%BB%93-qa)

### 1.方法总结

> 下面提供了排行榜前几使用的方法介绍链接

- 第二和第七：autogluon
  
  https://www.bilibili.com/video/BV1rh411m7Hb/

- 第三：h2o
  
  [AutoML(Using h2o) | Kaggle](https://www.kaggle.com/wuwawa/automl-using-h2o)

- 第四：随机森林
  
  [The 4th place approach (Random Forest) | Kaggle](https://www.kaggle.com/jackzh/the-4th-place-approach-random-forest)

### 2.分析

- 已知的排名靠前的4个成绩均使用了集成学习

- 目前不知道是否有使用书中的mlp取得好成绩
  
  > 通过调参数，是能够取得很好的结果的
  
  对于mlp来说，特征预处理和超参数的调节是取得好成绩的基础

- 数据的难点
  
  - 数值较大
    
    > 梯度相对较大，容易发生梯度爆炸
    
    一个解决方案是可以对数据取对数，再进行标准化
  
  - 有文本特征（地址，介绍）
    
    > 这些文字可能含有较多的噪声，对模型产生影响
    
    解决办法日后会讲解，比如第二名用的transformer
  
  - 训练数据是前6个月，公榜是后3个月，私榜是再往后3个月
    
    > 利用历史的数据进行训练，在实践中自然会有不同的影响（可能过拟合）
    > 
    > 因此公榜与私榜的排名有一定差异
    
    ```
           这个问题称为Covariate Shift，没有特别好的解决方案  ，可以让模型尽可能稳定，不去仔细调参
    ```

### 3.关于automl

#### 3.1 课程内容

> 这一部分李沐老师要表达的主要是我们应该深入去了解本后的原理，不要因为有"自动化"深度学习而产生一种依赖心理或者变得没有深究深度学习的动力，学习deep learning仍然是有意义的

- 数据科学家80%时间在处理数据，20%调模型
  
  > 处理数据是automl不能做的，automl的作用主要在调模型这块，数据科学家仍然能大展身手

- Automl现在能处理一些基础的情况
  
  > 目前节省10%时间，未来节省20%时间

- 为什么还要学习深度学习
  
  正如买菜只需要用到四则运算甚至不用，我们仍然需要学习三角函数去进行更深入的科学研究等其他事情。当人人都会用Automl的时候，我们仍然需要懂得一些底层的原理，毕竟Automl也是有局限性的，需要我们不断改进，或者想出其他算法。另一方面，我们也要肯定Automl带来的便利。

#### 3.2 补充内容

##### AutoGluon

> 与大部分automl框架是基于超参数搜索技术的不同，Autogluon会利用多个机器学习包来训练模型

- 房价预测竞赛中模型的改动
  
  > 1.对于数据中数值比较大且数据变化大的数值取log,CPU上训练2个小时，最终排第七
  > 
  > 2.房子描述里包含大量文本，使用mutimodal选项来用transformer提取特征，并做多模型融合,用GPU才跑得动，排名第二

- AutoGluon背后的技术
  
  > 1.stacking
  > 
  > 2.k-则交叉bagging
  > 
  > 3.多层stacking

- 总结
  
  > 1.autogluon在合理的计算开销下得到还不错的模型
  > 
  > 2.虽然autogluon可以做自动特征抽取，但是当加入一些人工数据处理也是不错的方法
  > 
  > 3.对于比较大的数据集计算开销仍然是瓶颈，需要使用GPU甚至多台机器做分布式训练，这仍是AutoML未来的研究方向
  > 
  > 4.具体讲解可参考：https://www.bilibili.com/video/BV1F84y1F7Ps/?spm_id_from=333.788.recommend_more_video.1

### 4.总结

这节课本身就是一次对预测房价竞赛的总结，主要介绍了排名的分布情况以及一些队伍使用的方法。

### 5.预测房价竞赛总结 Q&A

**Q1: 统计学专业本科生未来从事人工智能如何规划**

> 注重动手能力的培养

**Q2: 避免overfit是调参好还是不调参好？老师有何经验分享？**

> 调参是需要的，首先最好有一个比较好的验证集；当你找到一个在验证集效果比较好的超参数值的时候，最好在这一值上调或下调一点看看是否敏感，如果比较敏感说明这点可能只是在这点凑巧效果好罢了，泛化性就不好；当然在实践中调参并没有像在竞赛中那么重要

**Q3: 老师说的80%时间处理数据是指的找数据、清理数据这些？数据搭建pipeline不就好了， ？为什么改进模型等等不占主要时间？**

> 处理数据并不是搭建pipeline就好了，你需要决定从哪里获取数据、怎样获取数据、如何处理噪音（清理数据）......这些都是很费时间的

**Q4: AutoML与ML有严格的特征区别吗**

> AutoML可以看作是ML中的一类算法

**Q5: 用mlp做竞赛时发现层数深的时候预测出来的房价全是一样的，层数浅一点还不会出现这个问题，为什么？**

> 应该是梯度爆炸，或者梯度消失，也就是数值稳定性出现问题

**Q6：MLP有值得精细调参的价值吗？**

> 有。

**<mark>整合版来自→公众号：坚持打代码</mark>**

# 19-卷积层

#### 本讲文字介绍部分请参考沐神在线书籍~：https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/why-conv.html

#### 代码

```python
import torch
from torch import nn

def corr2d(X,K):    #X为输入，K为核矩阵
    h,w=K.shape    #h得到K的行数，w得到K的列数
    Y=torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))  #用0初始化输出矩阵Y
    for i in range(Y.shape[0]):   #卷积运算
        for j in range(Y.shape[1]):
          Y[i,j]=(X[i:i+h,j:j+w]*K).sum()
    return Y
```

```python
#样例点测试
X=torch.tensor([[0,1,2],[3,4,5],[6,7,8]])
K=torch.tensor([[0,1],[2,3]])
corr2d(X,K)
```

```
>>> tensor([[19., 25.],
            [37., 43.]])
```

```python
#实现二维卷积层
class Conv2d(nn.Module):
    def _init_(self,kernel_size):
        super()._init_()
        self.weight=nn.Parameter(torch.rand(kerner_size))
        self.bias=nn.Parameter(torch.zeros(1))
    def forward(self,x):
        return corr2d(x,self.weight)+self.bias 
```

```python
X=torch.ones((6,8))
X[:,2:6]=0
X
```

```
>>> tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.]])
```

```python
K=torch.tensor([[-1,1]])  #这个K只能检测垂直边缘
Y=corr2d(X,K)
Y
```

```
>>> tensor([[ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  0.,  1.,  0.]])
```

```python
corr2d(X.t(),K)
```

```
>>> tensor([[0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.]])
```

```python
conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)

X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y)**2
    conv2d.zero_grad()
    l.sum().backward()
    conv2d.weight.data[:] -= 3e-2 * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'batch {i+1}, loss {l.sum():.3f}')
```

```
>>> batch 2, loss 3.852
    batch 4, loss 1.126
    batch 6, loss 0.386
    batch 8, loss 0.145
    batch 10, loss 0.057
```

```python
conv2d.weight.data.reshape((1, 2))
```

```
>>> tensor([[-1.0173,  0.9685]])
```

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 20-填充和步幅

### 1. 填充

**填充**(Padding)指的是在输入周围添加额外的行/列

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-01.png)

**维度变化**：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-02.png)

**两种不同的卷积方式**：
①Valid 卷积：不进行填充，卷积运算过后得到的矩阵形状为(n-f+1)×(n-f+1)。

②Same 卷积：先对矩阵进行填充，然后再进行卷积运算，使得运算前后矩阵大小不变。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-03.png)

### 2. 步幅

**想法来源：**如果按照原来的操作(卷积步长为1)，那么给定输入大小为224x224，在使用5x5卷积核的情况下，需要**55层**才能将输出降低到4x4，也就是说，需要大量的计算才能得到维度较小的输出。

**步幅**是指行/列的滑动步长

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-04.png)

**维度变化**:

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/20/20-05.png) 注意：第三点可以当做结论来记(Same卷积或Valid卷积(且s≥k时))。一般来说，如果n是偶数，s取2，池化层做Valid卷积(不填充)且k=2，此时输出维度直接可以写成n/2 x n/2。如果怕搞混，直接记第一个公式每次现推也可。

### 3. 总结

- 填充和步幅是卷积层的**超参数**

- **填充**(padding)在输入周围添加额外的行/列，来控制输出形状的减少量

- **步幅**(stride)是每次滑动核窗口时的行/列的步长，可以成倍地减少输出形状

### 4. 代码

#### 4.1 填充和步幅

**导入包，定义comp_conv2d函数 (进行卷积操作, 输出后两维，便于观察高宽的维度变化)**

```python
import torch
from torch import nn

def comp_conv2d(conv2d, X):
    X = X.reshape((1, 1) + X.shape) #X的维度之前加入批量大小数(batch_size)和输入通道数(channel_in)
    Y = conv2d(X)                    
    return Y.reshape(Y.shape[2:])  #去掉前面的两维后(原来四维) 进行输出
```

#### 4.2 padding

**在所有侧边填充1个像素(padding=1, 即(1,1))**

```python
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1) #输入输出通道数为1, 卷积核大小3x3, 填充为1(上下左右各填充一行)
X = torch.rand(size=(8, 8))         
comp_conv2d(conv2d, X).shape
```

```python
>>> torch.Size([8, 8])
```

**填充不同的高度和宽度(padding=(2,1))**

```python
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
```

```python
>>> torch.Size([8, 8])
```

#### 4.3 stride

**将高度和宽度的步幅设置为2**

```python
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)
comp_conv2d(conv2d, X).shape
```

```python
>>> torch.Size([4, 4])
```

**一个稍微复杂的例子**

```python
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
```

```python
>>> torch.Size([2, 2])
```

<mark>**整合版来自→公众号：坚持打代码 **</mark>

# 21-多个输入和输出通道

### 本节目录：

- [21-多个输入和输出通道](#21-%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93)
  - [本节目录：](#%E6%9C%AC%E8%8A%82%E7%9B%AE%E5%BD%95)
  - [1.多个输入通道：](#1%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93)
  - [2.多个输出通道](#2%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93)
  - [3.多个输入和输出通道](#3%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93)
  - [4.1X1卷积层](#41x1%E5%8D%B7%E7%A7%AF%E5%B1%82)
  - [5.二维卷积层](#5%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E5%B1%82)
  - [6.总结](#6%E6%80%BB%E7%BB%93)
  - [7.Q&A](#7qa)

### 1.多个输入通道：

- 彩色图像可能有RGB三个通道

- 转换为灰度会丢失信息
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-01.png)

- 每个通道都有一个卷积和，结果是所有通道卷积结果的和
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-02.png)

- 输入**X**:![](https://latex.codecogs.com/svg.image?c_%7Bi%7D%5Ctimes&space;n_%7Bh%7D%5Ctimes&space;n_%7Bw%7D "c_{i}\times n_{h}\times n_{w}")

- 核**W**：![](https://latex.codecogs.com/svg.image?c_%7Bi%7D%5Ctimes&space;k_%7Bh%7D%5Ctimes&space;k_%7Bw%7D "c_{i}\times k_{h}\times k_{w}")

- 输出**Y**:![](https://latex.codecogs.com/svg.image?m_%7Bh%7D%5Ctimes&space;m_%7Bw%7D "m_{h}\times m_{w}")

![](https://latex.codecogs.com/svg.image?Y=%5Csum&space;_%7Bi=0%7D%5E%7Bc_%7Bi%7D%7DX_%7Bi,:,:%7D%5Cbigstar&space;W_%7Bi,:,:%7D "Y=\sum _{i=0}^{c_{i}}X_{i,:,:}\bigstar W_{i,:,:}")

多个输入通道：

```python
import torch
from d2l import torch as d2l

def corr2d_multi_in(X, K):
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
```

### 2.多个输出通道

- 无论有多少输入通道，到目前位置我们植绒到单输出通道
- 我们可以有多个三维卷积核，每个核生成一个输出通道
- 输入**X**:![](https://latex.codecogs.com/svg.image?c_%7Bi%7D%5Ctimes&space;k_%7Bh%7D%5Ctimes&space;k_%7Bw%7D "c_{i}\times k_{h}\times k_{w}")
- 核**W**：![](https://latex.codecogs.com/svg.image?c_%7Bo%7D%5Ctimes&space;c_%7Bi%7D%5Ctimes&space;k_%7Bh%7D%5Ctimes&space;k_%7Bw%7D "c_{o}\times c_{i}\times k_{h}\times k_{w}")
- 输出**Y**：![](https://latex.codecogs.com/svg.image?c_%7Bo%7D%5Ctimes&space;m_%7Bh%7D%5Ctimes&space;m_%7Bw%7D "c_{o}\times m_{h}\times m_{w}")

![](https://latex.codecogs.com/svg.image?Y_%7Bi,:,:%7D=X%5Cbigstar&space;W_%7Bi,:,:%7D%5Cqquad&space;for&space;%5Cquad&space;i=1,...,c_%7Bo%7D "Y_{i,:,:}=X\bigstar W_{i,:,:}\qquad for \quad i=1,...,c_{o}")

多个输出通道：

```python
def corr2d_multi_in_out(X, K):
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
```

### 3.多个输入和输出通道

- 每个通道可以识别特定的模式

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-03.png)

- 输入通道核识别并组合输入中的模式

### 4.1X1卷积层

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-04.png)

```python
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = X.reshape((c_i, h * w))
    K = K.reshape((c_o, c_i))
    Y = torch.matmul(K, X)
    return Y.reshape((c_o, h, w))
```

### 5.二维卷积层

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/21/21-05.png)

### 6.总结

- 输出通道数是卷积层的超参数
- 每个输入通道有独立的二维卷积和，所有通道结果相加得到一个输出通道结果
- 每个输出通道有独立的三维卷积核

### 7.Q&A

##### Q1:网络越深，Padding 0 越多，这里是否会影响性能？

> 这里性能分为计算性能和网络性能，Padding 0 不会影响网络精度，但会使计算复杂

##### Q2:计算卷积时，bias的有无对结果影响大吗？bias的作用怎么解释？

> 因为正则化的操作，bias对结果影响不大，但加入bias对计算性能基本无影响，故默认加入bias

##### Q3:如果是一个rgb图像，加上深度图，相当于是四个通道吗？

> 不是，输入输出通道单列，这里使用3d的卷积，输入变为4维，核是5维

##### Q4:怎么理解1x1卷积核不识别空间模式？

> 因为输出的一个像素只对应输入的一个像素，所以没有获取到空间信息

##### Q5:是不是可以3x3x3和1x1xN的卷积层叠加，来进行空间信息的检测和信息融合，以及输出通道的调整？

> 是的，mobile net就是这种思想

##### Q6：3d卷积是处理视频问题的吧？也可以处理rgb加深度信息吗？

> 都可以，rgb加深度信息甚至可以用2d卷积处理。

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 22- 池化层

本节将介绍*池化*（pooling）层，它具有目的：类似于数据增强，降低卷积层对位置的敏感性；一定程度减少计算。

## 最大池化层和平均池化层

与卷积层类似，池化层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口遍历的每个位置计算一个输出。
然而，不同于卷积层中的输入与卷积核之间的互相关计算，**池化层不包含参数**。
相反，池运算符是确定性的，我们通常计算池化窗口中所有元素的最大值或平均值。这些操作分别称为*最大池化层*（maximum pooling）和*平均池化层*（average pooling）。

在这两种情况下，与互相关运算符一样，池化窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在池化窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。计算最大值或平均值是取决于使用了最大池化层还是平均池化层。

![池化窗口形状为 2\times 2 的最大池化层。着色部分是第一个输出元素，以及用于计算这个输出的输入元素 \max0, 1, 3, 4=4](http://d2l.ai/_images/pooling.svg) 上图中的输出张量的高度为2，宽度为2。这四个元素为每个池化窗口中的最大值：

$$
\max(0, 1, 3, 4)=4,\\
\max(1, 2, 4, 5)=5,\\
\max(3, 4, 6, 7)=7,\\
\max(4, 5, 7, 8)=8.\\
$$

池化窗口形状为p \times q的池化层称为p \times q池化层，池化操作称为p \times q池化。

回到本节开头提到的对象边缘检测示例，现在我们将使用卷积层的输出作为2\times 2最大池化的输入。
设置卷积层输入为`X`，池化层输出为`Y`。
无论`X[i, j]`和`X[i, j + 1]`的值是否不同，或`X[i, j + 1]`和`X[i, j + 2]`的值是否不同，池化层始终输出`Y[i, j] = 1`。
也就是说，使用2\times 2最大池化层，即使在高度或宽度上移动一个元素，卷积层仍然可以识别到模式。

在下面的代码中的`pool2d`函数，我们(**实现池化层的前向传播**)。然而，这里我们没有卷积核，输出为输入中每个区域的最大值或平均值。

```python
import torch
from torch import nn
from d2l import torch as d2l
```

```python
def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):  # 枚举输出的每个位置，[i,j]对应输入的位置[i至i+p_h,j至j+p_w]
            if mode == 'max':  # 最大池化
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()  # max函数返回最大值
            elif mode == 'avg':  # 平均池化
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()  # mean函数返回平均值
    return Y
```

我们可以构建上图中的输入张量`X`，[**验证二维最大池化层的输出**]。

```python
X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
pool2d(X, (2, 2))
```

```
tensor([[4., 5.],
        [7., 8.]])
```

此外，我们还可以(**验证平均池化层**)。

```python
pool2d(X, (2, 2), 'avg')
```

```
tensor([[2., 3.],
        [5., 6.]])
```

## 填充和步幅

与卷积层一样，池化层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。
下面，我们用深度学习框架中内置的二维最大池化层，来演示池化层中填充和步幅的使用。
我们首先构造了一个输入张量`X`，它有四个维度，其中样本数和通道数都是1。

```python
X = torch.arange(16, dtype=torch.float32).reshape(
    (1, 1, 4, 4))  # 维度[batch_size，通道数，H，W]
X
```

```
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
```

默认情况下，(**深度学习框架中的步幅与池化窗口的大小相同**)。
因此，如果我们使用形状为`(3, 3)`的池化窗口，那么默认情况下，我们得到的步幅形状为`(3, 3)`。

```python
pool2d = nn.MaxPool2d(3)
pool2d(X)
```

```
tensor([[[[10.]]]])
```

[**填充和步幅可以手动设定**]。

```python
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```

```
tensor([[[[ 5.,  7.],
          [13., 15.]]]])
```

当然，我们可以(**设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度**)。

```python
pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))
pool2d(X)
```

```
tensor([[[[ 5.,  7.],
          [13., 15.]]]])
```

## 多个通道

在处理多通道输入数据时，[**池化层在每个输入通道上单独运算**]，而不是像卷积层一样在通道上对输入进行汇总。
这意味着池化层的输出通道数与输入通道数相同。
下面，我们将在通道维度上连结张量`X`和`X + 1`，以构建具有2个通道的输入。

```python
X = torch.cat((X, X + 1), 1)  # 在第一个维度也就是通道维度拼接
X
```

```
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[ 1.,  2.,  3.,  4.],
          [ 5.,  6.,  7.,  8.],
          [ 9., 10., 11., 12.],
          [13., 14., 15., 16.]]]])
```

如下所示，池化后输出通道的数量仍然是2。

```python
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```

```
tensor([[[[ 5.,  7.],
          [13., 15.]],

         [[ 6.,  8.],
          [14., 16.]]]])
```

## 小结

- 对于给定输入元素，最大池化层会输出该窗口内的最大值，平均池化层会输出该窗口内的平均值。
- 池化层的主要优点之一是减轻卷积层对位置的过度敏感。
- 我们可以指定池化层的填充和步幅。
- 使用最大池化层以及大于1的步幅，可减少空间维度（如高度和宽度）。
- 池化层的输出通道数与输入通道数相同。

## 问题和练习

1. 你能将平均池化层作为卷积层的特殊情况实现吗？

> 设卷积层大小是m\times n，卷积层里面每个元素参数是\dfrac{1} {m\times n}，这样就是一个平均池化层作为卷积层的实现

1. 假设池化层的输入大小为c\times h\times w，则汇聚窗口的形状为p_h\times p_w，填充为(p_h, p_w)，步幅为(s_h, s_w)。这个池化层的计算成本是多少？

> c\times \left \lfloor \dfrac {h-p_h+s_h}{s_h}\right \rfloor \times \left \lfloor \dfrac {w-p_w+s_w}{s_w}\right \rfloor

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 23-经典卷积神经网络

### 1.LeNet卷积神经网络

#### 1.1 手写数字识别

- LeNet网络最早是为了应用于手写数字的识别应用。
- 应用背景：
  - 邮政局希望可以自动读出信件上的邮政编码
  - 人们希望可以用支票自动取钱
- 该模型在80年代末的银行被真正的部署

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-01.png)

#### 1.2 MNIST

- LeNet所使用的数据集
- 50，000个训练数据
- 10，000个测试数据
- 图像大小为28*28
- 10类

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-02.png)

#### 1.3 LeNet的具体模型

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-03.png)

#### 1.4 总结

- LeNet是早期成功的神经网络
- 先使用卷积层来学习图片空间信息
- 然后使用全连接层来转换到类别空间

### 2.代码部分

#### 2.1 定义网络结构和准备工作

- 导入所需的库

```python
#导入所需的库
import torch
from torch import nn
from d2l import torch as d2l
```

- 定义网络结构（具体可参考上文“具体模型”的图）

```python
#定义网络结构
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```

- 查看每一层数据的变化情况

```python
#把每一层数据的shape给打印出来
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)#创建符合要求的张量
for layer in net:
    X = layer(X)#通过每一层
    print(layer.__class__.__name__,'output shape: \t',X.shape)#打印
```

#### 2.2 模型训练

- 下载数据集

```python
batch_size = 256#批量大小
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)#下载或加载数据集，得到训练和测试集的迭代对象
```

- 使用GPU计算模型在数据集上的精度

```python
def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    """使用GPU计算模型在数据集上的精度"""
    if isinstance(net, nn.Module):
        net.eval()  # 设置为评估模式
        if not device:
            device = next(iter(net.parameters())).device
    # 正确预测的数量，总预测的数量
    metric = d2l.Accumulator(2)#创建一个累加器，包含2个要累加的元素
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERT微调所需的（之后将介绍）
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(d2l.accuracy(net(X), y), y.numel())#把每一组数据预测结果正确的个数和长度累加
    return metric[0] / metric[1]
```

- 训练函数

```python
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """用GPU训练模型(在第六章定义)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)#对linear类型的层用xavier初始化
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])#动画需要
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # 训练损失之和，训练准确率之和，范例数
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()#梯度清零
            X, y = X.to(device), y.to(device)
            y_hat = net(X)#正向传播
            l = loss(y_hat, y)#计算损失
            l.backward()#反向传播
            optimizer.step()#梯度下降
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])#训练损失之和，训练准确率之和，范例数
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)#评估测试集的精度
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
```

- 运行

```python
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/23/23-04.png)

#### 2.3 总结

- 卷积神经网络（CNN）是一类使用卷积层的网络。
- 在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。
- 为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。
- 在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。
- LeNet是最早发布的卷积神经网络之一（80年代）

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 24-AlexNet

### 本节目录

- [1.历史](#1%E5%8E%86%E5%8F%B2)
  
  - [1.1 2000 流行的机器学习方法——SVM，核方法](#11-2000-%E6%B5%81%E8%A1%8C%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95svm%E6%A0%B8%E6%96%B9%E6%B3%95)
  - [1.2 2000计算机视觉主要方法——几何学](#12-2000%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%BB%E8%A6%81%E6%96%B9%E6%B3%95%E5%87%A0%E4%BD%95%E5%AD%A6)
  - [1.3 2010计算机视觉的热点问题——特征工程](#13-2010%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E7%83%AD%E7%82%B9%E9%97%AE%E9%A2%98%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B)
  - [1.4 硬件的发展奠定了深度学习的兴起](#14-%E7%A1%AC%E4%BB%B6%E7%9A%84%E5%8F%91%E5%B1%95%E5%A5%A0%E5%AE%9A%E4%BA%86%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%B4%E8%B5%B7)
  - [1.5 ImageNet（2010）](#15-imagenet2010)

- [2.AlexNet架构](#2alexnet%E6%9E%B6%E6%9E%84)

- [3.总结](#3%E6%80%BB%E7%BB%93)

- [4.QA](#4qa)
  
  ### 1.历史

#### 1.1 2000 流行的机器学习方法——SVM，核方法

- 核方法替代了之前的神经网络网络方法，SVM对于调参不敏感，现在也有一些应用

- 本质上是特征提取，具体的方法是选择核函数来计算，把特征映射到高纬空间，使得他们线性可分

- 经过核函数计算之后，原问题可以转化为凸优化问题，这是2006年左右的研究热点

- 核方法有很多漂亮的定理，有很好的数学解释性

- 2010年左右，深度学习才兴起

#### 1.2 2000计算机视觉主要方法——几何学

- 首先还是对图片进行特征抽取
- 希望把计算机视觉问题描述成几何问题，建立（非）凸优化模型，可以得到很多漂亮的定理。
- 可以假设这是一个几何问题，假设这个假设被满足了，可以推出很好的效果

#### 1.3 2010计算机视觉的热点问题——特征工程

- 特征工程就是怎么抽取一张图片的特征，因为直接输入一张图片效果非常的差
- 特征描述子：SIFT,SURF

#### 1.4 硬件的发展奠定了深度学习的兴起

- 数据的增长，硬件的计算能力奠定了人们对于方法的选择

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-01.png)

#### 1.5 ImageNet（2010）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-02.png)

- AlexNet赢得了2012年ImageNet竞赛冠军

- 本质上是一个加强版的LeNet，更深更大

- AlexNet主要改进措施：
  
  - dropout（正则）
  - ReLu（梯度更大）
  - MaxPooling（取最大值，梯度相对增大）

- 影响：计算机视觉方法论的改变，从人工提取特征过渡到CNN学习特征
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-03.png)

### 2.AlexNet架构

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-04.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-05.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-06.png)

- 网络代码

```python
net = nn.Sequential(
这里，我们使用一个11*11的更大窗口来捕捉对象。
    # 同时，步幅为4，以减少输出的高度和宽度。
    # 另外，输出通道的数目远大于LeNet
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 使用三个连续的卷积层和较小的卷积窗口。
    # 除了最后的卷积层，输出通道的数量进一步增加。
    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
    nn.Linear(4096, 10))
```

- 更多细节
  - 激活函数从sigmoid变成Relu，减缓梯度消失
  - 隐藏全连接层后加入了丢弃层（2个4096之后加入了dropout）
  - 数据增强，将一张图片进行变化，选取多个位置、光照之类的。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-07.png)

- 复杂度对比
  - 参数个数增加，每次更新数据增加

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/24/24-08.png)

### 3.总结

- AlexNet 是更大更深的LeNet，10x参数个数，260x计算复杂度
- 新加入了dropout，relu，maxpooling，数据增强
- 标志着新一轮神经网络热潮开始了

### 4.QA

- 问题大部分都在问如何炼丹，炼丹的理论，为啥炼丹的步骤要这样不哪有？
  
  - 老师说这个确实不好理解，只能从自己的角度去尝试解释

- 数据增强了，但是效果还不如之前的，为啥？
  
  - 太正常了，属于超参数没调好

- 为啥LeNet不属于深度卷积神经网络？
  
  - 为了包装现在的产品，更好卖（确实是这么回答的），这个我们研究者需要学习，好好宣传自己的产品

- 网络要求输入的size是固定的，实际使用的时候图片不一定是要求的size，怎么处理？
  
  - 如果是大的图片，在保持长宽比的情况下，把短边压成输入的size，然后在新的图片中随机抠出来几张图片（要求和网络输入一致）进行预测。效果上不会有太大的影响
  
  <mark>**整合版来自→公众号：坚持打代码**</mark>

# 25 -使用块的网络 VGG

### 目录

- [1. VGG块](#1-vgg%E5%9D%97)
  - [2. VGG架构](#2-vgg%E6%9E%B6%E6%9E%84)
  - [3. 总结](#3-%E6%80%BB%E7%BB%93)
  - [4. QA](#4-qa)

Alexnet最大的问题在于长得不规则，结构不甚清晰，也不便于调整。想要把网络做的更深更大需要更好的设计思想和标准框架。

### 1. VGG块

直到现在更深更大的模型也是我们努力的方向，在当时AlexNet比LeNet更深更大得到了更好的精度，大家也希望把网络做的更深更大。选择之一是使用更多的全连接层，但全连接层的成本很高；第二个选择是使用更多的卷积层，但缺乏好的指导思想来说明在哪加，加多少。最终VGG采取了将卷积层组合成块，再把卷积块组合到一起的思路。

VGG块可以看作是AlexNet思路的拓展，AlexNet中将三个相同的卷积层放在一起再加上一个池化层，而VGG将其拓展成可以使用任意个3x3，不改变输入大小的的卷积层，最后加上一个2x2的最大池化层。

![2501](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/25/25-01.PNG)

为什么选择3x3卷积呢？在计算量相同的情况下选用更大的卷积核涉及对网络会越浅，VGG作者经过实验发现用3x3卷积的效果要比5x5好，也就是说神经网络库深且窄的效果会更好。

### 2. VGG架构

多个VGG块后接全连接层，不同次数的重复块得到不同的架构，如VGG-16, VGG-19等，后面的数字取决于网络层数。

可以讲VGG看作是将AlexNet中连续卷积的部分取出加以推广和复制，并删去了AlexNet中不那么规整的前几层。

![2502](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/25/25-02.PNG)

VGG较AlexNet相比性能有很大的提升，而代价是处理样本速度的降低和内存占用的增加。

### 3. 总结

- VGG使用可重复使用的卷积块来构建深度卷积网络

- 不同卷积块个数和超参数可以得到不同复杂度的变种

这些思想影响了后面神经网络的设计，在之后的模型中被广泛使用。

### 4. QA

Q1: 视觉领域人工特征的研究还有无进展？

> 现在在计算机视觉做人工特征是一种“政治不正确”的事，可能会因被认为没有novelty而发不出paper ;-)
> 
> 老师认为人工特征提取确实应该被取代掉，随着技术进步可以把这部分工作交给机器，人去做更高级的事。

Q2: 需要学习特征值/特征向量/奇异值分解的知识吗？

> 这门课中不一定会讲，但很多深度学习模型用到矩阵分解的思想，但是用的不多，想学可以学。

Q3: Colab限时12小时与验证码的解决方法

> 充钱

Q4: 训练loss一直下降，测试loss一只不降的原因

> 代码写错了/过拟合(训练集和测试集很不一样)

Q5: 为什么VGG（1，1，224，224）输入高宽减半后通道数是64？

> 第一个卷积层的输出通道选的是64。(通道数变化是自定的，和高宽变化没有关系)

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 26-网络中的网络（NiN）

### 1. 动机

**全连接层的问题**

- **卷积层**需要的**参数较少**
- 而卷积层后的第一个**全连接层**的**参数较多**

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-01.png)

以VGG为例(图示)，全连接层需要先Flatten，输入维度为512x7x7，输出维度为4096，则需要参数个数为512x7x7x4096=102M。

### 2. NiN块

- 核心思想：一个卷积层后面跟两个1x1的卷积层，后两层起到全连接层的作用。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-02.png)

### 3. NiN架构

- 无全连接层
- 交替使用NiN块和步幅为2的最大池化层
  - 逐步减小高宽和增大通道数
- 最后使用全局平均池化得到输出
  - 其输入通道是类别数

### 4. NiN Networks

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/26/26-03.png)

NiN架构如上图右边所示，若干个NiN块(图示中为4个块)+池化层；前3个块后接最大池化层，最后一块连接一个全局平均池化层。

### 5. 总结

- NiN块结构：使用卷积层加两个1x1卷积层
  - 后者对每个像素增加了非线性性
- NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层
  - 不容易过拟合，更少的参数个数

### 6.代码

```python
# 如果在Colab上跑, 或没有安装过d2l包, 需要最开始pip install d2l
!pip install git+https://github.com/d2l-ai/d2l-zh@release  # installing d2l
```

**NiN块**

```python
import torch
from torch import nn
from d2l import torch as d2l

# 定义NiN块
def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1),
        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1),
        nn.ReLU())
```

**NiN模型**

```python
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),
    nn.MaxPool2d(3, stride=2),
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),
    nn.MaxPool2d(3, stride=2), nn.Dropout(0.5),
    # 标签类别数是10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),
    nn.AdaptiveAvgPool2d((1, 1)),          #全局平均池化，高宽都变成1
    nn.Flatten())             #消掉最后两个维度, 变成(batch_size, 10)
```

**demo测试，查看每个块的输出情况**

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__, 'output shape:\t', X.shape)
```

```python
>>>
Sequential output shape:     torch.Size([1, 96, 54, 54])
MaxPool2d output shape:         torch.Size([1, 96, 26, 26])
Sequential output shape:     torch.Size([1, 256, 26, 26])
MaxPool2d output shape:         torch.Size([1, 256, 12, 12])
Sequential output shape:     torch.Size([1, 384, 12, 12])
MaxPool2d output shape:         torch.Size([1, 384, 5, 5])
Dropout output shape:         torch.Size([1, 384, 5, 5])
Sequential output shape:     torch.Size([1, 10, 5, 5])
AdaptiveAvgPool2d output shape:     torch.Size([1, 10, 1, 1])
Flatten output shape:         torch.Size([1, 10])
```

**训练模型**

```python
lr, num_epochs, batch_size = 0.1, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

```python
>>> <Figure size 252x180 with 1 Axes>
```

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 27- GooLeNet

#### 目录

- [GooLeNet](#goolenet)
- [目录](#%E7%9B%AE%E5%BD%95)
- [含并行连结的网络](#%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C)
- [Inception块](#inception%E5%9D%97)
- [GooLeNet模型](#goolenet%E6%A8%A1%E5%9E%8B)
- [总结](#%E6%80%BB%E7%BB%93)

#### 含并行连结的网络

- GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。我们往往不确定到底选取什么样的层效果更好，到底是3X3卷积层还是5X5的卷积层，诸如此类的问题是GooLeNet选择了另一种思路“小学生才做选择，我全都要”，这也使得GooLeNet成为了第一个模型中超过1000个层的模型。

#### Inception块

- 在GoogLeNet中，基本的卷积块被称为*Inception块*（Inception block）
  
  ![截屏20220123 上午101118](https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/27/27-1.png)

- Inception块由四条并行路径组成。 前三条路径使用窗口大小为1×11×1、3×33×3和5×55×5的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行1×11×1卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用3×33×3最大汇聚层，然后使用1×11×1卷积层来改变通道数。 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。

#### GooLeNet模型

- GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。 第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。
  
  ![截屏20220123 上午101711](https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/27/27-2.png)

- 第一个模块是7×7卷积层。

- 第二个模块使用两个卷积层：第一个卷积层是1×1卷积层；第二个卷积层使用将通道数量增加三倍的3×3卷积层。 这对应于Inception块中的第二条路径。

- 第三个模块串联两个完整的Inception块。 第一个Inception块的输出通道数为64+128+32+32=25664+128+32+32=256，四个路径之间的输出通道数量比为64:128:32:32=2:4:1:164:128:32:32=2:4:1:1。 第二个和第三个路径首先将输入通道的数量分别减少到96/192=1/296/192=1/2和16/192=1/1216/192=1/12，然后连接第二个卷积层。第二个Inception块的输出通道数增加到128+192+96+64=480128+192+96+64=480，四个路径之间的输出通道数量比为128:192:96:64=4:6:3:2128:192:96:64=4:6:3:2。 第二条和第三条路径首先将输入通道的数量分别减少到128/256=1/2128/256=1/2和32/256=1/832/256=1/8。

- 第四模块更加复杂， 它串联了5个Inception块，其输出通道数分别是192+208+48+64=512192+208+48+64=512、160+224+64+64=512160+224+64+64=512、128+256+64+64=512128+256+64+64=512、112+288+64+64=528112+288+64+64=528和256+320+128+128=832256+320+128+128=832。 这些路径的通道数分配和第三模块中的类似，首先是含3×3卷积层的第二条路径输出最多通道，其次是仅含1×1卷积层的第一条路径，之后是含5×5卷积层的第三条路径和含3×3最大汇聚层的第四条路径。 其中第二、第三条路径都会先按比例减小通道数。 这些比例在各个Inception块中都略有不同。

#### 总结

- Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用1×1卷积层减少每像素级别上的通道维数从而降低模型复杂度。

- GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。

- GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。

**<mark>整合版来自→公众号：坚持打代码</mark>**

# 28-批量归一化

[批量归一化](#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96)

- [训练深层网络时的问题](#%E8%AE%AD%E7%BB%83%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C%E6%97%B6%E7%9A%84%E9%97%AE%E9%A2%98)
- [批量归一化](#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96-1)
- [形式化表达](#%E5%BD%A2%E5%BC%8F%E5%8C%96%E8%A1%A8%E8%BE%BE)
- [批量归一化层](#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82)
  - [全连接层](#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82)
  - [卷积层](#%E5%8D%B7%E7%A7%AF%E5%B1%82)
  - [预测过程中的批量归一化](#%E9%A2%84%E6%B5%8B%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96)
  - [实现细节](#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82)
- [吴恩达老师深度学习课程中的批量归一化](#%E5%90%B4%E6%81%A9%E8%BE%BE%E8%80%81%E5%B8%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96)

## 批量归一化

深层神经网络的训练，尤其是使网络在较短时间内收敛是十分困难的，**批量归一化[batch normalization]**是一种流行且有效的技术，能加速深层网络的收敛速度，目前仍被广泛使用。

### 训练深层网络时的问题

![deepmodel](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/28/deep_model.png)

深度神经网络在训练时会遇到一些问题：

- 收敛速度慢：
  
  - 由于训练时先正向传播后反向传播，且每层的梯度一般较小，若网络较深，则反向传播时会出现类似于梯度消失的现象，导致距离数据更近的层梯度较小，收敛慢，而距离输出更近的层梯度较大，收敛快。然而底部的层一般都用于提取较基础的特征信息，上方的层收敛后，由于底部提取基础特征的层仍在变化，上方的层一直在不停的重新训练，导致整个网络难以收敛，训练较慢。

- 内部协变量转移：
  
  - 分布偏移：偏移在视频课程中并未出现，但在《动手学深度学习》这本书中有提到过，在[4.9. 环境和分布偏移](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/environment.html)部分。偏移指的是训练数据可能和测试数据的分布不同，比如利用来自真实的猫和狗的照片的训练数据训练模型，然后让模型去预测动画中的猫和狗的图片。![catdogtrain](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/28/cat-dog-train.svg)
    
    ![catdogtest](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/28/cat-dog-test.svg)
  
  - 这显然会降低正确率也会对模型的进一步优化带来干扰。一般情况下对于分布偏移我们毫无办法，然而，在一些特定场景中，如果假定一些训练数据和测试数据分布的前提条件，就能对分布偏移进行处理，其中之一就是协变量偏移。
  
  - 协变量偏移：协变量偏移假设输入的分布可能随时间变化，但标签函数（条件分布P(y|\bold x)）没有改变。统计学家称这为*协变量偏移*（covariate shift）并给出了一些解决方案
  
  - **内部协变量偏移(Internal Covariate Shift)**：每一层的参数在更新过程中，会改变下一层输入的分布，导致网络参数变幻莫测，难以收敛，神经网络层数越多，表现得越明显。
  
  - 注意：
    
    - 1：内部协变量偏移这个词与标准的协变量偏移所有区别。
    - 2：能缓解内部协变量偏移仅仅是批量归一化的作者提出的假想，后续论文证实批量归一化实际对内部协变量偏移的缓解帮助不大
    - 3：批量归一化一般只影响模型的收敛速度，不影响精度

- 过拟合：
  
  - 由于网络深度加深，变得更为复杂，使得网络容易过拟合。

### 批量归一化

**批量归一化(batch normalization)**在 [[Ioffe & Szegedy, 2015]](https://zh-v2.d2l.ai/chapter_references/zreferences.html#ioffe-szegedy-2015)中被提出，用于解决上述训练深度网络时的这些问题，然而这只是人们的感性理解，关于批量归一化具体是怎样帮助训练这个问题目前仍待进一步研究。

批量归一化尝试将每个训练中的mini-batch小批量数据（即会导致参数更新的数据）在每一层的结果进行归一化，使其更稳定，归一化指的是对于当前小批量中的所有样本，求出期望和方差，然后将每个样本减去期望再除以标准差。

### 形式化表达

下面的运算均为向量运算，向量中的每个维度代表一个特征，对于每个特征分别进行计算再拼接在一起即为向量运算。

设 \bold x \in \mathcal{B}为来自一个小批量\mathcal{B}的输入，批量规范化BN根据下式进行转换
$$
\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}*\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}.
$ 式中\hat{\boldsymbol{\mu}}_\mathcal{B}为小批量\mathcal{B}样本均值，\hat{\boldsymbol{\sigma}}_\mathcal{B}$为样本标准差：
$$
\begin{split}\begin{aligned} \hat{\boldsymbol{\mu}}_\mathcal{B} &= \frac{1}{|\mathcal{B}|} \sum*{\mathbf{x} \in \mathcal{B}} \mathbf{x},\
\hat{\boldsymbol{\sigma}}*\mathcal{B}^2 &= \frac{1}{|\mathcal{B}|} \sum*{\mathbf{x} \in \mathcal{B}} (\mathbf{x} - \hat{\boldsymbol{\mu}}_{\mathcal{B}})^2 + \epsilon\end{aligned}\end{split}
$ 其中\epsilon用于防止分母为0，经过减期望与除以标准差后得到期望为1方差为0的小批量数据。然而，期望和方差为了使小批量有更自由的选择，再将其乘拉伸参数\boldsymbol {\gamma}，加偏移参数\boldsymbol \beta，这两个参数与\bold x$同样大小，是模型中的可学习参数，与其他参数一同更新。

由于\hat{\boldsymbol{\mu}}_\mathcal{B}和\hat{\boldsymbol{\sigma}}_\mathcal{B}为由当前小批量计算的值，实际上是整个分布对应的期望与标准差的估计值，由于小批量的随机选择，\hat{\boldsymbol{\mu}}_\mathcal{B}和\hat{\boldsymbol{\sigma}}_\mathcal{B}会给模型带来一定的与输入数据有关的噪音，而这些噪音也能对模型进行正则化，防止过拟合。为何这种噪音能加快训练并带来正则化还有待研究，不过已有理论说明了为什么批量规范化最适应50∼100范围中的中等批量大小的问题。

训练时不能使用整个数据集，只能一步步的训练和更新；而预测时模型已然固定，可以根据整个数据集精确计算均值和方差。因此，批量归一化对于训练和预测时有两种不同模式。

### 批量归一化层

批量归一化不再单独的考虑单个样本，需要对整个mini-batch进行，因此需要考虑多种情况。

#### 全连接层

通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。如下：
$$
\mathbf{h} = \phi(\mathrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}))
$$

#### 卷积层

在卷积层中，我们将通道视作每个位置的特征，将每个样本中的每个位置视作一个样本进行计算。每个通道都有着自己的拉伸参数{\gamma}和偏移参数\beta，所有通道加在一起组成了拉伸参数向量\boldsymbol {\gamma}和偏移参数向量\boldsymbol \beta，若样本数为m，卷积输出为p*q，计算时对m*p*q个向量进行批量归一化运算（即视作有m*p*q个样本）

#### 预测过程中的批量归一化

在训练过程中，我们需要不断地更新模型，方差和均值也就在不断地变化，就必须计算当前小批量数据对应的方差和均值，然而预测时我们的模型已经确定下来，可以用在整个训练数据集上得到的均值和方差来对预测时的结果进行归一化。

#### 实现细节

- 在实际实现时，一般使用指数加权平均来更新小批量的均值和方差，指数加权平均将旧值和当前计算结果不断进行加权平均，最终做到平滑的向更新值靠拢，公式如下：

- $$
  S_t = 
\begin{cases} 
Y_1, &t = 1 \\\\ 
\beta S_{t-1} + (1-\beta)Y_t, &t > 1 
\end{cases}
  $$

- 批量归一化的参数可以通过动量梯度下降，RMSProp，Adam等多种优化方法进行训练。

### 吴恩达老师深度学习课程中的批量归一化

吴恩达老师深度学习课程中的批量归一化中的部分内容与本课程有所出入，考虑到批量归一化这部分内容还没有精确的理论解释，目前的认识仅限于直觉，故将两课程中的区别即补充罗列在此作为参考：

- 关于dropout：
  - 本课中提到批量归一化有正则化效果，无需再进行dropout
  - 吴恩达老师课程中提到批量归一化正则化效果较差，不能作为正则化的手段，必要时需要dropout
- 对于线性层（包括其他带有偏置项的层）后的批量归一化，由于归一化时减去了均值，偏置项被消掉，可以省略归一化层之前的偏置项
- 标准化的输入能使梯度下降加快，批归一化能使得每层的输入都被归一化，这也是训练更快的原因之一
- 批量归一化可以使得不同层之间互相的影响减少，从而应对数据偏移，增强鲁棒性。

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 29-残差网络（ResNet）

目录

- [残差网络（ResNet）](#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet)

- [函数类](#%E5%87%BD%E6%95%B0%E7%B1%BB)

- [残差块](#%E6%AE%8B%E5%B7%AE%E5%9D%97)

- [ResNet模型](#resnet%E6%A8%A1%E5%9E%8B)

- [训练模型](#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B)

- [总结](#%E6%80%BB%E7%BB%93)

### 残差网络（ResNet）

随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在ResNet这种网络中，添加层会使网络更具表现力

#### 函数类

- 假设有一类特定的神经网络架构F，它包括学习速率和其他超参数设置。 对于所有*f*∈F，存在一些参数集（例如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。 现在假设*f*∗是我们真正想要找到的函数，如果是*f*∗∈F，那我们可以轻而易举的训练得到它，但通常我们不会那么幸运。 相反，我们将尝试找到一个函数*f*∗，这是我们在F中的最佳选择。
- 为了得到更近似真正*f*∗的函数我们需要设计一个更强大的架构F'，但是如果先前的框架F不包含于新框架F‘中就可能导致如下图中左侧的最优函数离实际预测函数误差反而随框架边强而增大，这不是我们期望的结果，所以我们选择使用下图中右侧的嵌套函数类以解决这个问题
- 引入方法：对于深度神经网络，如果我们能将新添加的层训练成*恒等映射*（identity function）*f*(**x**)=**x**，新模型和原模型将同样有效。 同时，由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/29/29-01.png)

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/29/29-02.png)

#### 残差块

- 神经网络中的具体实现：假设我们的原始输入为*x*，而希望学出的理想映射为*f*(**x**)，左图虚线框中的部分需要直接拟合出该映射*f*(**x**)，而右图虚线框中的部分则需要拟合出残差映射*f*(**x**)−**x**。而右图正是ResNet的基础架构–*残差块*（residual block）

- 残差块的代码实现：

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l


class Residual(nn.Module):
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        # 第一个卷积层
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        # 第二个卷积层
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        # 如果使用1 x 1卷积以使得输入变换成需要的形状
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        # 对应第一个卷积层的批量规范化层
        self.bn1 = nn.BatchNorm2d(num_channels)
        # 对应第二个卷积层的批量规范化层
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        # 第一层：卷积 -> 规范化 -> relu激活
        Y = F.relu(self.bn1(self.conv1(X)))
        # 第二层：卷积 -> 规范化
        Y = self.bn2(self.conv2(Y))
        # 如果要让输入变换成需要的形状
        if self.conv3:
            # 对X使用1 x 1卷积，以使输出成为需要的形状
            X = self.conv3(X)
        # 嵌套模型的实现，即对上一次训练后的模型进行嵌套
        Y += X
        # relu激活并输出
        return F.relu(Y)
```

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/29/29-03.png)

#### ResNet模型

- ResNet的前两层跟之前介绍的GoogLeNet中的一样： 在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3的最大汇聚层。 不同之处在于ResNet每个卷积层后增加了批量规范化层。

```python
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.BatchNorm2d(64), nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

- GoogLeNet在后面接了4个由Inception块组成的模块。 ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。

```python
def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels, num_channels,
                                use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk
```

- 接着在ResNet加入所有残差块，这里每个模块使用2个残差块。

```python
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))
```

- 最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。

```python
net = nn.Sequential(b1, b2, b3, b4, b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(), nn.Linear(512, 10))
```

- 每个模块有4个卷积层（不包括恒等映射的1×1卷积层）。 加上第一个7×7卷积层和最后一个全连接层，共有18层。 因此，这种模型通常被称为ResNet-18。 通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。 虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/29/29-04.png)

#### 训练模型

- 我们在Fashion-MNIST数据集上训练ResNet

```python
lr, num_epochs, batch_size = 0.05, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/29/29-05.png)

#### 总结

- 学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。
- 残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。
- 利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。
- 残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。

<mark>**整合版来自→公众号：坚持打代码**</mark>

# 30 第二部分完结竞赛：图片分类

竞赛地址：[Classify Leaves | Kaggle](https://www.kaggle.com/c/classify-leaves)

任务：给出叶子图片预测树种，共20000张图，176类，每类至少50张图。训练样本18353张，测试样本8800张。

这次公榜私榜是随机分的，正常来说两榜的名次差别不会太大。

# 31-CPU和GPU

### 本节目录：

- [CPU和GPU](#cpu%E5%92%8Cgpu)
  
  - [本节目录：](#%E6%9C%AC%E8%8A%82%E7%9B%AE%E5%BD%95)
  
  - [1.CPU：](#1cpu)
    
    - [1.1 提升CPU利用率一：](#11-%E6%8F%90%E5%8D%87cpu%E5%88%A9%E7%94%A8%E7%8E%87%E4%B8%80)
    - [1.2 样例分析：](#12-%E6%A0%B7%E4%BE%8B%E5%88%86%E6%9E%90)
    - [1.3 提升CPU利用率二：](#13-%E6%8F%90%E5%8D%87cpu%E5%88%A9%E7%94%A8%E7%8E%87%E4%BA%8C)
    - [1.4 样例分析：](#14-%E6%A0%B7%E4%BE%8B%E5%88%86%E6%9E%90)
  
  - [2.CPU vs GPU:](#2cpu-vs-gpu)
    
    - [2.1 提升GPU利用率](#21-%E6%8F%90%E5%8D%87gpu%E5%88%A9%E7%94%A8%E7%8E%87)
    - [2.2 CPU/GPU 带宽](#22-cpugpu-%E5%B8%A6%E5%AE%BD)
    - [2.3 更多的CPUs和GPUs](#23-%E6%9B%B4%E5%A4%9A%E7%9A%84cpus%E5%92%8Cgpus)
    - [2.4 CPU/GPU高性能计算编程](#24-cpugpu%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97%E7%BC%96%E7%A8%8B)
  
  - [3.总结](#3%E6%80%BB%E7%BB%93)
  
  - [4.Q&A](#4qa)
    
    ### 1.CPU：

#### 1.1 提升CPU利用率一：

- 在计算a+b之前，需要准备数据

- 主内存->L3->L2->L1->寄存器
  
  - L1访问延时：0.5ms
  - L2访问延时：7ns（14XL1）
  - 主内存访问延时：100ns(200XL1)

- 提升空间和时间的内存本地性
  
  - 时间：重用数据使它们在缓存里
  - 空间：按序读写数据是的可以预读取

#### 1.2 样例分析：

- 如果一个矩阵是按行存储，访问一行比访问一列要快
  
  - CPU一次读取64字节（缓存线）
  - CPU会“聪明的”提前读取下一个（缓存线）
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/31/31-01.png)

#### 1.3 提升CPU利用率二：

- 高端CPU有几十个核
  
  - EC2 P3.16xlarge:2 Intel Xeon CPUs,32物理核

- 并行来利用所用核
  
  - 超线程不一定提升性能，因为他们共享寄存器

#### 1.4 样例分析：

- 左边比右边慢（python）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/31/31-02.png)

- 左边调用n次函数，每次调用有开销
- 右边很容易被并行（例如下面的C++实现）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/31/31-03.png)

### 2.CPU vs GPU:

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/31/31-04.png)

#### 2.1 提升GPU利用率

- 并行
  
  - 使用数千个线程

- 内存本地性
  
  - 缓存更小，架构更简单

- 少用控制语句
  
  - 支持有限
  - 同步开销大

#### 2.2 CPU/GPU 带宽

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/31/31-05.png)

#### 2.3 更多的CPUs和GPUs

- CPU:AMD,ARM
- GPU:AMD,Intel,ARM,Qualcomm...

#### 2.4 CPU/GPU高性能计算编程

- CPU：C++或者任何高性能语言
  - 编译器成熟
- GPU
  - Nvida上用CUDA
    - 编译器和驱动成熟
  - 其他用OpenCL
    - 质量取决于硬件厂商

### 3.总结

- CPU:可以处理通用计算。性能优化考虑数据读写效率和多线程
- GPU：使用更多的小核和更好的内存带宽，适合能大规模并行的计算任务

### 4.Q&A

##### Q1: 如果要提高泛化性，就要增加数据？调参的意思是不是最大？

> 提高泛化性的有效手段是增加数据，但是数据的质量很重要，少量高质量数据和大量低质量数据可能有1:10或者1:100的换算关系。实际应用场景对调参要求不高，因为有不断增加的数据。

##### Q2:alexnet模型比resnet要大，为什么计算上resnet比alexnet运算量大？

> alexnet后面用到的几个连续的全连接层使模型变大，但是resnet使用的卷积层在少量参数下更消耗计算资源。模型大小和计算复杂度不能直接换算。

##### Q3:训练时为什么使用w-=lr*w.grad,而不写做w=w-lr*w.grad?

> 因为第二种写法定义了一个新的tensor，梯度参数会成为false

##### Q4:llc是显存还是缓存，是l1,l2,还是l3?

> llc是缓存，last level cash,是最后一层缓存，具体是ln取决于一共有几层缓存。

##### Q5:做计算时把for_lopps运算尽可能向量化？

> 是的，尽量不要用python写for-loop

##### Q6:可视化时，需要把数据在cpu和GPU之间切换，如何避免频繁传输？常见的错误操作有哪些？怎么看到和排查这种错误？

> 可视化操作不需要太担心，只要不是计算中来回传递就好。深度学习框架会有限制，只能在一个设备上做。框架没报错一般不会有太多问题

##### Q7:go怎么样？

> go分布式系统做的很好，和深度学习的分布式不太一样

##### Q8:怎样复现论文？

> 80%的论文无法复现，要读懂每一句话，和明白作者实现的细节。

##### Q9：分布式和高性能的区别？

> 没有本质区别，分布式更多考虑容错。高性能是分布式的一个应用

##### Q10:自动驾驶烧钱，短时间难以落地是不是和nas一样？

> 不是，自动驾驶有很好的商业前景。nas没有太多意义。

# 32-深度学习硬件

### 目录

- [32-深度学习硬件](#32-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%A1%AC%E4%BB%B6)
  
  - [目录](#%E7%9B%AE%E5%BD%95)
  - [1.DSP:数字信号处理](#1dsp%E6%95%B0%E5%AD%97%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86)
  - [2.可编程阵列（FPGA）](#2%E5%8F%AF%E7%BC%96%E7%A8%8B%E9%98%B5%E5%88%97fpga)
  - [3.AI ASIC](#3ai-asic)
  - [总结](#%E6%80%BB%E7%BB%93)

- 本节我们介绍除了 GPU CPU 之外更多的芯片

- 引入：手机内部的芯片有很多——GPU CPU ISP WIFI……

### 1.DSP:数字信号处理

- 为数字信号处理算法设计：点积、卷积、FFT

- 低功耗，高性能
  
  - 比移动GPU快5倍，功耗更低

- VLIW：very long instruction word
  
  - 频率低，核少，但是一条指令可以进行上百次的累加，便于重复

- 缺点：编程和调试困难，编译器良莠不齐（做的人少，工具不是很好用）

### 2.可编程阵列（FPGA）

- 有大量的可以用来编程的逻辑单元和可配置链接
- 可以配置成计算复杂函数
  - 编程语言：VHDL Verilog
- 通常比通用硬件更高效，但是体积更大不方便
- 缺点：工具链质量良莠不齐，一次编译需要数个小时（烧一次板子，物理上的改变）
- 用途：主要用来模拟，看看效果好不好，如果好可以进一步造芯片

### 3.AI ASIC

- 深度学习热门领域（针对特定领域）
  - 大公司都在造自己的芯片（Intel Qualcomm Google Amazon Facebook）
- Google TPU 是标志性芯片（听说在Google内部已经盛行 取代GPU了）
  - 能够媲美 Nvidia GPU性能
  - 在Google 大量部署
  - 核心是 systolic array（时间快 容易造）
- systolic array
  - 计算单元（PE）阵列
  - 特别适合做矩阵乘法
  - 设计和制造相对简单（核少）
  - 矩阵乘法例子：见PPT
    - 对于一般的矩阵乘法：通过切开、填充来匹配SA大小
    - 批量输入来降低延迟（避免空等，先出的硬件空闲）
    - 通常有其他硬件单元来处理别的NN操作子，例如激活层
  - 缺点：只针对深度学习这方面有用，别的方面效果不大

### 总结

- 灵活性、易用性：Intel(CPU) > GPU > DSP > FPGA > ASIC
- 性能功耗：Intel(CPU) < GPU < DSP < FPGA < ASIC

## 单机多卡并行

一台机器可以安装多个GPU（一般为1-16个），在训练和预测时可以将一个小批量计算切分到多个GPU上来达到加速目的，常用的切分方案有数据并行，模型并行，通道并行。

### 数据并行

将小批量的数据分为n块，每个GPU拿到完整的参数，对这一块的数据进行前向传播与反向传播，计算梯度。

数据并行通常性能比模型并行更好，因为对数据进行划分使得各个GPU的计算内容更加均匀。

#### 数据并行的大致流程

主要分为五部

- 1：每个GPU读取一个数据块（灰色部分）
- 2：每个GPU读取当前模型的参数（橙色部分）
- 3：每个GPU计算自己拿到数据块的梯度（绿色部分）
- 4：GPU将计算得到的梯度传给内存（CPU）（绿色箭头）
- 5：利用梯度对模型参数进行更新（橙色箭头）

数据并行并行性较好，主要因为当每个GPU拿到的数据量相同时计算量也相似，各个GPU的运算时间相近，幸能较好

### 模型并行

将整个模型分为n个部分，每个GPU拿到这个部分的参数和负责上一个部分的GPU的输出作为输入来进行计算，反向传播同理。

模型并行通常用于模型十分巨大，参数众多，即使在每个mini-batch只有一个样本的情况下单个GPU的显存仍然不够的情况，但并行性较差，可能有时会有GPU处于等待状态。

### 通道并行

通道并行是数据并行和模型并行同时进行

### 总结

- 当一个模型能用单卡计算时，通常使用数据并行扩展到多卡
- 模型并行则用在超大模型上

### Q&A（部分有价值的）

- 问1：若有4块GPU，两块显存大两块显存小怎么办？

- 答1：
  若GPU运算性能相同，则训练取决于小显存的GPU的显存大小，更大的显存相当于浪费掉
  若GPU运算性能不同，一般即为显存大的GPU性能更好，可以在分配数据时多分配一点

- 问2：数据拆分后，需存储的数据量会变大吗？会降低性能吗？

- 答2：每个GPU都单独存储了一份模型，这部分的数据量变大了，但如果只考虑运算时的中间变量，则中间变量的大小与数据量呈线性关系，每个GPU的数据小了，中间变量也会变小，所有GPU的中间变量加起来大小是不变的。
  数据拆分后性能会变低，在下节课讲解（数据通讯的开销，每个GPU的batch-size变小可能无法跑满GPU，总batch-size变大则相同计算量下训练次数变少）

## 单机多卡并行

一台机器可以安装多个GPU（一般为1-16个），在训练和预测时可以将一个小批量计算切分到多个GPU上来达到加速目的，常用的切分方案有数据并行，模型并行，通道并行。

### 数据并行

将小批量的数据分为n块，每个GPU拿到完整的参数，对这一块的数据进行前向传播与反向传播，计算梯度。

数据并行通常性能比模型并行更好，因为对数据进行划分使得各个GPU的计算内容更加均匀。

#### 数据并行的大致流程

<img src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/33/DataParallel.png" title="" alt="DataParallel" data-align="center">

主要分为五部

- 1：每个GPU读取一个数据块（灰色部分）
- 2：每个GPU读取当前模型的参数（橙色部分）
- 3：每个GPU计算自己拿到数据块的梯度（绿色部分）
- 4：GPU将计算得到的梯度传给内存（CPU）（绿色箭头）
- 5：利用梯度对模型参数进行更新（橙色箭头）

数据并行并行性较好，主要因为当每个GPU拿到的数据量相同时计算量也相似，各个GPU的运算时间相近，幸能较好

### 模型并行

将整个模型分为n个部分，每个GPU拿到这个部分的参数和负责上一个部分的GPU的输出作为输入来进行计算，反向传播同理。

模型并行通常用于模型十分巨大，参数众多，即使在每个mini-batch只有一个样本的情况下单个GPU的显存仍然不够的情况，但并行性较差，可能有时会有GPU处于等待状态。

### 通道并行

通道并行是数据并行和模型并行同时进行

### 总结

- 当一个模型能用单卡计算时，通常使用数据并行扩展到多卡
- 模型并行则用在超大模型上

### Q&A（部分有价值的）

- 问1：若有4块GPU，两块显存大两块显存小怎么办？

- 答1：
  若GPU运算性能相同，则训练取决于小显存的GPU的显存大小，更大的显存相当于浪费掉
  若GPU运算性能不同，一般即为显存大的GPU性能更好，可以在分配数据时多分配一点

- 问2：数据拆分后，需存储的数据量会变大吗？会降低性能吗？

- 答2：每个GPU都单独存储了一份模型，这部分的数据量变大了，但如果只考虑运算时的中间变量，则中间变量的大小与数据量呈线性关系，每个GPU的数据小了，中间变量也会变小，所有GPU的中间变量加起来大小是不变的。
  数据拆分后性能会变低，在下节课讲解（数据通讯的开销，每个GPU的batch-size变小可能无法跑满GPU，总batch-size变大则相同计算量下训练次数变少）

# 34- 多GPU训练实现

本讲内容为代码实现，这里整理QA，其余内容参考代码部分。

Q1: keras从tf分离，书籍会不会需要重新整理？

> 暂时不会有影响

Q2: 是否可以通过把resnet中的卷积层全替换成mlp来实现一个很深的网络？

> 可以，有这样做的paper，但是通过一维卷积（等价于全连接层）做的，如果直接换成全连接层很可能会过拟合。

Q3: 为什么batch norm是一种正则但只加快训练不提升精度？

> 老师也不太清楚并认为这是很好的问题，可以去查阅论文。

Q4: all_reduce, all_gather主要起什么作用？实际使用时发现pytorch的类似分布式op不能传导梯度，会破坏计算图不能自动求导，如何解决？

> all_reduce是把n个东西加在一起再把所有东西复制回去，all_gather则只是把来自不同地方东西合并但不相加。使用分布式的东西会破坏自动求导，跨GPU的自动求导并不好做，老师不确定pytorch能不能做到这一功能，如果不能就只能手写。

Q5: 两个GPU训练时最后的梯度是把两个GPU上的梯度相加吗？

> 是的。mini-batch的梯度就是每个样本的梯度求和，多GPU时同理，每个GPU向将自己算的那部分样本梯度求和，最后再将两个GPU的计算得的梯度求和。

Q6: 为什么参数大的模型不一定慢？flop数多的模型性能更好是什么原理？

> 性能取决于每算一个乘法需要访问多少个bit，计算量与内存访问的比值越高越好。通常CPU/GPU不会被卡在频率上而是访问数据/内存上，所以参数量小，算力高的模型性能较好（如卷积，矩阵乘法）。

Q7: 为什么分布到多GPU上测试精度会比单GPU抖动大？

> 抖动是因为学习率变大了，使用GPU数对测试精度没有影响，只会影响性能。但为了得到更好的速度需要把batchsize调大，使得收敛情况发生变化，把学习率上调就使得精度更抖。

Q8: batchsize太大会导致loss nan吗？

> 不会，batchsize中的loss是求均值的，理论上batchsize更大数值稳定性会更好，出现数值不稳定问题可能是学习率没有调好。

Q9: GPU显存如何优化？

> 显存手动优化很难，靠的是框架，pytorch的优化做的还不错。除非特别懂框架相关技术不然建议把batchsize调小或是把模型做简单一点。

Q10: 对于精度来说batchsize=1是一种最好的情况吗？

> 可能是。

Q11: parameter server可以和pytorch结合吗，具体如何实现？

> pytorch没有实现parameter server，但mxnet和tensorflow有。但是有第三方实现如byteps支持pytorch。

Q12: 用了nn.DataParallel()，是不是数据集也被自动分配到了多个GPU上？

> 是的。在算net.forward()的时候会分开。

Q13: 验证集准确率震荡大那个参数影响最大？

> 学习率。

Q14: 为了让网络前几层能够训练能否采用不同stage采用不同学习率的方法？

> 可以，主要的问题是麻烦，不好确定各部分学习率相差多少。

Q15: 在用torch的数据并行中将inputs和labels放到GPU0是否会导致性能问题，因为这些数据最终回被挪一次到其他GPU上。

> 数据相比梯度来说很少，不会对性能有太大影响。但这个操作看上去的确很多余，老师认为不需要做，但不这样做会报错。

Q16: 为什么batchsize较小精度会不怎么变化？

> 学习率太大了，batchsize小学习率就不能太大。

Q17: 使用两块不同型号GPU影响深度学习性能吗？

> 需要算好两块GPU的性能差。如一块GPU的性能是另一块的2倍，那么在分配任务时也应该分得2倍的任务量。保证各GPU在同样时间内算完同一部分。

Q18: 课内竞赛直接用教材的VGG11但不收敛，同样的dataloader用resnet可以收敛，如何解决这一问题？

> 可能是学习率太大，也可考虑加入batch normalization。

# 35-分布式训练

### 本节目录

- [1.分布式计算](#1%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97)
- [2. GPU机器架构](#2-gpu%E6%9C%BA%E5%99%A8%E6%9E%B6%E6%9E%84)
  - [2.1 样例：计算一个小批量](#21-%E6%A0%B7%E4%BE%8B%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E5%B0%8F%E6%89%B9%E9%87%8F)
  - [2.2 总结](#22-%E6%80%BB%E7%BB%93)
- [3. 关于性能](#3--%E5%85%B3%E4%BA%8E%E6%80%A7%E8%83%BD)
  - [3.1 对于**同步SGD**：](#31-%E5%AF%B9%E4%BA%8E%E5%90%8C%E6%AD%A5sgd)
  - [3.2 **性能**：](#32-%E6%80%A7%E8%83%BD)
  - [3.3 性能的权衡](#33-%E6%80%A7%E8%83%BD%E7%9A%84%E6%9D%83%E8%A1%A1)
- [4. 实践时的建议](#4-%E5%AE%9E%E8%B7%B5%E6%97%B6%E7%9A%84%E5%BB%BA%E8%AE%AE)
- [5. 总结](#5-%E6%80%BB%E7%BB%93)

### 1.分布式计算

- 本质上来说和之前讲的单机多卡并行没有区别。二者之间的区别是分布式计算是通过网络把数据从一台机器搬到另一台机器

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/35/35-01.png)

### 2. GPU机器架构

- 总的来说，gpu到gpu的通讯是很快的，gpu到cpu慢一点。机器到机器更慢。因而总体性能的关键就是尽量在本地做通讯而少在机器之间做通讯

##### 2.1 样例：计算一个小批量

- 每个worker从参数服务器那里获取模型参数：首先把样本复制到机器的内存，然后把样本分到每个gpu上
- 复制参数到每个gpu上：同样，先把每一次的参数放到内存里，然后再复制到每个gpu上
- 每个gpu计算梯度
- 再主内存上把所有gpu上的梯度加起来
- 梯度从主内存传回服务器
- 每个服务器对梯度求和，并更新参数

##### 2.2 总结

- 由于gpu到gpu和gpu到内存的通讯速度还不错，因此我们尽量再本地做聚合（如梯度相加），并减少再网络上的多次通讯

### 3. 关于性能

##### 3.1 对于**同步SGD**：

- 这里每个worker都是同步计算一个批量，称为同步SGD
- 假设有n个ggpu，每个gpu每次处理b个样本，那么同步SGD等价于再单gpu运行批量大小为nb的SGD
- 再理想情况下，n个gpu可以得到相对单gpu的n倍加速

##### 3.2 **性能**：

- t1 = 在单gpu上计算b个样本梯度时间
- 假设有m个参数，一个worker每次发送和接受m个参数、梯度
  - t2 = 发送和接受所用时间
- 每个批量的计算时间为max（t1，t2）
  - 选取足够大的b使t1>t2
  - 增加b或n导致更大的批量 大小，当值需要更多计算来得到给定的模型精度

##### 3.3 性能的权衡

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/35/35-02.png)

### 4. 实践时的建议

- 使用一个大数据集
- 需要好的gpu-gpu和机器-机器带宽
- 高效的数据读取和预处理
- 模型需要有好的计算和通讯比
  - Inception>ResNet>AlexNet
- 使用足够大的批量大小来得到更好的系统性能
- 使用高效的优化算法对应大批量大小

### 5. 总结

- 分布式同步数据并行是多gpu数据并行在多机器上的拓展
- 网络通讯通常是瓶颈
- 需要注意使用特别大的批量大小时的收敛效率
- 更复杂的分布式有异步、模型并行（这里没有介绍）

# 36 -数据增强

### 目录

- [1. 使用增强数据训练](#1-%E4%BD%BF%E7%94%A8%E5%A2%9E%E5%BC%BA%E6%95%B0%E6%8D%AE%E8%AE%AD%E7%BB%83)
- [2. 增强手段](#2-%E5%A2%9E%E5%BC%BA%E6%89%8B%E6%AE%B5)
  - [2.1 翻转](#21-%E7%BF%BB%E8%BD%AC)
  - [2.2 切割](#22-%E5%88%87%E5%89%B2)
  - [2.3 颜色](#23-%E9%A2%9C%E8%89%B2)
  - [2.4 其他](#24-%E5%85%B6%E4%BB%96)
- [3. 总结](#3-%E6%80%BB%E7%BB%93)
- [4. QA](#4-qa)

数据增广不仅用于处理图片，也可用于文本和语音，这里只涉及到图片。

### 1. 使用增强数据训练

采集数据得到的训练场景与实际部署场景不同是常见的问题，这种变化有时会显著影响模型表现。在训练集中尽可能模拟部署时可能遇到的场景对模型的泛化性十分重要。

数据增强是指在一个已有数据集上操作使其有更多的多样性。对语音来说可以加入不同的背景噪音，对图片而言可以改变其颜色，形状等。

一般来说不会先将数据集做增广后存下来再用于训练；而是直接在线生成，从原始数据中读图片并随机做数据增强，再进入模型训练。通常只在训练时做数据增强而测试时不用。可以将数据增强理解为一个正则项。

### 2. 增强手段

#### 2.1 翻转

一些例子：左右翻转，上下翻转

要注意不是所有增强策略都总是可行，如建筑图片上下翻转就不太合适，而之前的树叶分类竞赛中的树叶图片就没关系。

![3601](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/36/36-01.png)

#### 2.2 切割

从图片中切割一块然后变形到固定形状。一般做法是随机取一个高宽比，随机取图片大小（切下部分占原图的百分数），随机取位置。

![3602](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/36/36-02.png)

#### 2.3 颜色

改变色调，饱和度，明亮度。

![3603](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/36/36-03.png)

#### 2.4 其他

还可以有很多种不同的方法，如高斯模糊，部分像素变黑，图片变形，锐化等等。理论上讲Photoshop能做到的都可以用作图片数据增强，但效果好坏另当别论。如果测试集中有类似的效果那么相应的数据增广手段会更有效。

![3604](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/36/36-04.png)

### 3. 总结

- 数据增广通过变形数据来获取多样性从而使得模型泛化性能更好
- 常见图片增广包括翻转，切割，变色

### 4. QA

Q1: 理论上是不是原始样本足够多就不需要做增广？

> 是的，但实际情况中很难有足够多样性的图片能覆盖测试的所有情况。
> 
> 数据量大也不一定意味着足够多样，可能简单情况已经很充分，但对于很难的情况覆盖率不够。

Q2: （代码实现中的）num_worker值是不是根据GPU性能而定？

> 是。
> 
> 这里老师还提到虽然深度学习主要用GPU，但CPU也不能太差，否则可能数据预处理跟不上，CPU的内存带宽和到显卡的带宽不够。具体决定num_worker可以自己定一个值然后跑一个epoch看看耗时。

Q3: 金融风控领域经常面临极度偏斜数据（欺诈样本极少），是否可对正样本做数据增广？

> 可以，类似地震预测等等正样本少的情况都可以尝试对正样本做增广，负样本可以不用。

Q4: 测试一般做什么样的增广？如何理解对测试集增广能提高精度？

> 一般不对测试集做增广。也可以对一张测试图像做增广，将每个增广出的图片都做一次预测最后取平均，会一定程度改善精度。但这样会使得对每张图片预测计算量成倍增长，所以使用较少。

Q5: 课件里提到的对比实验固定了所有随机种子吗？昨晚增广后训练精度下降是不是意味着还可以继续训练减少gap？

> 没有。
> 
> 是的，课堂演示时往往跑的epoch较少，另外训练到后期gap一般不会减少。

Q6: 图片增广后需要人工一张张确认效果吗？

> 不用全看，大概看看效果即可。

Q7: 图片增广后训练数据与测试数据分布可能不同，会对模型最终精度有影响吗？

> 首先多数图片增广手段不改变数据分布，因为亮度变化等是随机的，数据的均值不变，翻转不影响分布，crop可能会有改变但影响不大。
> 
> 后面还有问题提到对增广不改变数据分布的理解，可理解成增广不改变均值但稍微增大方差。很多时候讨论训练集和测试集分布是否相同不是看原始的像素分布而是看各label比例或图片色调等是否差不多。

Q8: 关于图神经网络

> 图神经网络很强大但不好训练，目前落地还太早了

Q9: 关于mosaic和crop

> 把多张图片拼起来训练。这里老师理解错了问题，提到了加马赛克和本节代码中一次展示八张图片只是一起显示而不是使用了crop方法。

Q10: 用对一个事物的视频描述做数据集是不是会比增广更有效？

> 可以这么认为，但拍视频是很贵的事情，获取视频往往划不来。

Q11: 多张图片叠加是否也是有效的增广方式？

> 是的，这种方法叫mix-up，非常有用。
> 
> 后面有问到为什么mix-up有用，老师也不清楚。
> 
> lable的叠加是对两张图片label按特定分布随机取权重加权求和

Q12: 做车辆位置识别如果实际应用场景摄像头高度角度清晰度都和训练集不一样，是不是只能针对场景单独采集数据重新打标训练？

> 是，可以考虑将实际部署时识别错误的数据加入训练集使得训练集测试集分布趋同

Q13: 是否会出现图像增广减小类间差异，混淆不同类别的情况？

> 那倒不会。可以考虑不要crop太小的区域。

Q14: 实际操作用torchvision还是albumentation?

> 都差不多

# 37- 微调

- [微调](#%E5%BE%AE%E8%B0%83)
  - [背景](#%E8%83%8C%E6%99%AF)
  - [步骤](#%E6%AD%A5%E9%AA%A4)
  - [总结](#%E6%80%BB%E7%BB%93)

### 背景

- 很多时候，例如我们想对家具进行分类，但是往往在努力收集数据得到的数据集也比较小假如我们想识别图片中不同类型的椅子，然后向用户推荐购买链接。 一种可能的方法是首先识别100把普通椅子，为每把椅子拍摄1000张不同角度的图像，然后在收集的图像数据集上训练一个分类模型。 尽管这个椅子数据集可能大于Fashion-MNIST数据集，但实例数量仍然不到ImageNet中的十分之一。 适合ImageNet的复杂模型可能会在这个椅子数据集上过拟合。 此外，由于训练样本数量有限，训练模型的准确性可能无法满足实际要求。为了避免这种情况，我们可以有两种方法：
  - 显然的想法就是收集更多的数据，但是，收集和标记数据可能需要大量的时间和金钱。 例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究资金。 尽管目前的数据收集成本已大幅降低，但这一成本仍不能忽视。
  - 我们可以考虑迁移学习将从*源数据集*学到的知识迁移到*目标数据集*。 例如，尽管ImageNet数据集中的大多数图像与椅子无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。 这些类似的特征也可能有效地识别椅子。

### 步骤

- 如图所示，微调包括以下四个步骤：
  
  1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即***源模型***。
  2. 创建一个新的神经网络模型，即***目标模型***。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。
  3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
  4. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。
5. 通常来讲，微调速度更快，并且具有较强的正则性，一般学习率比较小，也不需要很多轮的数据迭代，对于不同的任务往往只需要改变最后输出层，这一层随机初始化即可。

### 总结

- 迁移学习将从源数据集中学到的知识“迁移”到目标数据集，微调是迁移学习的常见技巧。
- 除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是，目标模型的输出层需要从头开始训练。
- 通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率。

# 38- 第二次竞赛树叶分类结果

### 任务简介

对176种叶子进行分类，每类叶子的数据大约有100张图片，数据较干净，没有复杂背景且颜色并不关键，模型只要能识别出形状即可

### 结果

165个队伍参加，1800次提交，私榜排名第一的同学正确率有0.99272，李沐老师私下使用AutoML训练大约0.98，由于结果较好，6-20名的队伍如果分享代码也可获得签名书

### 方法总结

待参加比赛的同学分享代码后下周课程上讲解，但B站视频并未上传此部分。

# 39-CIFAR-10

### 1.目录

- [CIFAR-10](#cifar-10)
  - [1.目录](#1%E7%9B%AE%E5%BD%95)
    - [2.1 下载数据集：](#21-%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86)
    - [2.2 整理数据集](#22-%E6%95%B4%E7%90%86%E6%95%B0%E6%8D%AE%E9%9B%86)
  - [3.图像增广](#3%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF)
  - [4.读取数据集](#4%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86)
  - [5.定义模型](#5%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B)
  - [6.定义训练函数](#6%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0)
  - [7.训练和验证模型](#7%E8%AE%AD%E7%BB%83%E5%92%8C%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B)
  - [8.Q&A](#8qa)

首先，导入竞赛所需要的包和模块：

```python
import collections
import math
import os
import shutil
import pandas as pd
import torch
import torchvision
from torch import nn
from d2l import torch as d2l
```

#### 2.1 下载数据集：

```python
#@save
d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
                                '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')

# 如果你使用完整的Kaggle竞赛的数据集，设置demo为False
demo = True

if demo:
    data_dir = d2l.download_extract('cifar10_tiny')
else:
    data_dir = '../data/cifar-10/'
```

为了便于入门，我们提供包含前1000个训练图像和5个随机测试图像的数据集的小规模样本，如果要获取完整数据集，你需要将一下demo变量设置为False

#### 2.2 整理数据集

首先我们用以下函数读取CSV文件中的标签，它返回一个字典，该字典将文件名中不带拓展名德部分映射到其标签。

```python
def read_csv_labels(fname):
    """读取fname来给标签字典返回一个文件名"""
    with open(fname, 'r') as f:
        # 跳过文件头行(列名)
        lines = f.readlines()[1:]
    tokens = [
        # 训练样本 : 1000l.rstrip().split(',') for l in lines]
    return dict(((name, label) for name, label in tokens))

labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
print('# 训练样本 :', len(labels))
print('# 类别 :', len(set(labels.values())))
# 训练样本 : 1000
# 类别 : 10
```

接下来，我们定义reorg_train_valid函数来将验证集从原始的训练集中拆分出来。此函数中的参数valid_ratio是验证集中的样本数与原始训练集中的样本数之比。更具体的说，令n等于样本最少的类别中的图像数量，而r是比率。验证集将为每个类别拆分出max([nr],1)张图像。让我们以valid_ratio=0.1为例，由于原始的训练集有50000张图像，因此trian_valid_test/train路径中将有45000张图像用于训练，而剩下5000张图像将作为路径train_valid_test/valid中的验证集。组织数据集后，同类别的图像将被放置在同一文件夹下。

```python
def copyfile(filename, target_dir):
    """将文件复制到目标目录"""
    os.makedirs(target_dir, exist_ok=True)
    shutil.copy(filename, target_dir)

#@save
def reorg_train_valid(data_dir, labels, valid_ratio):
    """将验证集从原始的训练集中拆分出来"""
    # 训练数据集中样本最少的类别中的样本数
    n = collections.Counter(labels.values()).most_common()[-1][1]
    # 验证集中每个类别的样本数
    n_valid_per_label = max(1, math.floor(n * valid_ratio))
    label_count = {}
    for train_file in os.listdir(os.path.join(data_dir, 'train')):
        label = labels[train_file.split('.')[0]]
        fname = os.path.join(data_dir, 'train', train_file)
        copyfile(fname, os.path.join(data_dir, 'train_valid_test',
                                     'train_valid', label))
        if label not in label_count or label_count[label] < n_valid_per_label:
            copyfile(fname, os.path.join(data_dir, 'train_valid_test',
                                         'valid', label))
            label_count[label] = label_count.get(label, 0) + 1
        else:
            copyfile(fname, os.path.join(data_dir, 'train_valid_test',
                                         'train', label))
    return n_valid_per_label
```

其中os.listdir显示指定路径下的文件和文件夹列表

下面的reorg_test函数用来预测期间整理测试集，以方便读取。

```python
#@save
def reorg_test(data_dir):
    """在预测期间整理测试集，以方便读取"""
    for test_file in os.listdir(os.path.join(data_dir, 'test')):
        copyfile(os.path.join(data_dir, 'test', test_file),
                 os.path.join(data_dir, 'train_valid_test', 'test',
                              'unknown'))
```

最后我们使用一个函数来调用前面定义的函数read_csv_labels，reorg_train_valid和reorg_test。

```python
def reorg_cifar10_data(data_dir, valid_ratio):
    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))
    reorg_train_valid(data_dir, labels, valid_ratio)
    reorg_test(data_dir)
```

在这里，我们只将样本数据集的批量大小设置为32.在实际训练和测试中，应该使用Kaggle竞赛的完整数据集，并将batch_size设置为更大的整数，例如128.我们将10%的训练样本作为调整超参数的验证集。

```python
batch_size = 32 if demo else 128
valid_ratio = 0.1
reorg_cifar10_data(data_dir, valid_ratio)
```

### 3.图像增广

使用图像增广来解决过拟合问题。在训练中，我们可以随机水平翻转图像。我们可以对彩色图像的三个RGB通道执行标准化。下面为一些可以调整的操作

```python
transform_train = torchvision.transforms.Compose([
    # 在高度和宽度上将图像放大到40像素的正方形
    torchvision.transforms.Resize(40),
    # 随机裁剪出一个高度和宽度均为40像素的正方形图像，
    # 生成一个面积为原始图像面积0.64到1倍的小正方形，
    # 然后将其缩放为高度和宽度均为32像素的正方形
    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),
                                                   ratio=(1.0, 1.0)),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    # 标准化图像的每个通道
    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                     [0.2023, 0.1994, 0.2010])])
```

在测试期间，我们只对图像执行标准化，以消除评估结果中的随机性

```python
transform_test = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],
                                     [0.2023, 0.1994, 0.2010])])
```

### 4.读取数据集

读取由原始图像组成的数据集，每个样本都包括一张图片和一个标签。

```python
train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(
    os.path.join(data_dir, 'train_valid_test', folder),
    transform=transform_train) for folder in ['train', 'train_valid']]

valid_ds, test_ds = [torchvision.datasets.ImageFolder(
    os.path.join(data_dir, 'train_valid_test', folder),
    transform=transform_test) for folder in ['valid', 'test']]
```

当验证集在超参数调整过程中用于模型评估中，不应引入图像增广的随机性。在最终预测之前，我们根据训练集合验证集组合而成的训练模型进行训练，以充分利用所有标记的数据

```python
train_iter, train_valid_iter = [torch.utils.data.DataLoader(
    dataset, batch_size, shuffle=True, drop_last=True)
    for dataset in (train_ds, train_valid_ds)]

valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,
                                         drop_last=True)

test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,
                                        drop_last=False)
```

### 5.定义模型

直接使用Resnet-18模型

```python
def get_net():
    num_classes = 10
    net = d2l.resnet18(num_classes, 3)
    return net

loss = nn.CrossEntropyLoss(reduction="none")
```

### 6.定义训练函数

```python
def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
          lr_decay):
    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,
                              weight_decay=wd)
    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)
    num_batches, timer = len(train_iter), d2l.Timer()
    legend = ['train loss', 'train acc']
    if valid_iter is not None:
        legend.append('valid acc')
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=legend)
    net = nn.DataParallel(net, device_ids=devices).to(devices[0])
    for epoch in range(num_epochs):
        net.train()
        metric = d2l.Accumulator(3)
        for i, (features, labels) in enumerate(train_iter):
            timer.start()
            l, acc = d2l.train_batch_ch13(net, features, labels,
                                          loss, trainer, devices)
            metric.add(l, acc, labels.shape[0])
            timer.stop()
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (metric[0] / metric[2], metric[1] / metric[2],
                              None))
        if valid_iter is not None:
            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)
            animator.add(epoch + 1, (None, None, valid_acc))
        scheduler.step()
    measures = (f'train loss {metric[0] / metric[2]:.3f}, '
                f'train acc {metric[1] / metric[2]:.3f}')
    if valid_iter is not None:
        measures += f', valid acc {valid_acc:.3f}'
    print(measures + f'\n{metric[2] * num_epochs / timer.sum():.1f}'
          f' examples/sec on {str(devices)}')
```

使用随机梯度下降和学习率规划来训练模型，以更快达到收敛。

### 7.训练和验证模型

以下所有超参数都可以调节

```python
devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4
lr_period, lr_decay, net = 4, 0.9, get_net()
train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,
      lr_decay)
```

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/39/39-01.png)

### 8.Q&A

##### Q1:深度学习的损失函数一般是非凸的吗？

> 损失函数一般是凸的，但是神经网络是非凸的(非单层)。凸函数表示能力有限。

##### Q2:训练时的训练集交叉熵loss大于验证集，但是训练集acc也是大于验证集的？

> 应该是因为在训练集上加了数据增广

##### Q3:normalize参数怎么来的？

> 由imagenet数据集上RGB的均值和方差

##### Q4：weight decay和lr decay的作用有什么区别吗？

> weight decay是对权重更新的操作——正则化（统计），lr decay 是作用在学习率上——为了收敛（优化模型）

##### Q5:scheduler怎么设置是最好的最优的，怎么选择？

> 现在一般选用cosine函数，参数设置较少 。最好在前期保证比较大的lr，后期lr可以变小一点。具体流行什么说不准

##### Q6:lr decay和weight decay的效果？

> 效果类似，但是本质不同。

# 41- 物体检测和数据集

- [41 物体检测和数据集](#41-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86)

- [物体检测](#%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B)

- [边缘框实现](#%E8%BE%B9%E7%BC%98%E6%A1%86%E5%AE%9E%E7%8E%B0)

- [数据集](#%E6%95%B0%E6%8D%AE%E9%9B%86)

#### 物体检测

1. 图片分类和目标检测在任务上的区别：图片分类已知有一个确定目标，任务是识别该目标属于何种分类，而目标检测不仅需要检测出图片中所有感兴趣的目标类别，并确定其位置，所以目标检测要比图片分类更复杂应用场景更广。
2. 图片分类和目标检测在数据集上的区别：由于目标检测中每一张图片可能存在多个目标，每个目标我们不仅需要分类，还需要确定边缘框以给出目标位置信息，因此目标检测数据集的标注成本要显著高于图片分类，也就导致了目标检测数据集较小。
3. 边缘框：用一个尽量小矩形框将目标物体大体框起来，边框的位置信息就可以表示目标位置在图片中的位置信息，常见的边缘框有两种表示方法：
- （左上x，左上y，右下x，右下y）
- （左上x，左上y，宽，高）
4. 目标检测数据集的常见表示：每一行表示一个物体，对于每一个物体而言，用“图片文件名，物体类别，边缘框”表示，由于边缘框用4个数值表示，因此对于每一行的那一个物体而言，需要用6个数值表示。
5. 目标检测领域常用数据集：COCO（80类物体，330K图片，所有图片共标注1.5M物体）

#### 边缘框实现

1. 目标的位置

在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。 然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。 在计算机视觉里，我们将这类任务称为*目标检测*（object detection）或*目标识别*（object recognition）。目标检测在多个领域中被广泛使用。 例如，在无人驾驶里，我们需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍物的位置来规划行进线路。 机器人也常通过该任务来检测感兴趣的目标。安防领域则需要检测异常目标，如歹徒或者炸弹。

```python
%matplotlib inline
import torch
from d2l import torch as d2l

# 下面加载本节将使用的示例图像。可以看到图像左边是一只狗，右边是一只猫。 它们是这张图像里的两个主要目标。

d2l.set_figsize()
img = d2l.plt.imread('../img/catdog.jpg')
d2l.plt.imshow(img);
```

2. 边界框
- 在目标检测中，我们通常使用*边界框*（bounding box）来描述对象的空间位置。 边界框是矩形的，由矩形左上角的以及右下角的*x*和*y*坐标决定。 另一种常用的边界框表示方法是边界框中心的(*x*,*y*)轴坐标以及框的宽度和高度。

- 在这里，我们定义在这两种表示法之间进行转换的函数：`box_corner_to_center`从两角表示法转换为中心宽度表示法，而`box_center_to_corner`反之亦然。 输入参数`boxes`可以是长度为4的张量，也可以是形状为（*n*，4）的二维张量，其中*n*是边界框的数量。

```python
#@save
def box_corner_to_center(boxes):
    """从（左上，右下）转换到（中间，宽度，高度）"""
    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    cx = (x1 + x2) / 2
    cy = (y1 + y2) / 2
    w = x2 - x1
    h = y2 - y1
    boxes = torch.stack((cx, cy, w, h), axis=-1)
    return boxes

#@save
def box_center_to_corner(boxes):
    """从（中间，宽度，高度）转换到（左上，右下）"""
    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    x1 = cx - 0.5 * w
    y1 = cy - 0.5 * h
    x2 = cx + 0.5 * w
    y2 = cy + 0.5 * h
    boxes = torch.stack((x1, y1, x2, y2), axis=-1)
    return boxes
```

- 我们将根据坐标信息定义图像中狗和猫的边界框。 图像中坐标的原点是图像的左上角，向右的方向为*x*轴的正方向，向下的方向为*y*轴的正方向。

```python
# bbox是边界框的英文缩写
dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]
```

- 我们可以将边界框在图中画出，以检查其是否准确。 画之前，我们定义一个辅助函数`bbox_to_rect`。 它将边界框表示成`matplotlib`的边界框格式。

```python
#@save
def bbox_to_rect(bbox, color):
    # 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：
    # ((左上x,左上y),宽,高)
    return d2l.plt.Rectangle(
        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],
        fill=False, edgecolor=color, linewidth=2)
```

- 在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内。

```python
fig = d2l.plt.imshow(img)
fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))
fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));
```

2. 小结
- 目标检测不仅可以识别图像中所有感兴趣的物体，还能识别它们的位置，该位置通常由矩形边界框表示。
- 我们可以在两种常用的边界框表示（中间，宽度，高度）和（左上，右下）坐标之间进行转换。

#### 数据集

目标检测领域没有像MNIST和Fashion-MNIST那样的小数据集。 为了快速测试目标检测模型，我们收集并标记了一个小型数据集。 首先，我们拍摄了一组香蕉的照片，并生成了1000张不同角度和大小的香蕉图像。 然后，我们在一些背景图片的随机位置上放一张香蕉的图像。 最后，我们在图片上为这些香蕉标记了边界框。

1. 下载数据集
- 包含所有图像和CSV标签文件的香蕉检测数据集可以直接从互联网下载。

```python
%matplotlib inline
import os
import pandas as pd
from mxnet import gluon, image, np, npx
from d2l import mxnet as d2l

npx.set_np()

#@save
d2l.DATA_HUB['banana-detection'] = (
    d2l.DATA_URL + 'banana-detection.zip',
    '5de26c8fce5ccdea9f91267273464dc968d20d72')
```

2. 读取数据集
- 通过`read_data_bananas`函数，我们读取香蕉检测数据集。 该数据集包括一个的CSV文件，内含目标类别标签和位于左上角和右下角的真实边界框坐标。

```python
#@save
def read_data_bananas(is_train=True):
    """读取香蕉检测数据集中的图像和标签"""
    data_dir = d2l.download_extract('banana-detection')
    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train
                             else 'bananas_val', 'label.csv')
    csv_data = pd.read_csv(csv_fname)
    csv_data = csv_data.set_index('img_name')
    images, targets = [], []
    for img_name, target in csv_data.iterrows():
        images.append(torchvision.io.read_image(
            os.path.join(data_dir, 'bananas_train' if is_train else
                         'bananas_val', 'images', f'{img_name}')))
        # 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），
        # 其中所有图像都具有相同的香蕉类（索引为0）
        targets.append(list(target))
    return images, torch.tensor(targets).unsqueeze(1) / 256
```

- 通过使用`read_data_bananas`函数读取图像和标签，以下`BananasDataset`类别将允许我们创建一个自定义`Dataset`实例来加载香蕉检测数据集。

```python
#@save
class BananasDataset(torch.utils.data.Dataset):
    """一个用于加载香蕉检测数据集的自定义数据集"""
    def __init__(self, is_train):
        self.features, self.labels = read_data_bananas(is_train)
        print('read ' + str(len(self.features)) + (f' training examples' if
              is_train else f' validation examples'))

    def __getitem__(self, idx):
        return (self.features[idx].float(), self.labels[idx])

    def __len__(self):
        return len(self.features)
```

- 最后，我们定义`load_data_bananas`函数，来为训练集和测试集返回两个数据加载器实例。对于测试集，无须按随机顺序读取它。

```python
#@save
def load_data_bananas(batch_size):
    """加载香蕉检测数据集"""
    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),
                                             batch_size, shuffle=True)
    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),
                                           batch_size)
    return train_iter, val_iter
```

- 让我们读取一个小批量，并打印其中的图像和标签的形状。 图像的小批量的形状为（批量大小、通道数、高度、宽度），看起来很眼熟：它与我们之前图像分类任务中的相同。 标签的小批量的形状为（批量大小，*m*，5），其中*m*是数据集的任何图像中边界框可能出现的最大数量。

```python
batch_size, edge_size = 32, 256
train_iter, _ = load_data_bananas(batch_size)
batch = next(iter(train_iter))
batch[0].shape, batch[1].shape
```

3. 小结
- 我们收集的香蕉检测数据集可用于演示目标检测模型。
- 用于目标检测的数据加载与图像分类的数据加载类似。但是，在目标检测中，标签还包含真实边界框的信息，它不出现在图像分类中。

# 43-树叶分类竞赛技术总结

### 目录

- [1. 比赛结果](#1-%E6%AF%94%E8%B5%9B%E7%BB%93%E6%9E%9C)

- [2. 结果分析](#2-%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90)

- [3. 技术分析](#3-%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90)

- [4. 模型方面](#4-%E6%A8%A1%E5%9E%8B%E6%96%B9%E9%9D%A2)

- [5. AutoGluon](#5-autogluon)

- [6. 总结](#6-%E6%80%BB%E7%BB%93)
  
  ### 1. 比赛结果

- 176类，18353训练样本

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/43/43-01.png)

- 165只队伍参加
  - 41只队伍精度 > 98% (非常好)
  - 83只队伍精度 > 95% (够用)

### 2. 结果分析

- 16只队伍提供了代码：
  
  - [Classify Leaves | Kaggle](https://www.kaggle.com/c/classify-leaves/code)

- 额外加上Neko Kiku
  
  - 很多人参考了此代码 [simple resnet baseline | Kaggle](https://www.kaggle.com/nekokiku/simple-resnet-baseline)

### 3. 技术分析

相比于课程介绍的代码，同学们主要做了下面这些加强：

- **数据增强**，在测试时多次使用稍弱的增强然后取平均

- 使用**多个模型**预测，最后结果加权平均
  
  - 有使用10种模型的，也有使用单一模型的

- **训练算法**和**学习率**

- **清理数据**

### 4. 模型方面

- 模型多为ResNet变种
  
  - DenseNet，ResNeXt，ResNeSt, ...
  - EfficientNet

- 优化算法多为Adam或其变种

- 学习率一般是Cosine或者训练不动时往下调

### 5. AutoGluon

- 15行代码， 安装加训练耗时100分钟
  - [AutoGluon.vision: 0.96+ with 15 lines | Kaggle](https://www.kaggle.com/zhreshold/autogluon-vision-0-96-with-15-lines)
- 精度96%
  - 可以通过定制化提升精度
  - 下一个版本将搜索更多的模型超参数
  - AG目前主要仍是关注工业界应用上，而非比赛

### 6. 总结

- 提升精度思路：根据数据挑选增强，使用新模型、新优化算法，多个模型融合，测试时使用增强
- 数据相对简单，排名有相对随机性
- 在工业界应用中：
  - 少使用模型融合和测试时增强，计算代价过高
  - 通常固定模型超参数，而将精力主要花在提升数据质量

比赛/学术界：固定数据，调模型

工业界：固定模型，调数据

# 44.物体检测算法：R-CNN,SSD,YOLO

### 目录

- [44.物体检测算法：R-CNN,SSD,YOLO](#44%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95r-cnnssdyolo)
  - [1.区域卷积神经网络](#1%E5%8C%BA%E5%9F%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)
    - [1.1.R-CNN](#11r-cnn)
    - [1.2 Fast RCNN](#12-fast-rcnn)
    - [1.3 Faster RCNN](#13-faster-rcnn)
    - [1.4 Mask RCNN](#14-mask-rcnn)
    - [1.5 总结](#15-%E6%80%BB%E7%BB%93)
  - [2. 单发多框检测（SSD single shot detection）](#2-%E5%8D%95%E5%8F%91%E5%A4%9A%E6%A1%86%E6%A3%80%E6%B5%8Bssd-single-shot-detection)
  - [3. YOLO（you only look once）](#3-yoloyou-only-look-once)

### 1.区域卷积神经网络

#### 1.1.R-CNN

- 使用启发式搜索算法来选择锚框
- 使用预训练模型来对每个锚框抽取特征（每个锚框当作一个图片，用CNN）
- 训练一个SVM来类别分类（神经网络之前，category prediction）
- 训练一个线性回归模型来预测边缘框偏移（bounding box prediction）
- 兴趣区域（Rol）池化层
  - 给定一个锚框，均匀分割（如果没法均匀分割，取整）成 n x m 块，输出每块的最大值（max pooling）
  - 不管锚框多大，总是输出nm个值
  - 目的：每个锚框都可以变成想要的形状

#### 1.2 Fast RCNN

- RCNN需要对每个锚框进行CNN运算，这些特征抽取计算有重复，并且锚框数量大，特征抽取的计算量也大。Fast RCNN改进了这种计算量大的问题

- 使用CNN对整张图片抽取特征（快的关键）

- 使用Rol池化层对每个锚框（将在原图片中搜索到的锚框，映射到CNN得到的结果上），生成固定长度的特征
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/44/44-01.png)

#### 1.3 Faster RCNN

- 在Fast RCNN基础上变得更快
- 使用一个 **区域提议网络来替代启发式搜索获得更好的锚框**
- 如下图所示，将CNN结果输入到卷积层，然后用锚框去圈区域，这些锚框很多有好有坏，然后进行预测，binary 预测是预测这个锚框的好坏，即有没有有效的圈住物体，bounding box prediction预测是对锚框进行一些改进，最后用NMS（非极大值抑制）对锚框进行合并。
- 具体来说，区域提议网络的计算步骤如下：
  1. 使用填充为1的3×3的卷积层变换卷积神经网络的输出，并将输出通道数记为c。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为c的新特征。
  2. 以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们。
  3. 使用锚框中心单元长度为c的特征，分别预测该锚框的二元类别（含目标还是背景）和边界框。
  4. 使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/44/44-02.png)

#### 1.4 Mask RCNN

- 如果有**像素级别的标号**，使用FCN（fully convolutional network）利用这些信息。可以提升CNN的性能
- **Rol align**。之前的Rol进行池化的时候，如果没法整除，可以直接取整。但是像素级别的标号预测的时候，会造成偏差的累积，导致边界预测不准确。未来避免这种情况，使用Rol align，也就是当没法整除，对每个像素值进行按比例分配。
- 具体来说，Mask R-CNN将兴趣区域汇聚层替换为了*兴趣区域对齐*层，使用*双线性插值*（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。 兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。 它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/44/44-03.png)

#### 1.5 总结

- R-CNN是最早也是最有名的一类基于锚框和CNN的目标检测算法
- Fast/Faster RCNN 持续提升性能
- Faster RCNN和Mask RCNN是在要求高精度场景下常用的算法（但是速度是最慢的）

### 2. 单发多框检测（SSD single shot detection）

- 生成锚框
  - 对每个像素，生成多个以它为中心的锚框
  - 给定 n 个大小s_1,...,s_n和m个高宽比，生成n+m-1个锚框，其大小和高宽比分别为：(s_1,r_1),(s_2,r_1)...,(s_n,r_1),(s_1,r_2),...,(s_1,r_m)
- SSD模型
  - 对多个分辨率下的卷积特征，生成锚框，预测
  - 一个基础网络，抽取特征，然后用多个卷积层来减半高宽
  - 在每段都生成锚框
    - 底部段拟合小物体
    - 顶部段拟合大物体
  - 对每个锚框预测类别和边缘框

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/44/44-04.png)

- 总结
  - 速度快，精度很低。这么多年，作者没有持续的提升，但是启发了后面的一系列工作，实现上相对比较简单。
  - SSD通过单神经网络来检测模型（single shot）
  - 以像素为中心的产生多个锚框
  - 在多个段的输出上进行多尺度的检测

### 3. YOLO（you only look once）

- SSD中锚框大量重复，因此浪费了很多计算资源
- YOLO将图片均分为 S X S 个锚框
- 每个锚框预测 B 个边缘框（防止多个物体出现在一个锚框里面）
- 后续版本 v2 v3 v4 有持续改进
- 非锚框算法

# 46- 语义分割

### 目录

- [1.语义分割](#1%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2)
- [2.应用](#2%E5%BA%94%E7%94%A8)
- [3.实例分割](#3%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2)
- [4.QA](#4qa)

### 1.语义分割

有时只能实现框选的目标检测还是太粗糙了，无法得到更精细的信息。语义分割将图片中的每个像素分类到对应的类别。

分割这一概念在计算机视觉中由来已久。最早的图片分割对给定图片使用聚类等方法把语义上比较像的像素放在一起，但通常不会告诉我们这些像素到底是什么。而语义分割可以告诉我们每个像素对应的label是什么。

这也意味着我们需要对图片的每一个像素都做label，使得语义分割成为了一个比较精细且大的任务。语义分割的数据集成本也较高，往往规模小像素高。常用的数据集之一是Pascal VOC2012。

![4601](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/46/46-01.jpg)

### 2.应用

背景虚化：传统的背景替换往往采用绿幕。在没有绿幕的情况下传统相机可以通过光圈来实现背景虚化，对于手机等设备而言背景虚化通常使用的都是语义分割或结合图像景深信息。

路面分割：如无人驾驶时用于实时识别周围物体，实现找路的功能。

### 3.实例分割

语义分割只关心像素属于哪一类，而实例分割则更进一步，如图片里有两只狗，则需要得出哪个像素属于哪一只狗。可以将其理解为目标检测的进化版本。

![4602](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/46/46-02.png)

### 4.QA

Q1: 能否做更细的语义分割如狗的头/身/腿？

> 可以，有标注数据即可，不过可能会出现不同标注者对身体部位分界不同之类的问题。针对狗头这一例子可以考虑使用姿态识别得到关节点。

Q2: 目标检测里做图像增广，目标框做对应变换后不再是矩形怎么办？

> 如果很关心角度信息可以给框加入一个表示旋转角度的feature，也可以考虑在旋转后的原框外面画一个大框把它圈起来，这个大框是可以计算出来的。

Q3: 把人像语义分割做到slides中的效果大概需要多少训练集？

> 人像这块技术相对比较成熟，应该能找到很好的预训练模型在其基础上调整即可。人像的形状是比较容易做的，难点主要在于光照不同（可能使背景与人像/衣服模糊）

Q4: 三维语义分割标注怎么做？

> （这里把三维理解成有景深的图片）一个简单的做法是把图片压缩成2维，也可以使用三维卷积。三维的分割实际上是好做的，因为一个物体的像素景深往往是连续的且与背景差距较大。

Q5: 自动驾驶用语义分割，实例分割还是目标检测更合适？

> 自动驾驶需要用到大量不同的模型，语义分割主要用于路面分割，目标检测用于检测前车/行人及其距离/速度。

Q6: 语义分割有什么标注工具？

> 国内外的数据标注公司会有这方面的平台，可以自己找找，老师认为这个比较简单，工具大同小异。
> 
> （弹幕提及较多：labelme）

Q7: 摄像头怕过曝，逆光相关

> 过曝不常见，但欠曝在光线不足时很常见，一种方法是做大量数据增广。逆光更难做一点，不过photoshop可以模拟出逆光效果用于数据增强，也可以在采集数据时采集一些逆光照片，检查低置信度的照片确认是否为逆光，之后加以标注对模型重新训练。这样的方法可能会涉及到数据隐私问题。

Q8: 自动驾驶用纯视觉方案能不能做到很可靠？

> tesla做的就是纯视觉，国内/Google用激光雷达（贵但精准），此外大家都会用摄像头和雷达。老师的广电是使用纯摄像头方案一方面是因为技术团队在这方面有积累，另一个可能的原因是摄像头便宜，第三个原因是tesla的算力很大能很好处理大量摄像头信息，最后是Tesla有大量的数据积累，大量的数据可以弥补传感器方面的劣势。
> 
> 理论上纯视觉自动驾驶是可行的，但目前只有Tesla做的还算可靠。

# 47-转置卷积

### 本节目录

- [1.转置卷积](#1%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF)
- [2.转置卷积是一种卷积](#2%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E6%98%AF%E4%B8%80%E7%A7%8D%E5%8D%B7%E7%A7%AF)

### 1.转置卷积

- 转置卷积和卷积的区别：
  - 卷积不会增大输入的高宽，通常要么不变、要么减半
  - 转置卷积则可以用来增大输入高宽
- 转置卷积的具体实现：

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/47/47-01.png)

如图所示，input里的每个元素和kernel相乘，最后把对应位置相加，相当于卷积的逆变换

- 为什么称之为“转置：
  - 对于卷积Y=X*W
    - 可以对W构造一个V，使得卷积等价于矩阵乘法Y'=VX'
    - 这里Y',X'是Y,X对应的向量版本
  - 转置卷积等价于Y'=VTX'
  - 如果卷积将输入从（h，w）变成了（h‘，w’）
    - 同样超参数的转置卷积则从（h‘，w’）变成为（h，w）

### 2.转置卷积是一种卷积

- 重新排列输入和核
  
  - 当填充为0步幅为1时：
    - 将输入填充k-1（k时核窗口）
    - 将核矩阵上下、左右翻转
    - 然后做正常卷积（填充0、步幅1）
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/47/47-02.png)
  
  - 当填充为p步幅为1时：
    - 将输如填充k-p-1（k是核窗口）
    - 将核矩阵上下、左右翻转
    - 然后做正常卷积（填充0、步幅1）
  
      ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/47/47-03.png)
  
  - 当填充为p步幅为s时：
    
    - 在行和列之间插入s-1行或列
    
    - 将输如填充k-p-1（k是核窗口）
    
    - 将核矩阵上下、左右翻转
    
    - 然后做正常卷积（填充0、步幅1）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/47/47-04.png)

- 形状换算
  
  - 输入高（宽）为n，核k，填充p，步幅s：
    
    - 转置卷积：n‘=sn+k-2p-s
    
    - 卷积：n’=[(n-k-2p+s)/s]向下取整
  
  - 如果让高宽成倍增加，那么k=2p+s

- 同反卷积的关系
  
  - 数学上的反卷积是指卷积的逆运算
    - 若Y=conv（X,K），那么X=deconv（Y,K）
  - 反卷积很少用在深度学习中
    - 我们说的反卷积神经网络指用了转置卷积的神经网络

- 总结
  
  - 转置卷积是一种变化了输入和核的卷积，来得到上采用的目的
  - 不等同于数学上的反卷积操作

# 48-全连接卷积神经网络（FCN）

- [48.全连接卷积神经网络（FCN）](#48%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cfcn)
- FCN是深度神经网络来做语义分割的奠定性工作，现在用的不多了
- 他用**转置卷积层**来替换CNN最后的全连接层（还有池化层），从而实现每个像素的预测。（如果输入的图片是224 x 224，经过CNN变成 7 x 7，经过 transposed conv，可以还原到 224 x 224 x k，k为通道数）

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/48/48-01.png)

- 代码
  
  - 见code

# 49- 样式迁移

样式迁移类似于手机相机中的滤镜，指的是给定内容图片和风格图片，合成一张新的图片，使得他的内容与内容图片相似，而风格却是风格图片的样子，如下例：

![样式迁移示例](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/49/%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB%E7%A4%BA%E4%BE%8B.png)

### 方法

如下图所示，可以用一个预训练好的神经网络来实现样式迁移：

- 1：复制内容图片来初始化一张图片，之后将这张图片中的像素作为参数进行更新，最终得到合成的图片
- 2：将内容图片，当前的合成图片，样式图片分别输入一个相同的预训练好的神经网络
- 3：假设该神经网络的不同层分别提取与内容和风格相关的信息，可以据此得到一个损失：
  - 将合成图片与**内容**相关的层的输出与**内容**图片对应层的输出进行处理，得到**内容损失**
  - 将合成图片与**风格**相关的层的输出与**风格**图片对应层的输出进行处理，得到**风格损失**
  - 对合成图片本身，统计图片中的高频噪点（即过明或过暗像素点），得到**全变分损失**
  - 将三部分损失加起来得到总的样式迁移的**损失函数**
- 4：利用3定义的损失函数，以合成图片的每个像素点为参数进行梯度下降，得到最终合成的图片

![CNN实现](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/49/CNN%E5%AE%9E%E7%8E%B0.png)

### 内容损失

神经网络内容相关层的输出的相似度可以直接反应两张图片在内容上的相似度，因此内容损失可以直接将对应层的输出视为内容直接求平方差损失。

### 风格损失：格拉姆矩阵

对于内容层，可以直接将对应层的输出求平方差损失，但是对于风格则略有不同

一般认为，风格层对应的输出的多个通道分别对应着不同类型的信息，如果将输出的形状从[batch\_size=1,channels,h,w]转化为[channels,h*w]就能得到通道数个向量，以x_1,x_2...x_c表示，代表不同通道所提取出的不同信息，而风格可以视作这些信息之间的关联，即相似度。

定义格拉姆矩阵\bold X * \bold X^T \in R^{c \cdot c}，矩阵的第i行第j列即向量x_i与x_j的内积，这个矩阵就代表了一张图片的风格。

对于生成图片和风格图片的格拉姆矩阵求平方差损失就能得到所需的风格损失

此外，为了让风格损失不受格拉姆矩阵及向量的大小影响，实际将格拉姆矩阵除以这些大小channnels*h*w。

### 全变分损失

用于去除高频噪点（过明过暗像素点）
$$
\sum_{i, j} \left|x_{i, j} - x_{i+1, j}\right| + \left|x_{i, j} - x_{i, j+1}\right|
$$

# 50-课程竞赛：牛仔行头检测

### 主要内容

图像中的目标检测，检测牛仔的装备，主要包括：夹克，墨镜，靴子，牛仔帽，腰带

有6937张训练图片，12660个标注框，数据集使用MS-COCO格式，可以调用pycocotools库，评测使用mAP（评测对每个类预测的框的好坏）

挑战：五个类别出现次数不同，墨镜、夹克次数多，牛仔帽、靴子其次，腰带很少

![数据统计](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/50/%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1.png)

### 安排

Kaggle不支持mAP，提交结果csv文件时网址不同

公私榜分配和之前不同，之前kaggle从所有数据中选出一部分作为私榜，本次限定了时间公榜结束后12小时内拿到私榜数据并提交结果，至多提交三次

提供了一个示例程序

# 51-序列模型

### 1.目录

- [序列模型](#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B)
  
  - [1.目录](#1%E7%9B%AE%E5%BD%95)
  
  - [2.序列数据](#2%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE)
    
    - [2.1 更多例子](#21-%E6%9B%B4%E5%A4%9A%E4%BE%8B%E5%AD%90)
  
  - [3.统计工具](#3%E7%BB%9F%E8%AE%A1%E5%B7%A5%E5%85%B7)
  
  - [4.序列模型](#4%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B)
    
    - [4.1 方案A:马尔科夫假设](#41-%E6%96%B9%E6%A1%88a%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%81%87%E8%AE%BE)
    - [4.2 方案B:潜变量模型](#42-%E6%96%B9%E6%A1%88b%E6%BD%9C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B)
  
  - [5.总结](#5%E6%80%BB%E7%BB%93)
  
  - [6.Q&A：](#6qa)
    
    ### 2.序列数据

- 实际中很多数据是有时序的

- 电影的评价随时间变化而变化
  
  - 拿了奖后评分上升，直到奖项被遗忘
  - 看了很多好电影后，人们的期望变高
  - 季节性：贺岁片，暑期档
  - 导演、演员的负面报道导致评分变低

#### 2.1 更多例子

- 音乐、文本、语言和视频都是连续的
  
  - 标题“狗咬人”远没有“人咬狗”那么令人惊讶

- 大地震发生后，很有可能会有几次较小的余震

- 人的互动是连续的，从网上吵架可以看出

- 预测明天的股价要比填补昨天遗失的股价更困难

### 3.统计工具

- 在时间t观察到![](https://latex.codecogs.com/svg.image?X_%7Bt%7D "X_{t}")，那么得到T个不独立的随机变量![](https://latex.codecogs.com/svg.image?(X_%7B1%7D,...X_%7Bt%7D)%5Csim&space;p(X) "(X_{1},...X_{t})\sim p(X)")

- 使用条件概率展开
  
  ![](https://latex.codecogs.com/svg.image?p(a,b)=p(a)p(b%7Ca)=p(b)p(a%7Cb) "p(a,b)=p(a)p(b|a)=p(b)p(a|b)")

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/51/51-01.png)

### 4.序列模型

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/51/51-02.png)

- 对条件概率建模

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/51/51-03.png)

#### 4.1 方案A:马尔科夫假设

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/51/51-04.png)

- 假设当前数据只跟τ个过去数据点相关

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/51/51-05.png)

#### 4.2 方案B:潜变量模型

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/51/51-06.png)

- 引入潜变量![](https://latex.codecogs.com/svg.image?h_%7Bt%7D "h_{t}")来表示过去信息![](https://latex.codecogs.com/svg.image?h_%7Bt%7D=f(x_%7B1%7D,...x_%7Bt-1%7D) "h_{t}=f(x_{1},...x_{t-1})")
  - 这样![](https://latex.codecogs.com/svg.image?x_%7Bt%7D=p(x_%7Bt%7D%7Ch_%7Bt%7D) "x_{t}=p(x_{t}|h_{t})")

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/51/51-07.png)

### 5.总结

- 时序模型中，当前数据跟之前观察到的数据相关
- 自回归模型使用自身过去数据来预测未来
- 马尔科夫模型假设当前只跟最近少数数据相关，从而简化模型
- 潜变量模型使用潜变量来概括历史信息

### 6.Q&A：

##### Q1:在常规范围内tau是不是越大越好。刚才例子tau=5是不是比4好？

> 当然比4好，也有局限性，tau特别大，训练样本变小，模型变复杂

##### Q2：潜变量模型和隐马尔科夫模型有什么区别？

> 没有太多联系，两个不同的观点，但是潜变量模型可以使用隐马尔科夫假设。潜变量-怎么建模，隐马尔科夫-这个数据和之前多少个数据有关。

##### Q3：若预测一个月，tau=30,预测7天，tau=7，是否有这样的关系？

> tau取决于对数据的理解，没有固定的规则

##### Q4：在预测未来方面，现在的sota模型能做到多好？

> 具体问题具体分析，在有些领域做得好比如写作，写代码，在一些领域做的不好，比如预测股票。

##### Q5:tau能够随着xt的变化而变化吗？这样感觉更符合实际情况

> 当然可以，有计算量的增加，也不一定更好

##### Q6:预测电池之类很多参数的未来变化趋势时怎么长步预测？

> 与数据关系比较大，负类样本较少，所以比较难训练

# 53 -语言模型

- [语言模型](#53-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)

- [学习语言模型](#%E5%AD%A6%E4%B9%A0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)

- [马尔可夫模型与n元语法](#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8En%E5%85%83%E8%AF%AD%E6%B3%95)

- [自然语言统计](#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%BB%9F%E8%AE%A1)

- [读取长序列数据](#%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE)

- [随机采样](#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7)

- [顺序分区](#%E9%A1%BA%E5%BA%8F%E5%88%86%E5%8C%BA)

- 语言模型的目标：
  
  假设长度为*T*的文本序列中的词元依次为*x*~~1~~,*x*~~2~~,…,*x~~T~~*。 于是，*x~~t~~*（1≤*t*≤*T*） 可以被认为是文本序列在时间步*t*处的观测或标签。 在给定这样的文本序列时目标是估计序列的联合概率*P*(*x*~~1~~,*x~~2~~*,…,*x~~T~~*)

#### 学习语言模型

- 基本想法：
  
  *P*(*x*~~1~~,*x~~2~~*,…,*x~~T~~*) = *P*(*x~~t~~*∣*x*~~1~~,…,*x~~t−1~~*). (1 <= t <= T) 共T个结果相乘
  
  例如，包含了四个单词的一个文本序列的概率是：
  
  *P*(deep,learning,is,fun)=*P*(deep)*P*(learning∣deep)*P*(is∣deep,learning)*P*(fun∣deep,learning,is)
  
  为了训练语言模型，我们需要计算单词的概率， 以及给定前面几个单词后出现某个单词的条件概率。 这些概率本质上就是语言模型的参数。训练数据集中词的概率可以根据给定词的相对词频来计算。 例如，可以将估计值*P*^(deep) 计算为任何以单词“deep”开头的句子的概率。 一种（稍稍不太精确的）方法是统计单词“deep”在数据集中的出现次数， 然后将其除以整个语料库中的单词总数。 这种方法效果不错，特别是对于频繁出现的单词。

- 基本想法的问题：
  
  由于连续单词对“deep learning”的出现频率要低得多， 所以估计这类单词正确的概率要困难得多。 特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。 而对于三个或者更多的单词组合，情况会变得更糟。 许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。 除非我们提供某种解决方案，来将这些单词组合指定为非零计数， 否则将无法在语言模型中使用它们。 如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。

#### 马尔可夫模型与n元语法

- 如果*P*(x~~t+1~~∣*x~~t~~*,…,*x*~~1~~)=*P*(x~~t+1~~∣*x~~t~~*)， 则序列上的分布满足一阶马尔可夫性质。 阶数越高，对应的依赖关系就越长。 这种性质推导出了许多可以应用于序列建模的近似公式：
  - P(x~~1~~,x~~2~~,x~~3~~,x~~4~~) = P(x~~1~~)P(x~~2~~)P(x~~3~~)P(x~~4~~)
  - P(x~~1~~,x~~2~~,x~~3~~,x~~4~~) = P(x~~1~~)P(x~~2~~ |x~~1~~)P(x~~3~~|x~~2~~)P(x~~4~~|x~~3~~)
  - P(x~~1~~,x~~2~~,x~~3~~,x~~4~~) = P(x~~1~~)P(x~~2~~ |x~~1~~)P(x~~3~~|x~~1~~,x~~2~~)P(x~~4~~|x~~2~~,x~~3~~)

#### 自然语言统计

- 在真实数据上如果进行自然语言统计：

根据前几节介绍的时光机器数据集构建词表，并打印前10个最常用的单词

```python
import random
import torch
from d2l import torch as d2l

tokens = d2l.tokenize(d2l.read_time_machine())
# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起
corpus = [token for line in tokens for token in line]
vocab = d2l.Vocab(corpus)
vocab.token_freqs[:10]
```

```python
[('the', 2261),
 ('i', 1267),
 ('and', 1245),
 ('of', 1155),
 ('a', 816),
 ('to', 695),
 ('was', 552),
 ('in', 541),
 ('that', 443),
 ('my', 440)]
```

- 正如我们所看到的，最流行的词看起来很无聊， 这些词通常被称为*停用词*（stop words），因此可以被过滤掉。 尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。 此外，还有个明显的问题是词频衰减的速度相当地快。 例如，最常用单词的词频对比，第10个还不到第1个的1/5。 为了更好地理解，我们可以画出的词频图：

```python
freqs = [freq for token, freq in vocab.token_freqs]
d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',
         xscale='log', yscale='log')
```

我们可以发现：词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。

- 我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。

```python
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
bigram_vocab = d2l.Vocab(bigram_tokens)
bigram_vocab.token_freqs[:10]
```

```python
[(('of', 'the'), 309),
 (('in', 'the'), 169),
 (('i', 'had'), 130),
 (('i', 'was'), 112),
 (('and', 'the'), 109),
 (('the', 'time'), 102),
 (('it', 'was'), 99),
 (('to', 'the'), 85),
 (('as', 'i'), 78),
 (('of', 'a'), 73)]
```

这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的， 只有一个与“the time”有关。 我们再进一步看看三元语法的频率是否表现出相同的行为方式。

```python
trigram_tokens = [triple for triple in zip(
    corpus[:-2], corpus[1:-1], corpus[2:])]
trigram_vocab = d2l.Vocab(trigram_tokens)
trigram_vocab.token_freqs[:10]
```

```python
[(('the', 'time', 'traveller'), 59),
 (('the', 'time', 'machine'), 30),
 (('the', 'medical', 'man'), 24),
 (('it', 'seemed', 'to'), 16),
 (('it', 'was', 'a'), 15),
 (('here', 'and', 'there'), 15),
 (('seemed', 'to', 'me'), 14),
 (('i', 'did', 'not'), 14),
 (('i', 'saw', 'the'), 13),
 (('i', 'began', 'to'), 13)]
```

- 最后，我们直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法。

```python
bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]
trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]
d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',
         ylabel='frequency: n(x)', xscale='log', yscale='log',
         legend=['unigram', 'bigram', 'trigram'])
```

#### 读取长序列数据

- 由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。在前几节中我们以一种相当特别的方式做到了这一点： 当序列变得太长而不能被模型一次性全部处理时， 我们可能希望拆分这样的序列方便模型读取。

- 在介绍该模型之前，我们看一下总体策略。 假设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如*n*个时间步）的一个小批量序列。 现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。首先，由于文本序列可以是任意长的， 例如整本《时光机器》（*The Time Machine*）， 于是任意长的序列可以被我们划分为具有相同时间步数的子序列。 当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。 假设网络一次只处理具有*n*个时间步的子序列。下画出了从原始文本序列获得子序列的所有不同的方式， 其中*n*=5，并且每个时间步的词元对应于一个字符。 请注意，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。
  
  ![/images/timemachine5gramsvg](https://zh-v2.d2l.ai/_images/timemachine-5gram.svg)

因此，我们应该从中选择哪一个呢？ 事实上，他们都一样的好。 然而，如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 因此，我们可以从随机偏移量开始划分序列， 以同时获得*覆盖性*（coverage）和*随机性*（randomness）。 下面，我们将描述如何实现*随机采样*（random sampling）和 *顺序分区*（sequential partitioning）策略。

#### 随机采样

- 在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。下面的代码每次可以从数据中随机生成一个小批量。 在这里，参数`batch_size`指定了每个小批量中子序列样本的数目， 参数`num_steps`是每个子序列中预定义的时间步数。

```python
def seq_data_iter_random(corpus, batch_size, num_steps):  #@save
    """使用随机抽样生成一个小批量子序列"""
    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1
    corpus = corpus[random.randint(0, num_steps - 1):]
    # 减去1，是因为我们需要考虑标签
    num_subseqs = (len(corpus) - 1) // num_steps
    # 长度为num_steps的子序列的起始索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # 在随机抽样的迭代过程中，
    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻
    random.shuffle(initial_indices)

    def data(pos):
        # 返回从pos位置开始的长度为num_steps的序列
        return corpus[pos: pos + num_steps]

    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # 在这里，initial_indices包含子序列的随机起始索引
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
```

- 下面我们生成一个从0到34的序列。 假设批量大小为2，时间步数为5，这意味着可以生成 ⌊(35−1)/5⌋=6个“特征－标签”子序列对。 如果设置小批量大小为2，我们只能得到3个小批量。

```python
my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

#### 顺序分区

- 在迭代过程中，除了对原始序列可以随机抽样外， 我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。

```python
def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save
    """使用顺序分区生成一个小批量子序列"""
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
```

- 基于相同的设置，通过顺序分区读取每个小批量的子序列的特征`X`和标签`Y`。 通过将它们打印出来可以发现： 迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。

```python
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
```

- 现在，我们将上面的两个采样函数包装到一个类中， 以便稍后可以将其用作数据迭代器。

```python
class SeqDataLoader:  #@save
    """加载序列数据的迭代器"""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
```

- 最后，我们定义了一个函数`load_data_time_machine`， 它同时返回数据迭代器和词表， 因此可以与其他带有`load_data`前缀的函数类似地使用。

```python
def load_data_time_machine(batch_size, num_steps,  #@save
                           use_random_iter=False, max_tokens=10000):
    """返回时光机器数据集的迭代器和词表"""
    data_iter = SeqDataLoader(
        batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```

# 54-循环神经网络

- [循环神经网络RNN](#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn)
  - [使用潜变量](#%E4%BD%BF%E7%94%A8%E6%BD%9C%E5%8F%98%E9%87%8F)
  - [RNN](#rnn)
  - [困惑度](#%E5%9B%B0%E6%83%91%E5%BA%A6)
  - [梯度裁剪](#%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA)
  - [更多的应用RNNs](#%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%BA%94%E7%94%A8rnns)

### 使用潜变量

- RNN使用了隐藏层来记录过去发生的所有事件的信息，从而引入时许的特性，并且避免常规序列模型每次都要重新计算前面所有已发生的事件而带来的巨大计算量。![截屏20220212 下午21732](https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/54/54-01.png)

### RNN循环神经网络

- 流程如下，首先有一个输入序列，对于时刻t，我们用t-1时刻的输入x~~t-1~~和潜变量h~~t-1~~来计算新的潜变量h~~t~~。同时，对于t时刻的输出o~~t~~，则直接使用h~~t~~来计算得到。注意，计算第一个潜变量只需要输入即可（因为前面并不存在以往的潜变量）。
  
  ![截屏20220212 下午23414](https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/54/54-02.png)

- 值得注意的是，RNN本质也是一种MLP，尤其是将h~~t-1~~这一项去掉时就完全退化成了MLP。RNN的核心其实也就是h~~t-1~~这一项，它使得模型可以和前面的信息联系起来，将时序信息储存起来，可以把RNN理解为是包含时序信息的MLP。

### 困惑度

- 为了衡量一个语言模型的好坏，例如分类模型，可以使用平均交叉熵来衡量，就是将预测概率的负对数值求和之后再去平均，即常用的交叉熵损失。但是由于某些历史原因，NLP往往不是用这种方式，而是在这种方式的基础上最后再取指数，即exp，这样得到的结果如果是1，说明完美；如果是无穷大，说明结果很差。

### 梯度裁剪

- 在T个时间步中进行反向传播，会由于产生O(T)长度的梯度乘法链，导致导数数值不稳定，这里使用一个限制θ，通常为5到10，来控制梯度乘法链的长度。使用如下的公式
  
  ![截屏20220212 下午31038](https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/54/54-03.png)
  
  在这个公式中，如果梯度长度大于θ，梯度g会变为
  
  $$
  \frac{\theta}{\Vert g \Vert} g
  $$
  
  这样再对g求2范式就变成了θ，所以可以把梯度限制在θ以下。

### 更多的应用RNNs

- 基本的应用如下图

![截屏2022-02-12 下午3.36.29](https://github.com/kinza99/DeepLearning-MuLi-Notes/raw/main/imgs/54/54-04.png)

# 56-门控循环单元(GRU)

### 1. 动机：如何关注一个序列

- 不是每个观察值都是同等重要

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/56/56-01.png)

比如上图中的序列，若干个猫中出现了一个鼠，那么我们应该重点关注这个鼠，而中间重复出现的猫则减少关注。文本序列同理，通常长文本我们需要关注的是几个关键词，关键句。

- 想只记住相关的观察需要：
  - 能**关注**的机制（**更新门**）：顾名思义，是否需要根据我的输入，更新隐藏状态
  - 能**遗忘**的机制（**重置门**）：更新候选项时，是否要考虑前一隐藏状态。

### 2. 门的概念

- 更新门Zt，重置门Rt的公式大体相同，唯一不同的是学习到的参数。

- 需要注意的是，计算门的方式和原来RNN的实现中计算新的隐状态相似，只是激活函数改成了sigmoid。

- 门本来是电路中的一个概念，0,1代表不同的电平，可以用于控制电路的通断。此处sigmoid将门的数值归一化到0到1之间，是一种"软更新"方式。而从后面的公式上可以看出，本讲课程采用的是低电平有效（越靠近0，门的作用越明显）的方式控制。

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/56/56-02.png)

### 3. 候选隐状态

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/56/56-03.png)

- 候选隐状态，如果抛开公式中的R_{t}遗忘门来说，这个和之前RNN中计算当前步的隐状态没有差别。

- 但是这里引入了遗忘门，如果R_{t}无限接近于0，那么此时候选隐状态将不再考虑前一隐状态的影响，也就是和MLP没有区别，起到“遗忘”的作用；

- 反之，如果R_{t}无限接近于1，那么与RNN计算隐状态的过程没有差别，不进行遗忘。

- 公式中的⊙表示逐元素乘积。

> 为什么叫候选隐状态？
> 
> 在RNN中，这个所谓的候选隐状态就是当前步的隐状态（R_{t}无限接近1时）。但是由于引入了更新门，我们需要考虑是直接沿用上一步的隐藏状态，还是像RNN一样使用当前步计算的隐状态。所以这个结合了当前输入计算的隐状态，不能立马变成当前的H_{t}，而是需要用更新门和前一隐状态H_{t-1}做一个加权，所以它是一个候选项。

### 4. 隐状态

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/56/56-04.png)

用更新门对**候选隐状态**和**前一隐状态**做加权，得到当前步**隐状态**的值。

如果Z_{t}无限接近于0，更新起作用，候选隐状态“转正”，变为当前隐状态。

如果Z_{t}无限接近于1，更新不起作用，当前隐状态还是沿用前一隐状态。

### 5. 总结

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/56/56-05.png)

上图四行公式概括了GRU模型。在RNN的基础上，最重要的是引入了**更新门和重置门**，来决定前一隐状态对当前隐状态的影响。以最开始的猫鼠序列的例子来说，如果我的模型一直看到猫，模型可以学习到隐状态不怎么去更新，于是隐状态一直保留了猫的信息，而看到老鼠，隐状态才进行更新。

- 对于一个更具体的例子而言(语言模型)：

“The cat, which already ate ……, __(is/ was) full.”，假设我的句子很长，预测完前面的词后需要预测下一个词is还是was，如果引入这种更新/重置的机制，那我们的模型可以在was这个词之前尽可能去保持隐状态的信息，从而即使阅读了一个很长的定语从句，但我们还是保留了cat这个词的单数信息，从而模型预测下一个词为'was'。

- 一个与RNN的联动在于：

如果更新门完全发挥作用（无限接近于0），重置门不起作用（无限接近于1），此时GRU模型退化为RNN模型。

### 6. QA

问题：GRU为什么需要两个门？

> 重置门和更新门各司其职。重置门单方面控制自某个节点开始，之前的记忆（隐状态）不在乎了，直接清空影响，同时也需要更新门帮助它实现记忆的更新。更新门更多是用于处理梯度消失问题，可以选择一定程度地保留记忆，防止梯度消失。
> 
> 重置门影响的是当前步新的候选隐状态的计算，更新门影响的是当前步隐状态的更新程度。

# 57-长短期记忆网络

### 1.目录

### 2.长短期记忆网络：

- 忘记门：将值朝0减少
- 输入门：决定是不是忽略掉输入数据
- 输出门：决定是不是使用隐状态

可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。 长短期记忆网络引入了*记忆元*（memory cell），或简称为*单元*（cell）。 有些文献认为记忆元是隐状态的一种特殊类型， 它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。 为了控制记忆元，我们需要许多门。 其中一个门用来从单元中输出条目，我们将其称为*输出门*（output gate）。 另外一个门用来决定何时将数据读入单元，我们将其称为*输入门*（input gate）。 我们还需要一种机制来重置单元的内容，由*遗忘门*（forget gate）来管理， 这种设计的动机与门控循环单元相同， 能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。 让我们看看这在实践中是如何运作的。

#### 2.1 门：

输入门：![](https://latex.codecogs.com/svg.image?I_%7Bt%7D=%5Csigma&space;(X_%7Bt%7DW_%7Bxi%7D+H_%7Bt-1%7DW_%7Bhi%7D+b_%7Bi%7D) "I_{t}=\sigma (X_{t}W_{xi}+H_{t-1}W_{hi}+b_{i})")

忘记门：![](https://latex.codecogs.com/svg.image?F_%7Bt%7D=%5Csigma&space;(X_%7Bt%7DW_%7Bxf%7D+H_%7Bt-1%7DW_%7Bhf%7D+b_%7Bf%7D) "F_{t}=\sigma (X_{t}W_{xf}+H_{t-1}W_{hf}+b_{f})")

输出门：![](https://latex.codecogs.com/svg.image?O_%7Bt%7D=%5Csigma&space;(X_%7Bt%7DW_%7Bxo%7D+H_%7Bt-1%7DW_%7Bho%7D+b_%7Bo%7D) "O_{t}=\sigma (X_{t}W_{xo}+H_{t-1}W_{ho}+b_{o})")

这三个门的算式和普通RNN计算Ht算式相同。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/57/57-1.png)

#### 2.2候选记忆单元

![](https://latex.codecogs.com/svg.image?%5Cwidetilde%7BC_%7Bt%7D%7D=tanh(X_%7Bt%7DW_%7Bxc%7D+H_%7Bt-1%7DW_%7Bhc%7D+b_%7Bc%7D) "\widetilde{C_{t}}=tanh(X_{t}W_{xc}+H_{t-1}W_{hc}+b_{c})")

相当于在ht-1到ht的预测中又加了一层隐藏单元

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/57/57-2.png)

#### 2.2记忆单元

![](https://latex.codecogs.com/svg.image?C_%7Bt%7D=F_%7Bt%7D%5Codot&space;C_%7Bt-1%7D+I_%7Bt%7D%5Codot&space;%5Cwidetilde%7BC_%7Bt%7D%7D "C_{t}=F_{t}\odot C_{t-1}+I_{t}\odot \widetilde{C_{t}}")

如果遗忘门始终为(1)且输入门始终为(0)， 则过去的记忆元 将随时间被保存并传递到当前时间步。 引入这种设计是为了缓解梯度消失问题， 并更好地捕获序列中的长距离依赖关系。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/57/57-3.png)

#### 2.3隐状态

![](https://latex.codecogs.com/svg.image?H_%7Bt%7D=O_%7Bt%7D%5Codot&space;tanh(C_%7Bt%7D) "H_{t}=O_{t}\odot tanh(C_{t})")

最后，我们需要定义如何计算隐状态， 这就是输出门发挥作用的地方。 在长短期记忆网络中，它仅仅是记忆元的的门控版本。 这就确保了Ht的值始终在区间((-1, 1))内.

只要输出门接近1，我们就能够有效地将所有记忆信息传递给预测部分， 而对于输出门接近(0)，我们只保留记忆元内的所有信息，而不需要更新隐状态。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/57/57-4.png)

#### 2.4总结

LSTM的计算流程：

![](https://latex.codecogs.com/svg.image?I_%7Bt%7D=%5Csigma&space;(X_%7Bt%7DW_%7Bxi%7D+H_%7Bt-1%7DW_%7Bhi%7D+b_%7Bi%7D) "I_{t}=\sigma (X_{t}W_{xi}+H_{t-1}W_{hi}+b_{i})")

![](https://latex.codecogs.com/svg.image?F_%7Bt%7D=%5Csigma&space;(X_%7Bt%7DW_%7Bxf%7D+H_%7Bt-1%7DW_%7Bhf%7D+b_%7Bf%7D) "F_{t}=\sigma (X_{t}W_{xf}+H_{t-1}W_{hf}+b_{f})")

![](https://latex.codecogs.com/svg.image?O_%7Bt%7D=%5Csigma&space;(X_%7Bt%7DW_%7Bxo%7D+H_%7Bt-1%7DW_%7Bho%7D+b_%7Bo%7D) "O_{t}=\sigma (X_{t}W_{xo}+H_{t-1}W_{ho}+b_{o})")

![](https://latex.codecogs.com/svg.image?%5Cwidetilde%7BC_%7Bt%7D%7D=tanh(X_%7Bt%7DW_%7Bxc%7D+H_%7Bt-1%7DW_%7Bhc%7D+b_%7Bc%7D) "\widetilde{C_{t}}=tanh(X_{t}W_{xc}+H_{t-1}W_{hc}+b_{c})")

![](https://latex.codecogs.com/svg.image?C_%7Bt%7D=F_%7Bt%7D%5Codot&space;C_%7Bt-1%7D+I_%7Bt%7D%5Codot&space;%5Cwidetilde%7BC_%7Bt%7D%7D "C_{t}=F_{t}\odot C_{t-1}+I_{t}\odot \widetilde{C_{t}}")

![](https://latex.codecogs.com/svg.image?H_%7Bt%7D=O_%7Bt%7D%5Codot&space;tanh(C_%7Bt%7D) "H_{t}=O_{t}\odot tanh(C_{t})")

### 3.从零实现

加载时光机器数据集

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```

#### 3.1初始化模型参数

```python
def get_lstm_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xi, W_hi, b_i = three()  # 输入门参数
    W_xf, W_hf, b_f = three()  # 遗忘门参数
    W_xo, W_ho, b_o = three()  # 输出门参数
    W_xc, W_hc, b_c = three()  # 候选记忆元参数
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,
              b_c, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```

#### 3.2定义模型

在初始化函数中， 长短期记忆网络的隐状态需要返回一个*额外*的记忆元， 单元的值为0，形状为（批量大小，隐藏单元数）。 因此，我们得到以下的状态初始化。

```python
def init_lstm_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device),
            torch.zeros((batch_size, num_hiddens), device=device))
```

实际模型的定义与我们前面讨论的一样： 提供三个门和一个额外的记忆元。 请注意，只有隐状态才会传递到输出层， 而记忆元(\mathbf{C}_t)不直接参与输出计算。

```python
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
     W_hq, b_q] = params
    (H, C) = state
    outputs = []
    for X in inputs:
        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)
        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)
        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)
        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)
        C = F * C + I * C_tilda
        H = O * torch.tanh(C)
        Y = (H @ W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H, C)
```

#### 3.3训练和预测

```python
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,
                            init_lstm_state, lstm)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/57/57-5.png)

### Q&A

##### Q1：请问LSTM如果不要C,把公式里的换成，好像可以实现隐藏状态往下传递？

> ![](https://latex.codecogs.com/svg.image?&space;C_%7Bt-1%7D "C_{t-1}")的可以约束![](https://latex.codecogs.com/svg.image?&space;H_%7Bt-1%7D "H_{t-1}")的大小在0-1之间，避免梯度爆炸，而且使算式更加自然，c换成h复杂度降低。

##### Q2：I,F,O,C_tilda的初始化为零？

> 这些是计算的中间变量，不需要初始化

##### Q3：如何计算模型占用显存，batch占用的显存？

> 取决于框架和库，没法具体算

# 58 -深层循环神经网络

### 目录

- [1.深层循环神经网络](#1%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)
- [2.公式](#2%E5%85%AC%E5%BC%8F)
- [3.总结](#3%E6%80%BB%E7%BB%93)
- [4.QA](#4qa)

### 1.深层循环神经网络

之前讲的RNN都只有一个隐藏层（序列变长不算是深度），而一个隐藏层的RNN一旦做的很宽就容易出现过拟合。因此我们考虑将网络做的更深而非更宽，每层都只做一点非线性，靠层数叠加得到更加非线性的模型。

浅RNN：输入-隐层-输出

深RNN：输入-隐层-隐层-...-输出

![5801](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/58/58-01.png)

（课程视频中的图片有错误，最后输出层后一时间步是不受前一步影响的，即没有箭头）

### 2.公式

![](http://latex.codecogs.com/svg.latex?%5Cmathbf%7BH%7D_t%5E1=f_1(%5Cmathbf%7BH_%7Bt-1%7D%5E1%7D,%5Cmathbf%7BX_t%7D))

*第一层的第t步状态是关于第一层第t-1步状态和第t步输入的函数*

![](http://latex.codecogs.com/svg.latex?%5Cmathbf%7BH%7D_t%5Ej=f_j(%5Cmathbf%7BH_%7Bt-1%7D%5Ej%7D,%5Cmathbf%7BH_%7Bt%7D%5E%7Bj-1%7D)%7D)

*第j层的第t步状态是关于当前层上一步步状态和上一层当前步的函数*

![](http://latex.codecogs.com/svg.latex?%5Cmathbf%7BO%7D_t=g(%5Cmathbf%7BH%7D_t%5EL))

*由最后一个隐藏层得到输出*

### 3.总结

- 深度循环神经网络使用多个隐藏层来获得更多的非线性性

将RNN/GRU/LSTM做深都是一个道理，三者只是使用的函数f不同。

### 4.QA

Q1: NLP那个方向好找工作？文本翻译是不是现在只在学术研究中才需要自己实现？（2021-7-27）

> 文本翻译已经是一个很成熟的领域，NLP挺好找工作，人产生的文本远多于图片。

Q2: 关于BPTT

> 课上不讲，书上有讲原理

Q3: 深层RNN是不是每层都需要一个初始hidenstate?

> 是的

Q4: 可不可以手动实现hidden_size不一样的多层RNN？

> 应该没问题，但通常大家不会去调hidden_size，因为网络不会做的很深，最后还有全连接层。

Q5: 关于课上提到的classifier

> 分类的任务在最后的全连接层完成

# 61-编码器-解码器架构

### CNN中的解释

考虑一个CNN模型：

![CNN](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/61/CNN.png)

整个CNN实际上可以看作一个编码器，解码器两部分。

- 底层的神经网络，也就是编码器将输入编码成能被模型识别的中间表达形式，也就是特征
- 解码器将中间结果解码为输出

### RNN中的解释

对于RNN而言，同样有着类似的划分

![RNN](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/61/RNN.png)

- 编码器将输入文本表示为向量
- 解码器将向量表示为输出

### 抽象的编码器-解码器架构

指一个模型被分为两块：

- 一块是编码器，也叫encoder，用于将输入处理为一个中间状态
- 一块是解码器，也叫decoder，用于将中间状态表示为输出
- 解码器也可以有额外的输入提供信息

![encoderdecoder](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/61/encoder-decoder.png)

# 62-序列到序列学习

### 1. 应用举例：机器翻译

- 给定一个源语言的句子，自动翻译成目标语言
- 这两个句子可以有不同的长度

### 2. 模型架构：Seq2seq

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/62/62-01.png)

- 序列到序列模型由**编码器-解码器**构成。

- **编码器**RNN可以是**双向**，由于输入的句子是完整地，可以正着看，也可以反着看；而**解码器**只能是**单向**，由于预测时，只能正着去预测。

- 编码器，解码器采用**不同的RNN**，此RNN也可以是GRU，LSTM等。

### 3. 编码器-解码器细节

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/62/62-02.png)

- 编码器的RNN**没有**连接**输出层**

- **编码器**的**最后时间步的隐状态**用作**解码器**的**初始隐状态**（图中箭头的传递）

### 4. 训练和推理

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/62/62-03.png)

- 第3节中提到编码器没有输出层，只有解码器有，于是损失函数的计算只关注解码器的输出层。
- 训练和预测（推理）有区别的，训练时解码器使用目标句子（真值）作为输入，以指导模型训练；而推理时无法提前得知真值，需要一步一步进行预测。

### 5. 衡量生成序列的好坏：BLEU

#### 5.1 BLUE值定义：

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/62/62-04.png)

宗成庆老师《统计自然语言处理》（第二版）一书中关于BLEU的定义：

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/62/62-05.png)

同时，吴恩达深度学习课程中也是使用这一方式定义。但观察两种方式，BP惩罚因子的计算是一致的，pn也是使用了几何平均的方式，只是对于wn这一加权值的选择有所不同。

#### 5.2 定义式解析

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/62/62-06.png)

BLEU值衡量的是精确率，而且对不同n-gram进行集成打分。

- BP惩罚因子：为了惩罚过短的句子，由于过短的句子基数小，精确率容易提升，所以加上一个BP乘子，当预测句子长度<参考句子长度，则BP<1。

- wn的选择：李沐老师课程中是采用了\frac{1}{2^n}​作为加权因子，n越大，加权因子越小，但由于pn<1，赋予的权重越大，即长匹配具有更高的权重。而宗老师的书中所述：在BLEU的基线系统中取N＝4，wn＝1/N，也可以参考。

### 6. QA

问题：LSTM、GRU、Seq2Seq的区别是什么？

> Seq2Seq是一种由编码器和解码器组成的框架，而LSTM、GRU是组成编码器和解码器的一种单元。

问题：encoder的输出和decoder的输入，拼接和按位相加起来有什么区别么？

> 不能够按位加，由于encoder的输出最后维度是hidden_size，而decoder的输入最后维度是embedding_size，可能不一样，所以用拼接。

问题：embedding层是做word2vec吗？

> 这里不是，这里是从头开始训练。现在用的比较多得都是预训练，BERT等。

# 63-束搜索

在序列生成问题中，常用的方法是一个个词元地进行生成，但是先前步生成的词元会影响之后词元的概率分布，为此，我们需要使用搜索算法来得到一个较好的序列

### 贪心搜索

贪心搜索即每个时间步都选择具有最高条件概率的词元。
$$
y_{t'} = \operatorname*{argmax}*{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y*{t'-1}, \mathbf{c})
$$
我们的目标是找到一个最有序列，他的联合概率，也就是每步之间的条件概率的乘积，最大。
$$
\prod_{t'=1}^{T'} P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})
$$
然而，贪心搜索很可能搜索到的不是最优解，例如：

![](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/63/Greedy_or_not.png)

左侧的搜索方式为贪心搜索，每次找到当前条件概率最大的选项进行预测，但是这样可能会导致之后的条件概率较小，从而导致最终的联合概率较小，生成的序列不优。

而右侧的选择方式虽然在第二步选择了较小的选项，但之后在第三步时有了条件概率为0.6选项，最终结果反而更好。

### 穷举搜索

穷举搜索枚举所有可能的输出序列及其概率，然后选择概率最大的作为最终的输出，枚举搜索可以保证得到最优解，但是计算复杂度很高，难以实现

### 束搜索（beam search）

束搜索综合了贪心搜索和穷举搜索，在能接受的计算成本下得到比贪心搜索更好的结果。

束搜索有一个超参数，名为**束宽（beam size）**k，束搜索的具体流程如下：

- 1：在第一时间步选择条件概率最高的k个选项

- 2：对随后的每个时间步，基于上一时间步的k个候选输出序列预测这一时间步的所有可能选项的条件概率，从中取k个最大的

- 3：最后基于每步得到的序列，删去截止符和其后元素，获得最终候选序列集合，取出加权条件概率最大的

加权条件概率公式如下：
$$
\frac{1}{L^\alpha} \log P(y_1, \ldots, y_{L}\mid \mathbf{c}) = \frac{1}{L^\alpha} \sum_{t'=1}^L \log P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c}),
$ 式中\frac{1}{L^\alpha}$用于调整长序列的评估值使得长短序列间的比较公平

束宽k的选择：

- k=1时实际为贪心搜索
- k越小搜索速度越快，但结果越差，k越大则搜索速度越慢，但结果越好

束搜索只在测试时使用

# 65 -注意力分数

- [65 注意力分数](#65-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0)

- [掩蔽softmax操作](#%E6%8E%A9%E8%94%BDsoftmax%E6%93%8D%E4%BD%9C)

- [加性注意力](#%E5%8A%A0%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B)

- [缩放点积注意力](#%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B)

- [小结](#%E5%B0%8F%E7%BB%93) 

- 在上一节中，我们使用高斯核来对查询和键之间的关系建模。我们可以将上一节中的高斯核函数部分视为注意力评分函数，简称评分函数，然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，我们将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。

```python
import math
import torch
from torch import nn
from d2l import torch as d2l
```

#### 掩蔽softmax操作

- 正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。 例如，为了在 [9.5节](https://zh-v2.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#sec-machine-translation)中高效处理小批量数据集， 某些文本序列被填充了没有意义的特殊词元。 为了仅将有意义的词元作为值来获取注意力汇聚， 我们可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。 通过这种方式，我们可以在下面的`masked_softmax`函数中 实现这样的*掩蔽softmax操作*（masked softmax operation）， 其中任何超出有效长度的位置都被掩蔽并置为0。

```python
#@save
def masked_softmax(X, valid_lens):
    """通过在最后一个轴上掩蔽元素来执行softmax操作"""
    # X:3D张量，valid_lens:1D或2D张量
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,
                              value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)
```

#### 加性注意力

- 一般来说，当查询和键是不同长度的矢量时， 我们可以使用加性注意力作为评分函数。

```python
#@save
class AdditiveAttention(nn.Module):
    """加性注意力"""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # 在维度扩展后，
        # queries的形状：(batch_size，查询的个数，1，num_hidden)
        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)
        # 使用广播方式进行求和
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。
        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # values的形状：(batch_size，“键－值”对的个数，值的维度)
        return torch.bmm(self.dropout(self.attention_weights), values)
```

#### 缩放点积注意力

- 使用点积可以得到计算效率更高的评分函数， 但是点积操作要求查询和键具有相同的长度*d*。 假设查询和键的所有元素都是独立的随机变量， 并且都满足零均值和单位方差， 那么两个向量的点积的均值为0，方差为*d*。 为确保无论向量长度如何， 点积的方差在不考虑向量长度的情况下仍然是1， 我们将点积除以√d,在下面的缩放点积注意力的实现中，我们使用了暂退法进行模型正则化。

```python
#@save
class DotProductAttention(nn.Module):
    """缩放点积注意力"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # queries的形状：(batch_size，查询的个数，d)
    # keys的形状：(batch_size，“键－值”对的个数，d)
    # values的形状：(batch_size，“键－值”对的个数，值的维度)
    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # 设置transpose_b=True为了交换keys的最后两个维度
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
```

#### 小结

- 将注意力汇聚的输出计算可以作为值的加权平均，选择不同的注意力评分函数会带来不同的注意力汇聚操作。
- 当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的“点－积”注意力评分函数的计算效率更高。

# 68-Transformer

### 目录

- [68-Transformer](#68-transformer)
  
  - [目录](#%E7%9B%AE%E5%BD%95)
  
  - [1.transformer架构](#1transformer%E6%9E%B6%E6%9E%84)
  
  - [2.多头注意力](#2%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B)
  
  - [3.有掩码的多头注意力](#3%E6%9C%89%E6%8E%A9%E7%A0%81%E7%9A%84%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B)
  
  - [4.基于位置的前馈网络](#4%E5%9F%BA%E4%BA%8E%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C)
  
  - [5.层归一化](#5%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96)
  
  - [6.信息传递](#6%E4%BF%A1%E6%81%AF%E4%BC%A0%E9%80%92)
  
  - [7.预测](#7%E9%A2%84%E6%B5%8B)
  
  - [总结](#%E6%80%BB%E7%BB%93)
  
  - [QA](#qa)
    
    ### 1.transformer架构

- 基于encoder-decoder架构来处理序列对

- 跟使用注意力的seq2seq不同，transformer是纯基于注意力

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/68/68-01.png)

### 2.多头注意力

- 对同一key，value，query，希望抽取不同的信息
  
  - 例如短距离关系和长距离关系

- 多头注意力使用h个独立的注意力池化
  
  - 合并各个头（head）输出得到最终输出
    
    ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/68/68-02.png)

- 数学表达式
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/68/68-02.png)

### 3.有掩码的多头注意力

- 解码器对序列中一个元素输出的时候，不应该考虑该元素之后的元素
- 可以用掩码来实现
  - 也就是计算x_i输出的时候，假装当前序列长度为i

### 4.基于位置的前馈网络

- 将输入形状变化（b,n,d）变换成（bn，d）；输出形状由（bn，d）变成（b，n，d）
- 作用两个全连接层
- 等价于两层核窗口为1的一维卷积层（全连接）

### 5.层归一化

- 批量归一化对每个特征/通道里元素进行归一化
  - 不适合序列长度会变的nlp应用
- 层归一化对每个样本里面的元素进行归一化（ layer norm ）

### 6.信息传递

- 将编码器输出作为解码中第i个transformer块中多头注意力的key和value
  - query来自目标序列
- 意味着编码器和解码器中块的个数，输出维度都是一样的

### 7.预测

- 预测第t+1个输出时
- 解码器中输入前t个预测值（顺序）
  - 在自注意力中，前t个预测值作为key和value，第t个预测值还作为query

### 总结

- transformer是一个纯使用注意力的encoder-decoder
- 编码器和解码器都有n个transformer块
- 每个块里面使用多头注意力，基于位置的前馈网络，层归一化

### QA

- 多头注意力，concat和相加取平均怎么选择？
  - 老师认为concat保留的信息更全面，更好
- 为什么在获取词向量之后，需要对词向量进行缩放（乘以embedding size的开方之后再加上PE）
  - embedding之后，向量长度变长，元素值变小，乘以之后可以保证在-1，1之间，和position大小差不多
- num of head是什么？
  - 类似卷积的多通道，多个attention关注的是不同的特征

# 70- BERT预训练

### 1.目录：

- [BERT预训练](#bert%E9%A2%84%E8%AE%AD%E7%BB%83)
  - [1.目录：](#1%E7%9B%AE%E5%BD%95)
  - [2.BERT:](#2bert)
    - [2.1 NLP里的迁移学习](#21-nlp%E9%87%8C%E7%9A%84%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0)
    - [2.2 BERT的动机](#22-bert%E7%9A%84%E5%8A%A8%E6%9C%BA)
    - [2.3 BERT架构](#23-bert%E6%9E%B6%E6%9E%84)
    - [2.4 对输入的修改](#24-%E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84%E4%BF%AE%E6%94%B9)
    - [2.5 预训练任务](#25-%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1)
      - [2.5.1 带掩码的语言模型](#251-%E5%B8%A6%E6%8E%A9%E7%A0%81%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)
      - [2.5.2 下一个句子预测](#252-%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%8F%A5%E5%AD%90%E9%A2%84%E6%B5%8B)
    - [2.6 总结](#26-%E6%80%BB%E7%BB%93)
  - [3.代码实现](#3%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0)
    - [3.1 获取输入：](#31-%E8%8E%B7%E5%8F%96%E8%BE%93%E5%85%A5)
    - [3.2 BERT实现](#32-bert%E5%AE%9E%E7%8E%B0)
    - [3.3 预训练任务](#33-%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1)
      - [3.3.1 遮掩语言模型](#331-%E9%81%AE%E6%8E%A9%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)
      - [3.3.2 下一句预测](#332-%E4%B8%8B%E4%B8%80%E5%8F%A5%E9%A2%84%E6%B5%8B)
    - [3.4 整合代码](#34-%E6%95%B4%E5%90%88%E4%BB%A3%E7%A0%81)
    - [3.5 小结](#35-%E5%B0%8F%E7%BB%93)
  - [Q&A：](#qa)

### 2.BERT:

#### 2.1 NLP里的迁移学习

- 使用预训练好的模型来抽取词，句子的特征
  
  - 例如word2vec或语言模型

- 不更新预训练好的模型

- 需要构建新的网络来抓取任务需要的信息
  
  - Word2vec忽略了时序信息
  - 语言模型只看了一个方向

#### 2.2 BERT的动机

- 基于微调的NLP模型
- 预训练的模型抽取了足够多的信息
- 新的任务只需要增加一个简单地输出层

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/69/69-1.png)

#### 2.3 BERT架构

- 只有编码器的Transformer

- 两个版本：
  
  - Base:#blocks=12,hidden size=768,#heads=12,#parameters=110M
  - Large:#blocks=24,hidden size=1024,#heads=16,#paramerter=340M

- 在大规模数据上训练>3B词

#### 2.4 对输入的修改

- 每个样本是一个句子对
- 加入额外的片段嵌入
- 位置编码可学习

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/69/69-2.png)

#### 2.5 预训练任务

##### 2.5.1 带掩码的语言模型

- Transformer的编码器是双向的，标准语言模型要求单向

- 带掩码的语言模型每次随机（15%概率）将一些词元换成
  
  ![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/57/57-5.png)

##### 2.5.2 下一个句子预测

- 预测一个句子对中两个句子是不是相邻

- 训练样本中：
  
  - 50%概率选择相邻句子对：this movie is great i like it
  - 50%概率选择随机句子对：this movie is great hello world

- 将对应的输出放到一个全连接层来预测

#### 2.6 总结

- BERT针对微调设计
- 基于Transformer的编码器做了如下修改
  - 模型更大，训练数据更多
  - 输入句子对，片段嵌入，可学习的位置编码
  - 训练时使用两个任务：
    - 带掩码的语言模型
    - 下一个句子预测

### 3.代码实现

#### 3.1 获取输入：

在自然语言处理中，有些任务（如情感分析）以单个文本作为输入，而有些任务（如自然语言推断）以一对文本序列作为输入。BERT输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT输入序列是特殊类别词元“”、文本序列的标记、以及特殊分隔词元“”的连结。当输入为文本对时，BERT输入序列是“”、第一个文本序列的标记、“”、第二个文本序列标记、以及“”的连结。我们将始终如一地将术语“BERT输入序列”与其他类型的“序列”区分开来。例如，一个*BERT输入序列*可以包括一个*文本序列*或两个*文本序列*。

为了区分文本对，根据输入序列学到的片段嵌入eA和eB分别被添加到第一序列和第二序列的词元嵌入中。对于单文本输入，仅使用eA。

下面的`get_tokens_and_segments`将一个句子或两个句子作为输入，然后返回BERT输入序列的标记及其相应的片段索引。

```python
#@save
def get_tokens_and_segments(tokens_a, tokens_b=None):
    """获取输入序列的词元及其片段索引"""
    tokens = ['<cls>'] + tokens_a + ['<sep>']
    # 0和1分别标记片段A和B
    segments = [0] * (len(tokens_a) + 2)
    if tokens_b is not None:
        tokens += tokens_b + ['<sep>']
        segments += [1] * (len(tokens_b) + 1)
    return tokens, segments
```

BERT选择Transformer编码器作为其双向架构。在Transformer编码器中常见是，位置嵌入被加入到输入序列的每个位置。然而，与原始的Transformer编码器不同，BERT使用*可学习的*位置嵌入。总之， 下图表明BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/69/69-3.png)

#### 3.2 BERT实现

下面的`BERTEncoder`类类似于 [10.7节](https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html#sec-transformer)中实现的`TransformerEncoder`类。与`TransformerEncoder`不同，`BERTEncoder`使用片段嵌入和可学习的位置嵌入。

```python
#@save
class BERTEncoder(nn.Module):
    """BERT编码器"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=1000, key_size=768, query_size=768, value_size=768,
                 **kwargs):
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(2, num_hiddens)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module(f"{i}", d2l.EncoderBlock(
                key_size, query_size, value_size, num_hiddens, norm_shape,
                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,
                                                      num_hiddens))

    def forward(self, tokens, segments, valid_lens):
        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data[:, :X.shape[1], :]
        for blk in self.blks:
            X = blk(X, valid_lens)
        return X
```

假设词表大小为10000，为了演示`BERTEncoder`的前向推断，让我们创建一个实例并初始化它的参数。

```python
vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4
norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2
encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,
                      ffn_num_hiddens, num_heads, num_layers, dropout)
```

我们将`tokens`定义为长度为8的2个输入序列，其中每个词元是词表的索引。使用输入`tokens`的`BERTEncoder`的前向推断返回编码结果，其中每个词元由向量表示，其长度由超参数`num_hiddens`定义。此超参数通常称为Transformer编码器的*隐藏大小*（隐藏单元数）

```python
tokens = torch.randint(0, vocab_size, (2, 8))
segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])
encoded_X = encoder(tokens, segments, None)
encoded_X.shape
```

#### 3.3 预训练任务

##### 3.3.1 遮掩语言模型

我们实现了下面的`MaskLM`类来预测BERT预训练的掩蔽语言模型任务中的掩蔽标记。预测使用单隐藏层的多层感知机（`self.mlp`）。在前向推断中，它需要两个输入：`BERTEncoder`的编码结果和用于预测的词元位置。输出是这些位置的预测结果。

```python
#@save
class MaskLM(nn.Module):
    """BERT的掩蔽语言模型任务"""
    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):
        super(MaskLM, self).__init__(**kwargs)
        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),
                                 nn.ReLU(),
                                 nn.LayerNorm(num_hiddens),
                                 nn.Linear(num_hiddens, vocab_size))

    def forward(self, X, pred_positions):
        num_pred_positions = pred_positions.shape[1]
        pred_positions = pred_positions.reshape(-1)
        batch_size = X.shape[0]
        batch_idx = torch.arange(0, batch_size)
        # 假设batch_size=2，num_pred_positions=3
        # 那么batch_idx是np.array（[0,0,0,1,1]）
        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)
        masked_X = X[batch_idx, pred_positions]
        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))
        mlm_Y_hat = self.mlp(masked_X)
        return mlm_Y_hat
```

为了演示`MaskLM`的前向推断，我们创建了其实例`mlm`并对其进行了初始化。回想一下，来自`BERTEncoder`的正向推断`encoded_X`表示2个BERT输入序列。我们将`mlm_positions`定义为在`encoded_X`的任一输入序列中预测的3个指示。`mlm`的前向推断返回`encoded_X`的所有掩蔽位置`mlm_positions`处的预测结果`mlm_Y_hat`。对于每个预测，结果的大小等于词表的大小。

```python
mlm = MaskLM(vocab_size, num_hiddens)
mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])
mlm_Y_hat = mlm(encoded_X, mlm_positions)
mlm_Y_hat.shape
```

通过掩码下的预测词元`mlm_Y`的真实标签`mlm_Y_hat`，我们可以计算在BERT预训练中的遮蔽语言模型任务的交叉熵损失。

```python
mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])
loss = nn.CrossEntropyLoss(reduction='none')
mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))
mlm_l.shape
```

##### 3.3.2 下一句预测

下面的`NextSentencePred`类使用单隐藏层的多层感知机来预测第二个句子是否是BERT输入序列中第一个句子的下一个句子。由于Transformer编码器中的自注意力，特殊词元“”的BERT表示已经对输入的两个句子进行了编码。因此，多层感知机分类器的输出层（`self.output`）以`X`作为输入，其中`X`是多层感知机隐藏层的输出，而MLP隐藏层的输入是编码后的“”词元。

```python
#@save
class NextSentencePred(nn.Module):
    """BERT的下一句预测任务"""
    def __init__(self, num_inputs, **kwargs):
        super(NextSentencePred, self).__init__(**kwargs)
        self.output = nn.Linear(num_inputs, 2)

    def forward(self, X):
        # X的形状：(batchsize,num_hiddens)
        return self.output(X)
```

我们可以看到，`NextSentencePred`实例的前向推断返回每个BERT输入序列的二分类预测。

```python
encoded_X = torch.flatten(encoded_X, start_dim=1)
# NSP的输入形状:(batchsize，num_hiddens)
nsp = NextSentencePred(encoded_X.shape[-1])
nsp_Y_hat = nsp(encoded_X)
nsp_Y_hat.shape
```

#### 3.4 整合代码

在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。现在我们可以通过实例化三个类`BERTEncoder`、`MaskLM`和`NextSentencePred`来定义`BERTModel`类。前向推断返回编码后的BERT表示`encoded_X`、掩蔽语言模型预测`mlm_Y_hat`和下一句预测`nsp_Y_hat`。

```python
#@save
class BERTModel(nn.Module):
    """BERT模型"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=1000, key_size=768, query_size=768, value_size=768,
                 hid_in_features=768, mlm_in_features=768,
                 nsp_in_features=768):
        super(BERTModel, self).__init__()
        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,
                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,
                    dropout, max_len=max_len, key_size=key_size,
                    query_size=query_size, value_size=value_size)
        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),
                                    nn.Tanh())
        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)
        self.nsp = NextSentencePred(nsp_in_features)

    def forward(self, tokens, segments, valid_lens=None,
                pred_positions=None):
        encoded_X = self.encoder(tokens, segments, valid_lens)
        if pred_positions is not None:
            mlm_Y_hat = self.mlm(encoded_X, pred_positions)
        else:
            mlm_Y_hat = None
        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引
        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))
        return encoded_X, mlm_Y_hat, nsp_Y_hat
```

#### 3.5 小结

- word2vec和GloVe等词嵌入模型与上下文无关。它们将相同的预训练向量赋给同一个词，而不考虑词的上下文（如果有的话）。它们很难处理好自然语言中的一词多义或复杂语义。
- 对于上下文敏感的词表示，如ELMo和GPT，词的表示依赖于它们的上下文。
- ELMo对上下文进行双向编码，但使用特定于任务的架构（然而，为每个自然语言处理任务设计一个特定的体系架构实际上并不容易）；而GPT是任务无关的，但是从左到右编码上下文。
- BERT结合了这两个方面的优点：它对上下文进行双向编码，并且需要对大量自然语言处理任务进行最小的架构更改。
- BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。
- 预训练包括两个任务：掩蔽语言模型和下一句预测。前者能够编码双向上下文来表示单词，而后者则显式地建模文本对之间的逻辑关系。

### Q&A：

##### Q1:BERT是不是很少用在CV上？

> transformer架构这几年在大量的用于CV上

##### Q2:展示一下10W batch 训练结果？

> 微调时会用到

##### Q3：使用BERT large时显存不足，有什么方法吗？

> 单机多卡，模型并行，或改用小模型

# 72-优化算法

### 目录

- [72.优化算法](#72%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95)
  
  - [目录](#%E7%9B%AE%E5%BD%95)
  
  - [1.优化问题](#1%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98)
  
  - [2.局部最小 vs 全局最小](#2%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F-vs-%E5%85%A8%E5%B1%80%E6%9C%80%E5%B0%8F)
  
  - [3.凸集和凸函数](#3%E5%87%B8%E9%9B%86%E5%92%8C%E5%87%B8%E5%87%BD%E6%95%B0)
  
  - [4.梯度下降](#4%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D)
  
  - [5.冲量法](#5%E5%86%B2%E9%87%8F%E6%B3%95)
  
  - [6.Adam](#6adam)
  
  - [总结](#%E6%80%BB%E7%BB%93)
    
    ### 1.优化问题

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/72/72-01.png)

### 2.局部最小 vs 全局最小

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/72/72-02.png)

### 3.凸集和凸函数

- 凸集：形象化来说，就是这个集合上任意两个点连一条线，这个线在集合里面
- 凸函数：形象上来说函数上任取两个点连线，函数都在该线下面
- 凸优化问题：局部最小一定是全局最小。严格凸优化问题有唯一的全局最小。
  - 凸：线性回归，softmax回归
  - 非凸：其他（MLP,CNN,RNN,attention）

### 4.梯度下降

- 梯度下降——最简单的迭代求解算法
- 随机梯度下降
  - 求导数需要求所有样本导数，样本多的情况下代价太大
  - 理论依据：所用样本，和随机选取一个样本得到的数学期望是一样的。
- 小批量随机梯度下降（实际应用的）
  - 计算原因：计算单样本的梯度难以完全利用硬件资源
  - 采集一个随机子集
  - 理论依据：无偏近，但降低了方差

### 5.冲量法

- 使用平滑过的梯度对权重更新，不容易震荡
- momentum

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/72/72-03.png)

### 6.Adam

- 非常平滑，对于学习率不敏感
- 对于t比较小的时候，由于v_0=0,所以会导致一开始值比较小，做了一个修正。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/72/72-04.png)

- 为什么除以\sqrt{\widehat{s}_t}+\epsilon？
  - 在nlp里面常用，起到正则化的作用，控制每个维度的值在合适的大小。

![image](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/72/72-05.png)

### 总结

- 深度学习模型大部分是非凸的
- 小批量随机梯度下降是最常见的优化算法
- 冲量是对梯度做平滑
- Adam是对梯度做平滑，且对梯度各个维度值做重新调整，对于学习率不敏感